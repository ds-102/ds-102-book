
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Statistical Decision Theory &#8212; Data, Inference, and Decisions</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapters/01/05_decision_theory';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Chapter 2: Bayesian Inference" href="../02/intro.html" />
    <link rel="prev" title="Binary Classification" href="04_binary_classification.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data, Inference, and Decisions</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Chapter 1: Binary Decision-Making</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_decisions_and_errors.html">Binary Decision-Making and Error Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_hypothesis_testing.html">Hypothesis Testing and p-Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_multiple_tests.html">Multiple Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_binary_classification.html">Binary Classification</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Decision Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02/intro.html">Chapter 2: Bayesian modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02/01_parameter_estimation.html">Parameter Estimation and Bayesian Inference Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/02_hierarchical_models.html">Hierarchical Bayesian Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/03_graphical_models.html">Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/04_inference.html">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/05_inference_with_sampling.html">Bayesian Inference with Sampling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03/intro.html">Chapter 3: Prediction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03/01_prediction.html">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/02_regression_review.html">Linear Regression Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/03_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/04_model_checking.html">[WIP] Model Checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/05_uncertainty_quantification.html">[WIP] Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/06_nonparametric.html">Nonparametric Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/07_neural_networks.html">Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04/intro.html">Chapter 4: Causal Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04/01_association_correlation_causation.html">Understanding Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/02_quantifying_association.html">Quantifying Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/03_causality.html">Causality and Potential Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/04_randomized_experiments.html">Causality in Randomized Experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/05_observational_studies_unconfoundedness.html">Causality in Observational Studies: Unconfoundedness</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/01/05_decision_theory.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/chapters/01/05_decision_theory.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Statistical Decision Theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-decision-0-1-loss">Binary decision: 0-1 loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-decision-ell-2-loss">Continuous decision: <span class="math notranslate nohighlight">\(\ell_2\)</span> loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#known-and-unknown">Known and unknown</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-and-random-finding-the-average-loss-in-bayesian-and-frequentist-approaches">Fixed and random: finding the average loss in Bayesian and frequentist approaches</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-loss-analysis">Frequentist loss analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-loss-analysis">Bayesian loss analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-frequentist-and-bayesian-risk">Comparing frequentist and Bayesian risk</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-frequentist-risk-for-ell-2-loss-and-the-bias-variance-tradeoff">Example: frequentist risk for <span class="math notranslate nohighlight">\(\ell_2\)</span> loss and the bias-variance tradeoff</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-risk">Bayes risk</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="statistical-decision-theory">
<h1>Statistical Decision Theory<a class="headerlink" href="#statistical-decision-theory" title="Link to this heading">#</a></h1>
<p>Up until now, we’ve used error rates to help us understand tradeoffs in binary decision-making. In this section, we’ll introduce a more general theoretical framework to understand and quantify errors we make, and start to explore the theoretical branch of statistics known as <strong>statistical decision theory</strong>.</p>
<p>Remember our setup: we have some <strong>unknown quantity</strong> <span class="math notranslate nohighlight">\(\theta\)</span> that we’re interested in. We collect data <span class="math notranslate nohighlight">\(x\)</span>. Our data are random, and come from the distribution <span class="math notranslate nohighlight">\(p(x|\theta)\)</span>. We use the data to reason about <span class="math notranslate nohighlight">\(\theta\)</span>. Often, we’ll want to use the data to compute an estimate for <span class="math notranslate nohighlight">\(\theta\)</span> but sometimes, we may want to do something slightly different. In order to describe “the thing we do to the data”, we’ll use the notation <span class="math notranslate nohighlight">\(\delta(x)\)</span>. This represents the result of applying some procedure <span class="math notranslate nohighlight">\(\delta\)</span> to the data. For example, <span class="math notranslate nohighlight">\(\delta\)</span> might be the sample average of many data points, or the result of logistic regression. The obvious next question is: how do we choose which procedure <span class="math notranslate nohighlight">\(\delta\)</span> to use? We’ll decide by quantifying how “good” each <span class="math notranslate nohighlight">\(\delta\)</span> is, and then trying to choose the “best” one.</p>
<p>“Good” is a very abstract notion: to quantify it, we’ll need a quantitative measure of how good (or to be more precise, how bad) our procedure <span class="math notranslate nohighlight">\(\delta\)</span> is. We’ll call this a <strong>loss function</strong>. Notationally, we’ll write <span class="math notranslate nohighlight">\(\ell(\theta, \delta(x))\)</span> to represent the loss associated with the outcome <span class="math notranslate nohighlight">\(\delta(x)\)</span> if the true value is <span class="math notranslate nohighlight">\(\theta\)</span>. To summarize:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \text{Variable/notation} &amp; \quad \text{What it means} \\
    \hline
    \theta      &amp; \quad \text{unknown quantity/quantities of interest: parameter(s)} \\
    x           &amp; \quad \text{observed data} \\
    p(x|\theta) &amp; \quad \text{probability distribution for data $x$ (depends on $\theta$)} \\
    \delta(x)   &amp; \quad \text{decision or result computed from $x$, often an estimate of $\theta$} \\
    \ell(\delta(x), \theta) &amp; \quad \text{loss (badness) for output $\delta(x)$ and true parameter(s) $\theta$}
\end{align*}
\end{split}\]</div>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h2>
<p>That’s a very abstract definition: let’s make it more concrete with a few examples.</p>
<section id="binary-decision-0-1-loss">
<h3>Binary decision: 0-1 loss<a class="headerlink" href="#binary-decision-0-1-loss" title="Link to this heading">#</a></h3>
<p>For our first example, we’ll return to our binary decision-making setting. In that case:</p>
<ul class="simple">
<li><p>Our unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span> is binary, and corresponds to reality, which we’ve been calling <span class="math notranslate nohighlight">\(R\)</span>.</p></li>
<li><p>Our data <span class="math notranslate nohighlight">\(x\)</span> were whatever we used to compute the p-value.</p></li>
<li><p>The decision <span class="math notranslate nohighlight">\(\delta\)</span> is a binary decision, which we’ve been calling <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-right"><p></p></th>
<th class="head text-center"><p>D = <span class="math notranslate nohighlight">\(\delta(x)\)</span> = 0</p></th>
<th class="head text-center"><p>D = <span class="math notranslate nohighlight">\(\delta(x)\)</span> = 1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-right"><p><span class="math notranslate nohighlight">\(R=\theta=0\)</span></p></td>
<td class="text-center"><p>TN loss</p></td>
<td class="text-center"><p>FP loss</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p><span class="math notranslate nohighlight">\(R=\theta=1\)</span></p></td>
<td class="text-center"><p>FN loss</p></td>
<td class="text-center"><p>TP loss</p></td>
</tr>
</tbody>
</table>
</div>
<p>Here are a few concrete examples, and what each of these quantities would represent:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Example</p></th>
<th class="head"><p>Unknown <span class="math notranslate nohighlight">\(\theta\)</span> / <span class="math notranslate nohighlight">\(R\)</span></p></th>
<th class="head"><p>Data <span class="math notranslate nohighlight">\(x\)</span></p></th>
<th class="head"><p>Decision <span class="math notranslate nohighlight">\(\delta\)</span> / <span class="math notranslate nohighlight">\(D\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Disease testing</p></td>
<td><p>Whether a person has a disease</p></td>
<td><p>Collected clinical data (blood sample, vital signs, etc.)</p></td>
<td><p>Should we give the person treatments for that disease?</p></td>
</tr>
<tr class="row-odd"><td><p>Finding oil wells</p></td>
<td><p>Whether underground oil exists in a certain area</p></td>
<td><p>Readings from seismic sensors, etc.</p></td>
<td><p>Should we drill for oil in this location?</p></td>
</tr>
<tr class="row-even"><td><p>Product recommendation</p></td>
<td><p>Will a user buy this product?</p></td>
<td><p>User behavior, interest in similar products, etc.</p></td>
<td><p>Should we recommend the product to the user?</p></td>
</tr>
</tbody>
</table>
</div>
<p>Note that we haven’t really talked much about <span class="math notranslate nohighlight">\(p(x|\theta)\)</span>, since we’ve been working with <span class="math notranslate nohighlight">\(\delta(x)\)</span> directly: we’ll discuss this more in the next chapter.</p>
<p>Our loss function will depend on the problem we’re solving. Since in this case, both the inputs (<span class="math notranslate nohighlight">\(\theta\)</span>/<span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span>/<span class="math notranslate nohighlight">\(D\)</span>) are binary, we can write the loss in a 2x2 table that looks exactly like the ones we’ve seen before.
If both kinds of error (false positive and false negative) are equally bad, we can use the simplest loss function, the <strong>0-1 loss</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\ell(\delta(x), \theta) = \begin{cases}
    0 &amp; \text{if }\theta = \delta(x) \\
    1 &amp; \text{if }\theta \neq \delta(x)
\end{cases}
\end{split}\]</div>
<p><strong>Exercise</strong>: Suppose we have a situation where a false positive is five times worse than a false negative. How would you write the loss function?</p>
</section>
<section id="continuous-decision-ell-2-loss">
<h3>Continuous decision: <span class="math notranslate nohighlight">\(\ell_2\)</span> loss<a class="headerlink" href="#continuous-decision-ell-2-loss" title="Link to this heading">#</a></h3>
<p>Now, suppose our parameter<span class="math notranslate nohighlight">\(\theta\)</span> is continuous, and <span class="math notranslate nohighlight">\(\delta(x)\)</span> is our estimate of the parameter from the data. To make things a little more concrete, <span class="math notranslate nohighlight">\(\theta\)</span> could be the average height of people in a population, and <span class="math notranslate nohighlight">\(x\)</span> could be the heights of people in a random sample from that population. In this case, our loss shouldn’t just be right vs wrong: we should use a loss function that’s smaller when our estimate is close, and larger when our estimate is far away.</p>
<p>You’ve probably already seen one before: the squared error loss, also known as the <strong><span class="math notranslate nohighlight">\(\ell_2\)</span> loss</strong>:</p>
<div class="math notranslate nohighlight">
\[
\ell(\delta(x), \theta) = \big(\delta(x) - \theta\big)^2
\]</div>
<p>We’ll analyze the <span class="math notranslate nohighlight">\(\ell_2\)</span> loss a little more later.</p>
<p><strong>Exercise</strong>: Suppose we have a situation where it’s much worse to make a guess that’s too high, compared to a guess that’s too low. How would you construct a loss function for this problem?</p>
</section>
</section>
<section id="known-and-unknown">
<h2>Known and unknown<a class="headerlink" href="#known-and-unknown" title="Link to this heading">#</a></h2>
<p>At this point, you may be wondering: if <span class="math notranslate nohighlight">\(\theta\)</span> is unknown, how can we ever compute the loss function? It’s important to keep in mind that when we apply <span class="math notranslate nohighlight">\(\delta(x)\)</span> on real data, we don’t know <span class="math notranslate nohighlight">\(\theta\)</span>. But right now, we’re building up some machinery to help us analyze different procedures. In other words, we’re trying to get to a place where we can answer questions like “what procedures are most likely to give us estimates that are close to <span class="math notranslate nohighlight">\(\theta\)</span>?”</p>
</section>
<section id="fixed-and-random-finding-the-average-loss-in-bayesian-and-frequentist-approaches">
<h2>Fixed and random: finding the average loss in Bayesian and frequentist approaches<a class="headerlink" href="#fixed-and-random-finding-the-average-loss-in-bayesian-and-frequentist-approaches" title="Link to this heading">#</a></h2>
<p>The loss function is a function of <span class="math notranslate nohighlight">\(\delta(x)\)</span>, which is the procedure result for particular data <span class="math notranslate nohighlight">\(x\)</span>, and the particular parameter <span class="math notranslate nohighlight">\(\theta\)</span>. This isn’t particularly useful to us: we’d like to understand how the loss does “on average”. But in order to compute any kind of averages, we need to decide what’s random and what’s fixed. This is an important fork in the road: we can either take the Bayesian or the frequentist route. Let’s examine what happens if we try each one:</p>
<section id="frequentist-loss-analysis">
<h3>Frequentist loss analysis<a class="headerlink" href="#frequentist-loss-analysis" title="Link to this heading">#</a></h3>
<p>In the frequentist world, we assume that our unknown <span class="math notranslate nohighlight">\(\theta\)</span> is <strong>fixed</strong>. The data are the only random piece. So, we’re going to look at the average across different possibilities for the data <span class="math notranslate nohighlight">\(x\)</span>. Since the data comes from the distribution <span class="math notranslate nohighlight">\(p(x|\theta)\)</span>, which depends on <span class="math notranslate nohighlight">\(\theta\)</span>, we should expect that this “averaging” will produce something that depends on <span class="math notranslate nohighlight">\(\theta\)</span>. We’ll call our average the <strong>frequentist risk</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    R(\theta) 
        &amp;= E_{x|\theta}\left[\ell(\delta(x), \theta)\right] \\
        &amp;= \begin{cases} 
                \displaystyle \sum_x \ell(\delta(x), \theta) p(x|\theta) &amp; \text{if $x$ discrete} \\
                \displaystyle \int \ell(\delta(x), \theta) p(x|\theta) dx &amp; \text{if $x$ continuous} 
           \end{cases}
\end{align*}
\end{split}\]</div>
<p>The frequentist risk is a function of <span class="math notranslate nohighlight">\(\theta\)</span>. It tells us: for a particular value of <span class="math notranslate nohighlight">\(\theta\)</span>, how poorly does the procedure <span class="math notranslate nohighlight">\(\delta\)</span> do if we average over all possible datasets?</p>
</section>
<section id="bayesian-loss-analysis">
<h3>Bayesian loss analysis<a class="headerlink" href="#bayesian-loss-analysis" title="Link to this heading">#</a></h3>
<p>In the Bayesian world, we assume that our unknown <span class="math notranslate nohighlight">\(\theta\)</span> is <strong>random</strong>. Since we observe a particular dataset <span class="math notranslate nohighlight">\(x\)</span>, we’ll be a lot more interested in the randomness in <span class="math notranslate nohighlight">\(\theta\)</span> than the randomness in <span class="math notranslate nohighlight">\(x\)</span>. So, we’ll condition on the particular dataset we got, and look at the average across different possibilities for the unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span>. We’ll call our average the <strong>Bayesian posterior risk</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \rho(x) 
        &amp;= E_{\theta|x}\left[\ell(\delta(x), \theta)\right] \\
        &amp;= \begin{cases} 
                \displaystyle \sum_\theta \ell(\delta(x), \theta) p(\theta|x) &amp; \text{if $\theta$ discrete} \\
                \displaystyle \int \ell(\delta(x), \theta) p(\theta|x) d\theta &amp; \text{if $\theta$ continuous} 
           \end{cases}
\end{align*}
\end{split}\]</div>
<p>The Bayesian risk is a function of <span class="math notranslate nohighlight">\(x\)</span>. It tells us: given that we observed a particular dataset <span class="math notranslate nohighlight">\(x\)</span>, how poorly does the procedure <span class="math notranslate nohighlight">\(\delta\)</span> do, averaged over all possible values of the parameter <span class="math notranslate nohighlight">\(\theta\)</span>?</p>
</section>
<section id="comparing-frequentist-and-bayesian-risk">
<h3>Comparing frequentist and Bayesian risk<a class="headerlink" href="#comparing-frequentist-and-bayesian-risk" title="Link to this heading">#</a></h3>
<p>Operationally, both of these look kind of similar: we’re averaging the loss with respect to some conditional probability distribution. But conceptually, they’re very different: the frequentist risk fixes the parameter, and averages over all the data;  the Bayesian posterior risk fixes the data, and averages over all parameters.</p>
</section>
</section>
<section id="example-frequentist-risk-for-ell-2-loss-and-the-bias-variance-tradeoff">
<h2>Example: frequentist risk for <span class="math notranslate nohighlight">\(\ell_2\)</span> loss and the bias-variance tradeoff<a class="headerlink" href="#example-frequentist-risk-for-ell-2-loss-and-the-bias-variance-tradeoff" title="Link to this heading">#</a></h2>
<p>Let’s work through an example compuing the frequentist risk using the <span class="math notranslate nohighlight">\(\ell_2\)</span> loss. We’ll find that the result can give us some important insights.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
R(\theta) 
&amp;= E_{x|\theta}\left[\ell(\delta(x), \theta)\right] \\
&amp;= E_{x|\theta}\Big[\big(\delta(x) - \theta\big)^2\Big] \\
\end{align}
\end{split}\]</div>
<p>To make the math work out later, we’ll add and subtract the term <span class="math notranslate nohighlight">\(E_{x|\theta}[\delta(x)]\)</span>. Before we work out the result, let’s think about what this term means. It’s the average value of the procedure <span class="math notranslate nohighlight">\(\delta\)</span>: in other words, for a particular <span class="math notranslate nohighlight">\(\theta\)</span>, it tells us what value of <span class="math notranslate nohighlight">\(\delta(x)\)</span> we should expect to get, averaged across different possible values of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
R(\theta) 
&amp;= E_{x|\theta}\Big[\big(\delta(x) - \theta\big)^2\Big] \\
&amp;= E_{x|\theta}\Big[\big(
    \delta(x) \overbrace{- E_{x|\theta}[\delta(x)] + E_{x|\theta}[\delta(x)]}^{=0} - \theta
\big)^2\Big] \\
&amp;= E_{x|\theta}\Big[\big(
    \underbrace{\delta(x) - E_{x|\theta}[\delta(x)]}_{\text{prediction minus avg. prediction}} + 
    \underbrace{E_{x|\theta}[\delta(x)] - \theta}_{\text{avg. prediction minus true value}}
\big)^2\Big] \\
\end{align}
\end{split}\]</div>
<p>To make the math a little easier to read, we’ll write <span class="math notranslate nohighlight">\(\delta = \delta(x)\)</span> and <span class="math notranslate nohighlight">\(\bar{\delta} = E_{x|\theta}[\delta(x)]\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
R(\theta) 
&amp;= E_{x|\theta}\Big[\big(
    \delta(x) - E_{x|\theta}[\delta(x)] + E_{x|\theta}[\delta(x)] - \theta
\big)^2\Big] \\
&amp;= E_{x|\theta}\Big[\big(
    \delta - \bar{\delta} + \bar{\delta} - \theta
\big)^2\Big] \\
&amp;= E_{x|\theta}\Big[
    \big(\delta - \bar{\delta}\big)^2 +
    \underbrace{2\big(\delta - \bar{\delta}\big)\big(\bar{\delta} - \theta\big)}_{=0} + 
    \big(\bar{\delta} - \theta\big)^2
\Big] \\
&amp;= E_{x|\theta}\Big[\big(\delta - \bar{\delta}\big)^2\Big] + 
     E_{x|\theta}\Big[\big(\bar{\delta} - \theta\big)^2\Big] \\
&amp;= \underbrace{E_{x|\theta}\Big[\big(\delta - \bar{\delta}\big)^2\Big]}_{\text{variance of }\delta(x)} + 
     \big(\underbrace{\bar{\delta} - \theta}_{\text{bias of }\delta(x))}\big)^2 \\
\end{align}
\end{split}\]</div>
<p>We’ve shown that for the <span class="math notranslate nohighlight">\(\ell_2\)</span> loss, the frequentist risk is the sum of two terms, called the <strong>variance</strong> and the square of the <strong>bias</strong>.</p>
<p>The <strong>variance</strong>, <span class="math notranslate nohighlight">\(E_{x|\theta}\Big[\big(\delta(x) - E_{x|\theta}[\delta(x)]\big)^2\Big]\)</span>, answers the question: as the data varies, how far away will <span class="math notranslate nohighlight">\(\delta\)</span> be from its average value? In general, if your procedure <span class="math notranslate nohighlight">\(\delta\)</span> is very sensitive to variations in the data, your variance will be high.</p>
<p>The <strong>bias</strong>, <span class="math notranslate nohighlight">\(E_{x|\theta}[\delta(x)] - \theta\)</span>, answers the question: how far is the average value of <span class="math notranslate nohighlight">\(\delta\)</span> from the true parameter <span class="math notranslate nohighlight">\(\theta\)</span>? In general, if your procedure <span class="math notranslate nohighlight">\(\delta\)</span> does a good job of capturing the complexity of predicting <span class="math notranslate nohighlight">\(\theta\)</span>, your bias will be low.</p>
<p>When trying to reduce the risk (average loss), most methods try to reduce the variance and/or the bias. Many methods for estimation and prediction try to deal with the tradeoff between variance and bias: ideally we’d like both to be as small as possible, but we often need to accept a little more of one in order to make big reductions in the other.</p>
</section>
<section id="bayes-risk">
<h2>Bayes risk<a class="headerlink" href="#bayes-risk" title="Link to this heading">#</a></h2>
<p>The two risks above are obtained by taking the expectation with respect to either the data <span class="math notranslate nohighlight">\(x\)</span> or the parameter <span class="math notranslate nohighlight">\(\theta\)</span>. What if we take the expectation with respect to both? The <strong>Bayes risk</strong> is exactly that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
R(\delta) 
&amp;= E_{x, \theta} [\ell(\delta(x), \theta)]  \\
&amp;= E_\theta [R(\theta)] \\
&amp;= E_x [R(x)]
\end{align*}
\end{split}\]</div>
<p>where the last two inequalities follow from Fubini’s theorem (i.e., that we can do the integrals for the expectations in either order and get the same result). The Bayes risk is a single number that summarizes the procedure <span class="math notranslate nohighlight">\(\delta\)</span>. The name is somewhat misleading: it isn’t really Bayesian or frequentist.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/chapters/01"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_binary_classification.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Binary Classification</p>
      </div>
    </a>
    <a class="right-next"
       href="../02/intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 2: Bayesian Inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-decision-0-1-loss">Binary decision: 0-1 loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-decision-ell-2-loss">Continuous decision: <span class="math notranslate nohighlight">\(\ell_2\)</span> loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#known-and-unknown">Known and unknown</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-and-random-finding-the-average-loss-in-bayesian-and-frequentist-approaches">Fixed and random: finding the average loss in Bayesian and frequentist approaches</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-loss-analysis">Frequentist loss analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-loss-analysis">Bayesian loss analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-frequentist-and-bayesian-risk">Comparing frequentist and Bayesian risk</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-frequentist-risk-for-ell-2-loss-and-the-bias-variance-tradeoff">Example: frequentist risk for <span class="math notranslate nohighlight">\(\ell_2\)</span> loss and the bias-variance tradeoff</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-risk">Bayes risk</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Data 102 Staff
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>