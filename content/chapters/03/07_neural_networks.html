
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Neural networks and backpropagation &#8212; Data, Inference, and Decisions</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapters/03/07_neural_networks';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Chapter 4: Causal Inference" href="../04/intro.html" />
    <link rel="prev" title="Nonparametric methods" href="06_nonparametric.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data, Inference, and Decisions</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01/intro.html">Chapter 1: Binary Decision-Making</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01/01_decisions_and_errors.html">Binary Decision-Making and Error Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/02_hypothesis_testing.html">Hypothesis Testing and p-Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/03_multiple_tests.html">Multiple Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/04_binary_classification.html">Binary Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/05_decision_theory.html">Decision Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02/intro.html">Chapter 2: Bayesian modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02/01_parameter_estimation.html">Parameter Estimation and Bayesian Inference Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/02_hierarchical_models.html">Hierarchical Bayesian Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/03_graphical_models.html">Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/04_inference.html">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/05_inference_with_sampling.html">Bayesian Inference with Sampling</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Chapter 3: Prediction</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_prediction.html">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_regression_review.html">Linear Regression Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_model_checking.html">[WIP] Model Checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_uncertainty_quantification.html">[WIP] Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_nonparametric.html">Nonparametric Methods</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04/intro.html">Chapter 4: Causal Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04/01_association_correlation_causation.html">Understanding Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/02_quantifying_association.html">Quantifying Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/03_causality.html">Causality and Potential Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/04_randomized_experiments.html">Causality in Randomized Experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/05_observational_studies_unconfoundedness.html">Causality in Observational Studies: Unconfoundedness</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/03/07_neural_networks.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/chapters/03/07_neural_networks.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural networks and backpropagation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-feed-forward-neural-network">A feed-forward neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-stochastic-gradient-descent">Review: Stochastic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-and-backpropagation">Gradients and Backpropagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-gradients-with-the-chain-rule">Computing gradients with the chain rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-an-example">Backpropagation: an example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-backpropagation-in-pytorch">(Optional) Backpropagation in pytorch</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-networks-and-backpropagation">
<h1>Neural networks and backpropagation<a class="headerlink" href="#neural-networks-and-backpropagation" title="Link to this heading">#</a></h1>
<p>Neural nets are a very powerful class of methods that have become popular in fields like computer vision and natural language processing, where coming up with good features can be challenging.</p>
<p>While there’s a rich mathematical foundation underlying neural nets, in this class we’ll focus on one of the big computational ideas that’s at the heart of most neural net implementations: backpropagation and automatic differentiation. While these ideas were initially conceived for neural nets, they’re now used in many other ways too: libraries like PyMC use automatic differentiation to do efficient Bayesian inference; and much more.</p>
<p>In general, automatic differentiation and backpropagation are useful for any problem where the solution involves computing gradients!</p>
<section id="a-feed-forward-neural-network">
<h2>A feed-forward neural network<a class="headerlink" href="#a-feed-forward-neural-network" title="Link to this heading">#</a></h2>
<p>As we’ve already seen, linear regression is a simple but powerful model: to predict a value <span class="math notranslate nohighlight">\(y\)</span> from a vector of features <span class="math notranslate nohighlight">\(x = (x_1, \ldots, x_k)\)</span>, linear regression uses the following:</p>
<div class="math notranslate nohighlight">
\[
y = Wx + b
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(W\)</span> is a vector of coefficients, sometimes also called <strong>weights</strong>, and <span class="math notranslate nohighlight">\(b\)</span> is a scalar that we call the intercept or bias. As we saw in the previous section, linear models can fail when the relationship between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is nonlinear. We also saw that if we want to model complex, nonlinear interactions <em>while still using linear models</em>, we need to define more complex features.</p>
<p>Motivated by this, what if we tried using another layer of linear regression that could compute features for us? It might look something like this:</p>
<div class="math notranslate nohighlight">
\[
y = W_2(\overbrace{W_1 x + b_1}^\text{features}) + b_2
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(W_1\)</span> is now an <span class="math notranslate nohighlight">\(m \times k\)</span> matrix of weights, and the result of the matrix-vector multiplication and addition <span class="math notranslate nohighlight">\(W_1x + b_1\)</span> is an <span class="math notranslate nohighlight">\(m\)</span>-dimensional vector of features. We then apply linear regression with those features, using the weights in the vector <span class="math notranslate nohighlight">\(W_2\)</span> and intercept/bias in the scalar <span class="math notranslate nohighlight">\(b_2\)</span>, to obtain <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Unfortunately, this doesn’t work because it reduces to a single layer of linear regression. Applying a bit of algebra, we can simplify the above equation to <span class="math notranslate nohighlight">\(y = \big(W_2W_1\big)x + \big(W_2b_1 + b_2\big)\)</span>, which is just linear regression written in an unnecessarily complicated way.</p>
<p>In order to prevent the simplification to linear regression, we could apply a nonlinear function <span class="math notranslate nohighlight">\(f\)</span> as part of computing the features:</p>
<div class="math notranslate nohighlight">
\[
y = W_2 \overbrace{f(W_1 x + b_1)}^\text{features} + b_2
\]</div>
<p>This is now the simplest possible neural network, which we call a <strong>feed-forward</strong> or <strong>fully connected</strong> network with one hidden layer (the so-called “hidden layer” is the result of the computation <span class="math notranslate nohighlight">\(f(W_1 x + b_1)\)</span>).</p>
<p>The nonlinear function <span class="math notranslate nohighlight">\(f\)</span> can be anything from a sigmoid or logistic function to the ReLU (restricted linear unit) function, <span class="math notranslate nohighlight">\(f(z) = \max(0, z)\)</span>.</p>
<p>In order to fit a linear regression model, we had to estimate good coefficients. In probabilistic terms, we did this using  In order to fit a neural network, we have to estimate good weights <span class="math notranslate nohighlight">\(W_1, W_2, \ldots\)</span> and biases <span class="math notranslate nohighlight">\(b_1, b_2, ldots\)</span>.</p>
<p>To make our notation a little simpler, we’ll use <span class="math notranslate nohighlight">\(\theta\)</span> to denote all our parameters: <span class="math notranslate nohighlight">\(\theta = (W_1, W_2, b_1, b_2)\)</span>. In order to find the best values of <span class="math notranslate nohighlight">\(\theta\)</span>, we’ll define a loss function <span class="math notranslate nohighlight">\(\ell(\theta, y)\)</span> and then use stochastic gradient descent to minimize it.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/mgaohBtnub4"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="empirical-risk-minimization">
<h2>Empirical risk minimization<a class="headerlink" href="#empirical-risk-minimization" title="Link to this heading">#</a></h2>
<p>We start by choosing a loss function. In general, the choice of loss function depends on the problem we’re solving, but two common choices are the squared error loss (also known as <span class="math notranslate nohighlight">\(\ell_2\)</span> loss) and the binary cross-entropy loss (BCE). Let’s consider the <span class="math notranslate nohighlight">\(\ell_2\)</span> loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\ell(\theta, y) 
    &amp;= (y - \hat{y})^2 \\
    &amp;= \left(y - \left[W_2 f(W_1 x + b_1) + b_2\right]\right)^2
\end{align*}
\end{split}\]</div>
<p>We’ll minimize the average loss:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
R(\theta)
    &amp;= \frac{1}{n} \sum_{i=1}^n \left(y_i - \left[W_2 f(W_1 x_i + b_1) + b_2\right]\right)^2
\end{align*}
\]</div>
<p>Here, we’re averaging over the empirical distribution of the data in our training set, which makes this a <strong>frequentist risk</strong>. The process of minimizing this loss is often referred to as <strong>empirical risk minimization</strong>.</p>
</section>
<section id="review-stochastic-gradient-descent">
<h2>Review: Stochastic gradient descent<a class="headerlink" href="#review-stochastic-gradient-descent" title="Link to this heading">#</a></h2>
<p><em>For more on stochastic gradient descent, you may find it helpful to review Chapter of the Data 100 textbook</em>.</p>
<p>(Stochastic) gradient descent is a powerful tool that lets us find the minimum of any function, as long as we can compute its gradient. Recall that a gradient is a vector of partial derivatives with respect to each parameter. In the example above, our gradient would be</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_\theta \ell (\theta, y) = 
    \begin{pmatrix}
        \frac{\partial \ell}{\partial W_1}(\theta, y)\\
        \frac{\partial \ell}{\partial W_2}(\theta, y)\\
        \frac{\partial \ell}{\partial b_1}(\theta, y)\\
        \frac{\partial \ell}{\partial b_2}(\theta, y)
    \end{pmatrix}
\end{split}\]</div>
<p><strong>Gradient descent</strong> is an optimization procedure where we start with an initial estimate for our parameters, <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span>. We then repeatedly apply the following update to get <span class="math notranslate nohighlight">\(\theta^{(1)}, \theta^{(2)}, \ldots\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla_\theta \ell(\theta^{(t)}, y)
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\alpha\)</span> is a learning rate (typically a small positive number, also sometimes called a step size), and <span class="math notranslate nohighlight">\(y\)</span> is the data we observed. In <strong>stochastic gradient descent</strong>, instead of computing the gradient using all of our data, we divide our data into batches, and at each iteration, compute the gradient on one batch in sequence.</p>
<p>This means that we must compute the gradient at every single iteration. So, anything we can do to compute gradients faster and more efficiently will make our entire optimization process faster and more efficient.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/2g9dRaB6_XA"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="gradients-and-backpropagation">
<h2>Gradients and Backpropagation<a class="headerlink" href="#gradients-and-backpropagation" title="Link to this heading">#</a></h2>
<p>Backpropagation is an algorithm for efficiently computing gradients by applying the chain rule in an order designed to avoid redundant computation. To see how it works, we’ll consider a very simple loss function of three variables. We’ll compute the gradient manually using the chain rule, and then we’ll see how backpropagation can do the same computation more efficiently.</p>
<section id="computing-gradients-with-the-chain-rule">
<h3>Computing gradients with the chain rule<a class="headerlink" href="#computing-gradients-with-the-chain-rule" title="Link to this heading">#</a></h3>
<p>Consider a very simple loss function involving three variables, <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, and <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="math notranslate nohighlight">
\[
L(a, b, c) = (a + 3b)c^2
\]</div>
<p>We can compute the partial derivatives with respect to <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, and <span class="math notranslate nohighlight">\(c\)</span>. To make it a little clearer when and where we’re using the chain rule, let <span class="math notranslate nohighlight">\(q = a+3b\)</span> and <span class="math notranslate nohighlight">\(r = c^2\)</span>, so that <span class="math notranslate nohighlight">\(L = qr\)</span>. The partial derivatives are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial a} &amp;= \frac{\partial L}{\partial q}\cdot\frac{\partial q}{\partial a} = c^2 \cdot 1 \\
\frac{\partial L}{\partial b} &amp;= \frac{\partial L}{\partial q}\cdot\frac{\partial q}{\partial b} = c^2 \cdot 3 \\
\frac{\partial L}{\partial c} &amp;= \frac{\partial L}{\partial r}\cdot\frac{\partial r}{\partial c} = (a+3b) \cdot 2c
\end{align*}
\end{split}\]</div>
<p>Even in this simple example, we can see that there was some redundant work involved: in doing this computation, we needed to compute <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial q} = c^2\)</span> twice. In a more complicated expression, especially one with many nested function calls, the redundant work would become much worse. Backpropagation gives us a way to compute these gradients more efficiently.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/79p0iipN-_g"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/79p0iipN-_g"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="backpropagation-an-example">
<h3>Backpropagation: an example<a class="headerlink" href="#backpropagation-an-example" title="Link to this heading">#</a></h3>
<p>Instead of representing the computation as an algebraic expression, let’s express it as a computation graph. This is a visual representation of the mathematical expression:</p>
<p><img alt="" src="content/chapters/03/computation_graph.png" /></p>
<p>Given specific numerical values for <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, and <span class="math notranslate nohighlight">\(c\)</span>, backpropagation is an efficient way to compute the loss and the gradient (i.e., all the partial derivatives), with no redundant computation.</p>
<p>We start by computing the loss itself. This involves just doing the computations specified by the graph, denoted by the blue numbers above the arrows:</p>
<p><img alt="" src="../../../_images/backprop_forward.jpg" /></p>
<p>Next, let’s notice that when we did the calculations in the previous section to find the gradient, most of our expressions started at the loss, then, using the chain rule, computed partial derivatives with respect to things like <span class="math notranslate nohighlight">\(q\)</span> and <span class="math notranslate nohighlight">\(r\)</span>. Let’s try to write these partial derivatives on the graph, and see if we can use them to keep working backwards.</p>
<ol class="arabic simple">
<li><p>First, we’ll start with the easiest derivative, the derivative of the loss with respect to itself: <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial L}\)</span>. This is just 1!</p></li>
<li><p>Next, we’ll compute the derivative of the loss with respect to <span class="math notranslate nohighlight">\(q\)</span> (top right branch of the graph). How did we get from <span class="math notranslate nohighlight">\(q\)</span> to <span class="math notranslate nohighlight">\(L\)</span>? We multiplied by 16 (that is, for these specific numbers, <span class="math notranslate nohighlight">\(L = 16q\)</span>). So, the partial derivative of <span class="math notranslate nohighlight">\(L\)</span> with respect to <span class="math notranslate nohighlight">\(q\)</span> is just 16.</p></li>
<li><p>Continuing along the top part of the graph, now we can compute the derivative with respect to <span class="math notranslate nohighlight">\(a\)</span>. How did we get from <span class="math notranslate nohighlight">\(a\)</span> to <span class="math notranslate nohighlight">\(q\)</span>? We added 9 (that is, for these specific numbers, <span class="math notranslate nohighlight">\(q = a + 9\)</span>). So, the partial derivative of <span class="math notranslate nohighlight">\(q\)</span> with respect to <span class="math notranslate nohighlight">\(a\)</span> is just 1. But we’re trying to compute <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial a}\)</span>, not <span class="math notranslate nohighlight">\(\frac{\partial q}{\partial a}\)</span>. So, we’ll take advantage of the chain rule and multiply by the “derivative so far”: that’s just <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial q}\)</span> = 16. So, our answer is <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial a} = 1 \cdot 16 = 16\)</span>.</p></li>
<li><p>Next, we’ll look at the <span class="math notranslate nohighlight">\(b\)</span> branch of the graph. From similar reasoning to above, the derivative at the output of the “multiply by three” block is just 16. How do we use that to compute the derivative with respect to <span class="math notranslate nohighlight">\(b\)</span>? To get from <span class="math notranslate nohighlight">\(b\)</span> to that value, we multiplied by 3. So, the corresponding term in the chain rule is 3. We multiply that with what we have so far, 16, to get 48.</p></li>
<li><p>Finally, let’s look at the <span class="math notranslate nohighlight">\(c\)</span> branch at the bottom of the graph. We’ll start by computing the derivative with respect to <span class="math notranslate nohighlight">\(r\)</span>. Similar to step 2 above, we multiplied <span class="math notranslate nohighlight">\(r\)</span> by 11 to get <span class="math notranslate nohighlight">\(L\)</span>, so that means that the derivative is 11.</p></li>
<li><p>All we have left is to go through the “square” block. The derivative of its output with respect to its input is two times the input (in other words, <span class="math notranslate nohighlight">\(\frac{\partial r}{\partial c} = 2c\)</span>). Since the input was 4, that means our new term is 8, and our overall derivative on this branch is <span class="math notranslate nohighlight">\(11 \cdot 8 = 88\)</span>.</p></li>
</ol>
<p>Now we’re done! We’ve computed the derivatives, as shown in this completed graph with the backpropagation intermediate and final results in red below the arrows:</p>
<p><img alt="" src="../../../_images/backprop_filled.jpg" /></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/PqOz2vsfL14"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="backpropagation">
<h3>Backpropagation<a class="headerlink" href="#backpropagation" title="Link to this heading">#</a></h3>
<p>In general, all we need to successfully run backpropagation is the ability to differentiate every mathematical building block of our loss (don’t forget, the loss depends on the prediction). For every building block, we need to know how to compute the forward pass (the mathematical operation, like addition, multiplication, squaring, etc.) and the backward pass (multiplying by the derivative).</p>
</section>
<section id="optional-backpropagation-in-pytorch">
<h3>(Optional) Backpropagation in pytorch<a class="headerlink" href="#optional-backpropagation-in-pytorch" title="Link to this heading">#</a></h3>
<p>Let’s see what this looks like in code using pytorch. We start by defining tensors for a, b, and c: tensors are the basic datatype of pytorch, much like arrays in numpy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Torch tensors are like numpy arrays</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We then define tensors for q and r. Note that each one contains both the value computed as well as the necessary operation to compute the gradient in the backward pass:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">b</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">c</span> <span class="o">**</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(11., grad_fn=&lt;AddBackward0&gt;) tensor(16., grad_fn=&lt;PowBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Finally, we define our loss:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">r</span>
<span class="n">L</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(176., grad_fn=&lt;MulBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Now that we’ve computed the loss, we can have PyTorch run backpropagation and compute all the derivatives with the <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the gradient for each input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(16.) tensor(48.) tensor(88.)
</pre></div>
</div>
</div>
</div>
<p>We can see that the results match up precisely with what we computed above manually!</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/GBwLIjFF3Rg"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/chapters/03"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="06_nonparametric.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Nonparametric methods</p>
      </div>
    </a>
    <a class="right-next"
       href="../04/intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 4: Causal Inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-feed-forward-neural-network">A feed-forward neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-stochastic-gradient-descent">Review: Stochastic gradient descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-and-backpropagation">Gradients and Backpropagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-gradients-with-the-chain-rule">Computing gradients with the chain rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-an-example">Backpropagation: an example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-backpropagation-in-pytorch">(Optional) Backpropagation in pytorch</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Data 102 Staff
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>