
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Model Checking and Evaluation &#8212; Data, Inference, and Decisions</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapters/03/04_model_checking';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Uncertainty Quantification" href="05_uncertainty_quantification.html" />
    <link rel="prev" title="Generalized Linear Models" href="03_glms.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data, Inference, and Decisions</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01/intro.html">Chapter 1: Binary Decision-Making</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01/01_decisions_and_errors.html">Binary Decision-Making and Error Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/02_hypothesis_testing.html">Hypothesis Testing and p-Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/03_multiple_tests.html">Multiple Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/04_binary_classification.html">Binary Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/05_decision_theory.html">Decision Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02/intro.html">Chapter 2: Bayesian modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02/01_parameter_estimation.html">Parameter Estimation and Bayesian Inference Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/02_hierarchical_models.html">Hierarchical Bayesian Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/03_graphical_models.html">Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/04_inference.html">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/05_inference_with_sampling.html">Bayesian Inference with Sampling</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Chapter 3: Prediction</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_prediction.html">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_regression_review.html">Linear Regression Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Model Checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_uncertainty_quantification.html">Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_nonparametric.html">Nonparametric Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_neural_networks.html">Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04/intro.html">Chapter 4: Causal Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04/01_association_correlation_causation.html">Understanding Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/02_quantifying_association.html">Quantifying Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/03_causality_potential_outcomes.html">Causality and Potential Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/04_randomized_experiments.html">Causality in Randomized Experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/05_observational_studies_unconfoundedness.html">Causality in Observational Studies: Unconfoundedness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/06_instrumental_variables.html">Causality in Observational Studies: Natural Experiments</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../05/intro.html">Chapter 5: Tail Bounds and Concentration Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../05/01_concentration.html">Tail Bounds and Concentration Inequalities</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/03/04_model_checking.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/chapters/03/04_model_checking.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Checking and Evaluation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-model-checking">Frequentist Model Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-comparison-with-log-likelihood">Model comparison with log-likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-with-chi-squared-statistic">Model evaluation with chi-squared statistic</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-behind-the-chi-squared-statistic">Intuition behind the chi-squared statistic</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-checking">Bayesian Model Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-distributions">Posterior Predictive Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-ppd-to-check-regression">Using the PPD to check regression</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="model-checking-and-evaluation">
<h1>Model Checking and Evaluation<a class="headerlink" href="#model-checking-and-evaluation" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">YouTubeVideo</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pymc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">arviz</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">az</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">bambi</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">bmb</span>

<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Turn off logging (console output) for PyMC</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;pymc&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>When fitting a model to data, it’s important to ask the question: <strong>is this model a good representation of the data</strong>?</p>
<p>In this section, we’ll answer this question from frequentist and Bayesian perspectives.</p>
<p>We’ll consider model evaluation from two viewpoints:</p>
<ol class="arabic simple">
<li><p>Are our models a good fit for the data we used to fit them (in the case of prediction, our training data)?</p></li>
<li><p>Will our models (particularly prediction models) generalize well to new, previously unseen data?</p></li>
</ol>
<p>When formulating and fitting a model, it’s important to answer both of these questions. In practice, it’s often easier to answer (and debug/iterate) the first question before moving on to the second.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">YouTubeVideo</span><span class="p">(</span><span class="s1">&#39;RlQNAjvm55A&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/RlQNAjvm55A"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<section id="frequentist-model-checking">
<h2>Frequentist Model Checking<a class="headerlink" href="#frequentist-model-checking" title="Link to this heading">#</a></h2>
<p>In the frequentist setting, we often use <strong>goodness-of-fit</strong> measures to evaluate our models. Let’s return to the three different models we used to predict the number of turbines in Oklahoma:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load and preprocess the data</span>
<span class="n">turbines</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;turbines.csv&#39;</span><span class="p">)</span>
<span class="c1"># The &quot;year&quot; column contains how many years since the year 2000</span>
<span class="n">turbines</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">turbines</span><span class="p">[</span><span class="s1">&#39;p_year&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2000</span>
<span class="n">turbines</span> <span class="o">=</span> <span class="n">turbines</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;p_year&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Turbines in Oklahoma from 2000 on</span>
<span class="n">ok_filter</span> <span class="o">=</span> <span class="p">(</span><span class="n">turbines</span><span class="o">.</span><span class="n">t_state</span> <span class="o">==</span> <span class="s1">&#39;OK&#39;</span><span class="p">)</span>
<span class="n">ok_turbines</span> <span class="o">=</span> <span class="n">turbines</span><span class="p">[</span><span class="n">ok_filter</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">)</span>

<span class="n">ok_turbines</span> <span class="o">=</span> <span class="n">ok_turbines</span><span class="p">[</span><span class="n">ok_turbines</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;totals&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;t_built&quot;</span><span class="p">])</span>
<span class="c1"># Log-transform the counts, too</span>
<span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;log_totals&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;totals&quot;</span><span class="p">])</span>
<span class="n">ok_turbines</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>t_state</th>
      <th>t_built</th>
      <th>t_cap</th>
      <th>year</th>
      <th>totals</th>
      <th>log_totals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>361</th>
      <td>OK</td>
      <td>1</td>
      <td>100.0</td>
      <td>1.0</td>
      <td>1</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>362</th>
      <td>OK</td>
      <td>113</td>
      <td>176250.0</td>
      <td>3.0</td>
      <td>114</td>
      <td>4.736198</td>
    </tr>
    <tr>
      <th>363</th>
      <td>OK</td>
      <td>182</td>
      <td>298200.0</td>
      <td>5.0</td>
      <td>296</td>
      <td>5.690359</td>
    </tr>
    <tr>
      <th>364</th>
      <td>OK</td>
      <td>40</td>
      <td>60000.0</td>
      <td>6.0</td>
      <td>336</td>
      <td>5.817111</td>
    </tr>
    <tr>
      <th>365</th>
      <td>OK</td>
      <td>85</td>
      <td>154500.0</td>
      <td>7.0</td>
      <td>421</td>
      <td>6.042633</td>
    </tr>
    <tr>
      <th>366</th>
      <td>OK</td>
      <td>91</td>
      <td>141900.0</td>
      <td>8.0</td>
      <td>512</td>
      <td>6.238325</td>
    </tr>
    <tr>
      <th>367</th>
      <td>OK</td>
      <td>153</td>
      <td>299100.0</td>
      <td>9.0</td>
      <td>665</td>
      <td>6.499787</td>
    </tr>
    <tr>
      <th>368</th>
      <td>OK</td>
      <td>195</td>
      <td>352260.0</td>
      <td>10.0</td>
      <td>860</td>
      <td>6.756932</td>
    </tr>
    <tr>
      <th>369</th>
      <td>OK</td>
      <td>257</td>
      <td>524900.0</td>
      <td>11.0</td>
      <td>1117</td>
      <td>7.018402</td>
    </tr>
    <tr>
      <th>370</th>
      <td>OK</td>
      <td>596</td>
      <td>1127050.0</td>
      <td>12.0</td>
      <td>1713</td>
      <td>7.446001</td>
    </tr>
    <tr>
      <th>371</th>
      <td>OK</td>
      <td>369</td>
      <td>648100.0</td>
      <td>14.0</td>
      <td>2082</td>
      <td>7.641084</td>
    </tr>
    <tr>
      <th>372</th>
      <td>OK</td>
      <td>710</td>
      <td>1399960.0</td>
      <td>15.0</td>
      <td>2792</td>
      <td>7.934513</td>
    </tr>
    <tr>
      <th>373</th>
      <td>OK</td>
      <td>602</td>
      <td>1457525.0</td>
      <td>16.0</td>
      <td>3394</td>
      <td>8.129764</td>
    </tr>
    <tr>
      <th>374</th>
      <td>OK</td>
      <td>323</td>
      <td>850725.0</td>
      <td>17.0</td>
      <td>3717</td>
      <td>8.220672</td>
    </tr>
    <tr>
      <th>375</th>
      <td>OK</td>
      <td>272</td>
      <td>543245.0</td>
      <td>18.0</td>
      <td>3989</td>
      <td>8.291296</td>
    </tr>
    <tr>
      <th>376</th>
      <td>OK</td>
      <td>33</td>
      <td>100050.0</td>
      <td>19.0</td>
      <td>4022</td>
      <td>8.299535</td>
    </tr>
    <tr>
      <th>377</th>
      <td>OK</td>
      <td>440</td>
      <td>1120550.0</td>
      <td>20.0</td>
      <td>4462</td>
      <td>8.403352</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gaussian_model_intercept</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">totals</span><span class="p">),</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">year</span><span class="p">),</span>
    <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">gaussian_results</span> <span class="o">=</span> <span class="n">gaussian_model_intercept</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gaussian_results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                 totals   No. Observations:                   17
Model:                            GLM   Df Residuals:                       15
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                          1.1810
Method:                          IRLS   Log-Likelihood:                -24.472
Date:                Tue, 28 Oct 2025   Deviance:                       17.716
Time:                        03:33:22   Pearson chi2:                     17.7
No. Iterations:                     3   Pseudo R-squ. (CS):             0.9131
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          3.2602      0.590      5.526      0.000       2.104       4.417
year           0.3023      0.047      6.435      0.000       0.210       0.394
==============================================================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poisson_model_freq</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span>
    <span class="n">ok_turbines</span><span class="o">.</span><span class="n">totals</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">year</span><span class="p">),</span>
    <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">poisson_results</span> <span class="o">=</span> <span class="n">poisson_model_freq</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">poisson_results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                 totals   No. Observations:                   17
Model:                            GLM   Df Residuals:                       15
Model Family:                 Poisson   Df Model:                            1
Link Function:                    Log   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -755.42
Date:                Tue, 28 Oct 2025   Deviance:                       1366.3
Time:                        03:33:22   Pearson chi2:                 1.20e+03
No. Iterations:                     5   Pseudo R-squ. (CS):              1.000
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          4.9697      0.023    219.386      0.000       4.925       5.014
year           0.1829      0.001    132.547      0.000       0.180       0.186
==============================================================================
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">negbin_model_freq</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span>
    <span class="n">ok_turbines</span><span class="o">.</span><span class="n">totals</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">year</span><span class="p">),</span>
    <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">negbin_results</span> <span class="o">=</span> <span class="n">negbin_model_freq</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">negbin_results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                 totals   No. Observations:                   17
Model:                            GLM   Df Residuals:                       15
Model Family:        NegativeBinomial   Df Model:                            1
Link Function:                    Log   Scale:                          1.0000
Method:                          IRLS   Log-Likelihood:                -134.14
Date:                Tue, 28 Oct 2025   Deviance:                       7.1483
Time:                        03:33:22   Pearson chi2:                     1.90
No. Iterations:                    11   Pseudo R-squ. (CS):             0.6999
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          4.2059      0.544      7.725      0.000       3.139       5.273
year           0.2389      0.043      5.514      0.000       0.154       0.324
==============================================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/statsmodels/genmod/families/family.py:1367: ValueWarning: Negative binomial dispersion parameter alpha not set. Using default value alpha=1.0.
  warnings.warn(&quot;Negative binomial dispersion parameter alpha not &quot;
</pre></div>
</div>
</div>
</div>
<section id="model-comparison-with-log-likelihood">
<h3>Model comparison with log-likelihood<a class="headerlink" href="#model-comparison-with-log-likelihood" title="Link to this heading">#</a></h3>
<p>The first way we’ll compare these three models is by looking at the <strong>log-likelihood</strong>, <span class="math notranslate nohighlight">\(\log(p(y|\beta, x))\)</span>. We’ve already seen this quantity before when we looked at simpler frequentist models. It describes how likely the observed <span class="math notranslate nohighlight">\(y\)</span>-values are, given particular coefficients <span class="math notranslate nohighlight">\(\beta\)</span> and predictors <span class="math notranslate nohighlight">\(x\)</span>. For</p>
<p>For each of these three models, the specific form of the likelihood function is different:</p>
<ol class="arabic simple">
<li><p>The first log-likelihood describes the <strong>density</strong> of observing the log-transformed <span class="math notranslate nohighlight">\(y\)</span>-values, assuming linear regression</p></li>
<li><p>The second log-likelihood describes the <strong>probability</strong> of observing the original <span class="math notranslate nohighlight">\(y\)</span>-values as counts, assuming a Poisson model with an exponential inverse link function</p></li>
<li><p>The third log-likelihood describes the <strong>probability</strong> of observing the original <span class="math notranslate nohighlight">\(y\)</span>-values as counts, assuming a negative binomial model with an exponential inverse link function</p></li>
</ol>
<p>So, we can interpret the log-likelihood as: “how likely are the data, according to a specific model?”</p>
<p>Note that because of two key differences (continuous vs discrete likelihoods and log-transformed data), we cannot directly compare the log-likelihood from the first model with the second two. But, we can compare the second two log-likelihoods directly, since they each describe how likely <strong>the same observations</strong> <span class="math notranslate nohighlight">\(y_1, \ldots, y_n\)</span> are, according to two different models. Just as we saw with maximum likelihood, it is reasonable to assume that the model with a higher log-likelihood is a better fit for the data.</p>
<p>Comparing the models, we see that the Poisson model has a log-likelihood of -755, while the negative binomial model has a log-likelihood around -134. We also know that because of how the models are fit, that these represent the best possible log-likelihood values for each model family. So, we can reasonably conclude that under a negative binomial GLM, the data are much more likely than they are under a Poisson GLM.</p>
<p>Intuitively, this aligns with what we saw in the Bayesian setting: because the variance of the Poisson likelihood is too low, values far away from the predicted average <span class="math notranslate nohighlight">\(\hat{y}\)</span> are very unlikely, and so the model assigns low probability to the turbine counts that we actually observed.</p>
<p>Note that the log-likelihood for any model by itself does not tell us anything about that model’s performance. Rather, it is useful as a <strong>relative measure when comparing different models applied to the same dataset.</strong> So, in general, the log-likelihood tells us how well a model does on a particular dataset, and can be used to compare how well different models fit the same dataset.</p>
</section>
<section id="model-evaluation-with-chi-squared-statistic">
<h3>Model evaluation with chi-squared statistic<a class="headerlink" href="#model-evaluation-with-chi-squared-statistic" title="Link to this heading">#</a></h3>
<p>We can also use the chi-squared (<span class="math notranslate nohighlight">\(\chi^2\)</span>) statistic to evaluate models: here, larger values are better. While the full details are beyond the scope of this class, in general, we should expect the Chi-squared statistic to roughly be equal to the number of data points minus the number of parameters. In the examples above, we have 17 data points (observations), and two parameters (slope and intercept), so we should expect a value close to 15.</p>
<p>We can see that the linear model (with log-transformed <span class="math notranslate nohighlight">\(y\)</span>-values) has a Chi-squared value of 17.7, fairly close to 15, indicating that the model fits the data well. The Poisson model, on the other hand, has a much higher value of 1200, indicating a very poor fit.</p>
<section id="intuition-behind-the-chi-squared-statistic">
<h4>Intuition behind the chi-squared statistic<a class="headerlink" href="#intuition-behind-the-chi-squared-statistic" title="Link to this heading">#</a></h4>
<p><em>Coming soon: see video</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">YouTubeVideo</span><span class="p">(</span><span class="s1">&#39;wTNETHYrUl8&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/wTNETHYrUl8"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
</section>
<section id="bayesian-model-checking">
<h2>Bayesian Model Checking<a class="headerlink" href="#bayesian-model-checking" title="Link to this heading">#</a></h2>
<section id="posterior-predictive-distributions">
<h3>Posterior Predictive Distributions<a class="headerlink" href="#posterior-predictive-distributions" title="Link to this heading">#</a></h3>
<p>Consider a model with observed data points <span class="math notranslate nohighlight">\(x_{1:n} = x_1, \ldots, x_n\)</span> and an unknown parameter, with a likelihood <span class="math notranslate nohighlight">\(p(x_{1:n}|\theta)\)</span> and a prior <span class="math notranslate nohighlight">\(p(\theta)\)</span>. As usual, we’ll assume that the data points are conditionally i.i.d. given <span class="math notranslate nohighlight">\(\theta\)</span>. So far, we’ve focused on obtaining the posterior distribution <span class="math notranslate nohighlight">\(p(\theta|x_{1:n})\)</span>.</p>
<p>We can also consider the distribution for a new data point, given all the previous data points (after marginalizing out the unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span>). This is called the <strong>posterior predictive distribution (PPD)</strong>, and will be our primary tool for model checking:</p>
<div class="math notranslate nohighlight">
\[
p(x_{n+1} | x_1, \ldots, x_n)
\]</div>
<p>How do we go about computing the PPD? Looking at the previous paragraph, we know more about the distribution of <span class="math notranslate nohighlight">\(x_{n+1}\)</span> given <span class="math notranslate nohighlight">\(\theta\)</span>, so we can use total probability:</p>
<div class="math notranslate nohighlight">
\[
p(x_{n+1}|x_{1:n}) = \int p(x_{n+1}|x_{1:n}, \theta)p(\theta|x_{1:n})\, d\theta
\]</div>
<p>We can simplify the first conditional distribution because <span class="math notranslate nohighlight">\(x_{n+1}\)</span> and <span class="math notranslate nohighlight">\(x_{1:n}\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(\theta\)</span>: in other words, <span class="math notranslate nohighlight">\(p(x_{n+1}|x_{1:n}, \theta) = p(x_{n+1}|\theta)\)</span>. The second conditional distribution is the posterior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>. So:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(x_{n+1}|x_{1:n}) 
    &amp;= \int p(x_{n+1}|x_{1:n}, \theta)p(\theta|x_{1:n})\, d\theta \\
    &amp;= \int p(x_{n+1}|\theta)p(\theta|x_{1:n})\, d\theta \\
    &amp;= \mathbb{E}_{\theta|x_{1:n}}\left[p(x_{n+1}|\theta)\right]
\end{align*}
\end{split}\]</div>
<p>In practice, computing this integral exactly is just as hard as computing the denominator in Bayes’ rule, because we have to integrate out over all possible <span class="math notranslate nohighlight">\(\theta\)</span>. In the case of high-dimensional <span class="math notranslate nohighlight">\(\theta\)</span>, as before, the integral can be impossible! In practice, we usually approximate the posterior with samples, and then use that to approximate the PPD.</p>
<p>Specifically, to compute the PPD for a new data point, we compute the expectation above using the empirical distribution for <span class="math notranslate nohighlight">\(\theta|x_{1:n}\)</span> as defined by the samples.</p>
<p>In other words, given samples <span class="math notranslate nohighlight">\(\theta^{(1)}, \ldots, \theta^{(t)}\)</span> that approximate <span class="math notranslate nohighlight">\(p(\theta|x_{1:n})\)</span>, we can:</p>
<ul class="simple">
<li><p><strong>compute the PPD for a specific value</strong> <span class="math notranslate nohighlight">\(x_{n+1}\)</span> by averaging <span class="math notranslate nohighlight">\((1/t)\sum_{i=1}^t p(x_{n+1}|\theta^{(t)})\)</span></p></li>
<li><p><strong>draw a sample from the PPD</strong> by:</p>
<ul>
<li><p>first, drawing a sample uniformly at random from <span class="math notranslate nohighlight">\(\theta^{(1)}, \ldots, \theta^{(t)}\)</span>, calling it <span class="math notranslate nohighlight">\(\theta^*\)</span>;</p></li>
<li><p>and then drawing from the likelihood conditioned on that sample, <span class="math notranslate nohighlight">\(p(x_{n+1}|\theta^*)\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="using-the-ppd-to-check-regression">
<h3>Using the PPD to check regression<a class="headerlink" href="#using-the-ppd-to-check-regression" title="Link to this heading">#</a></h3>
<p>In a Bayesian GLM, our observed data points are the <span class="math notranslate nohighlight">\(y\)</span>-values <span class="math notranslate nohighlight">\(y_1, \ldots, y_n\)</span>. So, the PPD is <span class="math notranslate nohighlight">\(p(y_{n+1} | x_{n+1}, y_1, \ldots, y_n)\)</span>. Note that we condition on the corresponding <span class="math notranslate nohighlight">\(x\)</span>-value because of the nature of a prediction/regression problem. <span class="math notranslate nohighlight">\(x\)</span> is fixed and known, and <span class="math notranslate nohighlight">\(y_{n+1}\)</span> will depend on <span class="math notranslate nohighlight">\(x_{n+1}\)</span>. Another way of seeing this is by trying to answer the question of “how would you predict <span class="math notranslate nohighlight">\(y_{n+1}\)</span> without knowing <span class="math notranslate nohighlight">\(x_{n+1}\)</span>, and would that prediction even be useful?”</p>
<p>PyMC handles all the details of setting up the posterior predictive check for us, so we can define the following function to help us visualizes the samples that it draws. The most important line is <code class="docutils literal notranslate"><span class="pre">model.predict(trace,</span> <span class="pre">kind='response')</span></code>, which tells PyMC to generate predictions for each <span class="math notranslate nohighlight">\(x\)</span>-value by sampling from the posterior predictive distribution.</p>
<p><em>Exercise: what are the other possible values of <code class="docutils literal notranslate"><span class="pre">kind=</span></code> that you can pass to the <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code> function, and what do they do?</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">show_posterior_predictive</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">trace</span><span class="p">,</span> <span class="n">turbines_df</span><span class="p">,</span> 
    <span class="n">is_log_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_lines</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">turbines_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Adds posterior predictive samples to the trace</span>
    <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;response&#39;</span><span class="p">)</span>

    <span class="n">y_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">response_component</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">name</span>
    <span class="c1"># a num_samples x 17 array with num_samples different</span>
    <span class="c1"># predictions from the PPC</span>
    <span class="n">pred_arr</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="n">y_name</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_log_y</span><span class="p">:</span>
        <span class="n">pred_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">pred_arr</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_lines</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">turbines_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">pred_arr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span>
        <span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">turbines_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">turbines_df</span><span class="p">[</span><span class="s1">&#39;totals&#39;</span><span class="p">])</span>
    <span class="c1"># This value is hard-coded to help us see the PPCs for all 3</span>
    <span class="c1"># models on the same scale</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30000</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s use this function to visualize a PPC for the three models we looked at in the previous section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, we need to set up and draw posterior samples for each model</span>
<span class="n">gaussian_model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;log_totals ~ year&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">ok_turbines</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">)</span>
<span class="n">gaussian_trace</span> <span class="o">=</span> <span class="n">gaussian_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">progressbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">poisson_model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;totals ~ year&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">ok_turbines</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;poisson&#39;</span><span class="p">)</span>
<span class="n">poisson_trace</span> <span class="o">=</span> <span class="n">poisson_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">progressbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">negbin_model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;totals ~ year&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">ok_turbines</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;negativebinomial&#39;</span><span class="p">)</span>
<span class="n">negbin_trace</span> <span class="o">=</span> <span class="n">negbin_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">progressbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Now, visualize the PPC using the function we just defined</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">show_posterior_predictive</span><span class="p">(</span><span class="n">gaussian_model</span><span class="p">,</span> <span class="n">gaussian_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">is_log_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Linear regression (log-xfm y)&#39;</span><span class="p">)</span>

<span class="n">show_posterior_predictive</span><span class="p">(</span><span class="n">poisson_model</span><span class="p">,</span> <span class="n">poisson_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">is_log_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Poisson regression&#39;</span><span class="p">)</span>

<span class="n">show_posterior_predictive</span><span class="p">(</span><span class="n">negbin_model</span><span class="p">,</span> <span class="n">negbin_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">is_log_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Negative binomial regression&#39;</span><span class="p">)</span>

<span class="n">f</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Posterior predictive check: three methods for turbine prediction&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pytensor/link/c/cmodule.py:2986: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.
This usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.
Alternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == &#39;NUMBA&#39;` or passing `mode=&#39;NUMBA&#39;` when compiling a PyTensor function.
For more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library
  warnings.warn(
</pre></div>
</div>
<img alt="../../../_images/dd1e96e437e9ac262078bbff50d1b9474cd37e206976bbd5ad386b69574ef8c4.png" src="../../../_images/dd1e96e437e9ac262078bbff50d1b9474cd37e206976bbd5ad386b69574ef8c4.png" />
</div>
</div>
<p>Each green line represents one sample from the PPD. We can think of these as possible sequences of data that are dreamt up by the models, based on their representation.</p>
<p>Looking at the results, we can immediately make a few observations:</p>
<ul class="simple">
<li><p>We can see again that Poisson regression is overconfident: every single one of the PPD samples is on (or extremely close to) the regression line, with little observed variability around it.</p></li>
<li><p>The other two models both have trajectories that go up and down, while the true pattern is monotonically nondecreasing. This is a weakness of our models: specifically, we assume that the <span class="math notranslate nohighlight">\(y\)</span>-values are conditionally i.i.d.</p></li>
<li><p>The samples from the linear regression have an enormous amount of variability: we can see that many of them make predictions well over 15,000 turbines starting in 2015, which is far more than the actual observed values. This tells us that the model allows for too much variability.</p></li>
<li><p>The negative binomial model has a much more reasonable range of values, at least before 2018. One could imagine that under different political and economic circumstances, Oklahoma might have built more or fewer turbines in accordance with these predictions. While the model still has a little too much variability, it’s still much more reasonable than the</p></li>
</ul>
<p>Note that all of the above required some subjective evaluation and a bit of domain knowledge! In many cases, where we have not just one observed <span class="math notranslate nohighlight">\(y\)</span>-value per unique value of <span class="math notranslate nohighlight">\(x\)</span> but instead a cloud of points, we can produce quantitative evaluations from the PPC, such as comparing the variance of the observed <span class="math notranslate nohighlight">\(y\)</span>-values to the variance of the PPC samples, but the details of such procedures are beyond the scope of this class.</p>
<p><em>Exercise: Recall that the outlier in 2000 (<span class="math notranslate nohighlight">\(t=0\)</span>) affected our predictions, likely causing the slope to be too high across all three models. Try fitting the three models above after removing that outlier from the dataset. Do the results change? How and why?</em></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/chapters/03"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_glms.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Generalized Linear Models</p>
      </div>
    </a>
    <a class="right-next"
       href="05_uncertainty_quantification.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Uncertainty Quantification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-model-checking">Frequentist Model Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-comparison-with-log-likelihood">Model comparison with log-likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-with-chi-squared-statistic">Model evaluation with chi-squared statistic</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-behind-the-chi-squared-statistic">Intuition behind the chi-squared statistic</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-checking">Bayesian Model Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-distributions">Posterior Predictive Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-ppd-to-check-regression">Using the PPD to check regression</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Data 102 Staff
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>