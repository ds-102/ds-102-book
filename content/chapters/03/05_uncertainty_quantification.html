
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Uncertainty Quantification &#8212; Data, Inference, and Decisions</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapters/03/05_uncertainty_quantification';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Nonparametric methods" href="06_nonparametric.html" />
    <link rel="prev" title="Model Checking and Evaluation" href="04_model_checking.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data, Inference, and Decisions</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01/intro.html">Chapter 1: Binary Decision-Making</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01/01_decisions_and_errors.html">Binary Decision-Making and Error Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/02_hypothesis_testing.html">Hypothesis Testing and p-Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/03_multiple_tests.html">Multiple Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/04_binary_classification.html">Binary Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/05_decision_theory.html">Decision Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02/intro.html">Chapter 2: Bayesian modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02/01_parameter_estimation.html">Parameter Estimation and Bayesian Inference Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/02_hierarchical_models.html">Hierarchical Bayesian Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/03_graphical_models.html">Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/04_inference.html">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/05_inference_with_sampling.html">Bayesian Inference with Sampling</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Chapter 3: Prediction</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_prediction.html">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_regression_review.html">Linear Regression Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_model_checking.html">Model Checking</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_nonparametric.html">Nonparametric Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_neural_networks.html">Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04/intro.html">Chapter 4: Causal Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04/01_association_correlation_causation.html">Understanding Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/02_quantifying_association.html">Quantifying Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/03_causality_potential_outcomes.html">Causality and Potential Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/04_randomized_experiments.html">Causality in Randomized Experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/05_observational_studies_unconfoundedness.html">Causality in Observational Studies: Unconfoundedness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/06_instrumental_variables.html">Causality in Observational Studies: Natural Experiments</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/03/05_uncertainty_quantification.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/chapters/03/05_uncertainty_quantification.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Uncertainty Quantification</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-uncertainty-quantification">Frequentist Uncertainty Quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uq-with-the-central-limit-theorem">UQ with the Central Limit Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uq-with-the-bootstrap">UQ with the Bootstrap</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap-limitations">Bootstrap limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">Confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-in-glms-comparing-coefficient-and-prediction-intervals">Uncertainty in GLMs: comparing coefficient and prediction intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bootstrap-to-quantify-uncertainty-in-glms">Using Bootstrap to Quantify Uncertainty in GLMs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-uncertainty-quantification">Bayesian Uncertainty Quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#credible-intervals">Credible Intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-hdis-from-samples">Constructing HDIs from samples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-uq-for-predictions">Bayesian UQ for Predictions</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="uncertainty-quantification">
<h1>Uncertainty Quantification<a class="headerlink" href="#uncertainty-quantification" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pymc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">arviz</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">az</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">bambi</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">bmb</span>

<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Turn off logging (console output) for PyMC</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;pymc&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>(Almost) every dataset we work with has some amount of uncertainty in it. Everything from the randomness in a sample, measurement error, differences between the data and the underlying truth they represent, and much more can lead to uncertainty.</p>
<p>Therefore, anything we <strong>compute</strong> using the data also inherits some uncertainty. So, it’s important to always report that uncertainty:</p>
<ul class="simple">
<li><p>If we infer a parameter or trend, we should report our uncertainty in the inference.</p></li>
<li><p>If we make a prediction, we should report our uncertainty in the prediction.</p></li>
<li><p>If we draw a conclusion, we should report our uncertainty in the conclusion.</p></li>
</ul>
<p>While uncertainty quantification looks slightly different from frequentist and Bayesian perspectives, the core ideas above remain true regardless.</p>
<section id="frequentist-uncertainty-quantification">
<h2>Frequentist Uncertainty Quantification<a class="headerlink" href="#frequentist-uncertainty-quantification" title="Link to this heading">#</a></h2>
<p>In the frequentist setting, we treat our data as random, but our unknowns as fixed. So, any uncertainty we quantify is only over the data! We <strong>cannot</strong> make statements like “<em>The unknown is probably within</em>” or “<em>The probability of the parameter being..</em>”.</p>
<p>While these statements can be more intuitive and might align with what we want to say when quantifying uncertainty, they only make sense in the context of a probability model for the unknown: we’ll have to take a Bayesian approach in order to say anything of this sort.</p>
<p>The main way we’ll quantify uncertainty in the frequentist setting is by using the distribution of our estimators. Specifically, whenever we estimate some parameter <span class="math notranslate nohighlight">\(\theta\)</span> with an estimator <span class="math notranslate nohighlight">\(\hat{\theta} = f(x_1, \ldots, x_n)\)</span>, the estimator is random, because it depends on the random data (even if <span class="math notranslate nohighlight">\(\theta\)</span> is fixed). So, we can use the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> to quantify uncertainty.</p>
<p>We already know that if we have the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, we can construct a <strong>confidence interval</strong>, which gives us an idea of how much our estimator could vary if we had observed different datasets. But before we can construct one, we need to determine the distribution for <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/ZDqSXtm0vBM"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<section id="uq-with-the-central-limit-theorem">
<h3>UQ with the Central Limit Theorem<a class="headerlink" href="#uq-with-the-central-limit-theorem" title="Link to this heading">#</a></h3>
<p><em>For more on the Central Limit Theorem, see <a class="reference external" href="https://data140.org/textbook/content/Chapter_14/00_The_Central_Limit_Theorem.html">Chapter 14 of the Data 140 textbook</a>.</em></p>
<p>If we want to use the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> to quantify our uncertainty, then we have to start by answering: “what is the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>?</p>
<p>In the case where <span class="math notranslate nohighlight">\(\theta\)</span> is the sample mean, we can answer this with one of the most beautiful results in statistics: the <strong>Central Limit Theorem</strong>. Briefly, this states that if <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is the sample mean of data points <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>, where each <span class="math notranslate nohighlight">\(x_i\)</span> has mean <span class="math notranslate nohighlight">\(\mu\)</span> and (finite) variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, then the distribution of <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> converges to a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2/n\)</span>.</p>
<p>Note that a similar result holds for any maximum likelihood estimate! While the details are beyond the scope of this class, one can show that if <span class="math notranslate nohighlight">\(\hat{\theta}_{MLE}\)</span> is the maximum likelihood estimate for some parameter <span class="math notranslate nohighlight">\(\theta\)</span> based on data points <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>, then the distribution for <span class="math notranslate nohighlight">\(\hat{\theta}_{MLE}\)</span> <strong>also</strong> converges to a normal distribution, with mean <span class="math notranslate nohighlight">\(\theta\)</span> and variance <span class="math notranslate nohighlight">\(1/(nI(\theta))\)</span>. Here, <span class="math notranslate nohighlight">\(I(\theta)\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Fisher_information">Fisher informatoin</a> of the likelihood, which quantifies how much information each <span class="math notranslate nohighlight">\(x_i\)</span> gives us about <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>While the details (e.g., Fisher information) are beyond the scope of this class, the takeaway is important:</p>
<p>Asymptotically (i.e., as the number of observations gets very large), <strong>the distribution of a maximum likelihood estimator, like that of a sample mean, converges to a normal distribution</strong>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/TiqwK0YYrNo"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="uq-with-the-bootstrap">
<h3>UQ with the Bootstrap<a class="headerlink" href="#uq-with-the-bootstrap" title="Link to this heading">#</a></h3>
<p><em>You may find it helpful to review <a class="reference external" href="https://inferentialthinking.com/chapters/13/2/Bootstrap.html">Chapter 13.2 of the Data 8 textbook</a>, which covers the bootstrap.</em></p>
<p>In many cases, we might not be able to compute the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> analytically. In some of these cases, we can use the <strong>bootstrap</strong> to quantify uncertainty.</p>
<section id="bootstrap-limitations">
<h4>Bootstrap limitations<a class="headerlink" href="#bootstrap-limitations" title="Link to this heading">#</a></h4>
<p>The bootstrap works well in most, but not all situations. Here are some guidelines for when to (and when not to) use it:</p>
<ul class="simple">
<li><p>The bootstrap is a <strong>good</strong> choice when the presence or absence of a single sample won’t change the estimate dramatically (e.g., this is not true for the min or max)</p></li>
<li><p>The bootstrap is a <strong>good</strong> choice when the number of data points is very small</p></li>
<li><p>The bootstrap is a <strong>good</strong> choice when the number of parameters <span class="math notranslate nohighlight">\(d\)</span> being estimated is much smaller than the number of data points <span class="math notranslate nohighlight">\(n\)</span> (e.g., this is not true for neural networks, where we are estimating a very large number of parameters)</p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/hmQAc5VMBDY"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/ZQfrPKruYLg"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
<section id="confidence-intervals">
<h3>Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Link to this heading">#</a></h3>
<p><em>You may find it helpful to review <a class="reference external" href="https://inferentialthinking.com/chapters/13/3/Confidence_Intervals.html">Section 13.3 of the Data 8 textbook</a>, which covers confidence intervals.</em></p>
<p>A confidence interval provides a measure of uncertainty about our estimator, based on its distribution. Recall that confidence intervals provide a guarantee about the process, not about the location of the (fixed) unknown parameter. Specifically, thinking about a 95% confidence interval, we know that 95% of datasets will produce an interval that contains the true parameter. We don’t know whether our interval happens to be one of the lucky 95%, or one of the unlucky 5%.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/Zn9NqVID-qg"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="uncertainty-in-glms-comparing-coefficient-and-prediction-intervals">
<h3>Uncertainty in GLMs: comparing coefficient and prediction intervals<a class="headerlink" href="#uncertainty-in-glms-comparing-coefficient-and-prediction-intervals" title="Link to this heading">#</a></h3>
<p>When quantifying uncertainty in GLMs, it’s important to keep in mind that there are multiple sources of uncertainty. For example, consider our estimate of the coefficients, <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>, computed from data <span class="math notranslate nohighlight">\((x_1, y_1), \ldots, (x_n, y_n)\)</span>. These are computed as the maximum likelihood estimates given the observed data, and so have inherent uncertainty in them. In other words, if we had observed a different dataset, we might have gotten a different set of estimated coefficients <span class="math notranslate nohighlight">\(\hat{\beta}\)</span>.</p>
<p>Now, consider our estimated prediction for a new data point, <span class="math notranslate nohighlight">\(\hat{y}_{n+1}\)</span>. To simplify, we’ll focus on linear regression with a scalar <span class="math notranslate nohighlight">\(x\)</span>. In this case, we can write the distribution for the new value <span class="math notranslate nohighlight">\(y_{n+1}\)</span> as:
$<span class="math notranslate nohighlight">\(
\hat{y}_{n+1} \sim \mathcal{N}\left(\hat{\beta}_0 + \hat{\beta}_1 x_{n+1}\,,\, \sigma^2\right)
\)</span><span class="math notranslate nohighlight">\(
Here, there are two sources of uncertainty: (1) the uncertainty in our estimator for the coefficients, \)</span>\hat{\beta}<span class="math notranslate nohighlight">\(; and (2) the uncertainty in the observed value and how far away it'll be from the average prediction (i.e., prediction line), as quantified by \)</span>\sigma$.</p>
<p>So, if we want to construct a confidence interval for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span>, we need only consider the first source of uncertainty. But, if we want to construct a confidence interval for <span class="math notranslate nohighlight">\(\hat{y}_{n+1}\)</span>, we need to consider <strong>both</strong> sources of uncertainty! This second type of interval is often called a <strong>prediction interval</strong>.</p>
</section>
<section id="using-bootstrap-to-quantify-uncertainty-in-glms">
<h3>Using Bootstrap to Quantify Uncertainty in GLMs<a class="headerlink" href="#using-bootstrap-to-quantify-uncertainty-in-glms" title="Link to this heading">#</a></h3>
<p><em>Text coming soon: see video</em></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/73I1wfQfmf4"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
<section id="bayesian-uncertainty-quantification">
<h2>Bayesian Uncertainty Quantification<a class="headerlink" href="#bayesian-uncertainty-quantification" title="Link to this heading">#</a></h2>
<p>In the Bayesian setting, we treat our unknown parameters as random. This means that we can make statements like “The unknown is probably between A and B”, which we couldn’t in the frequentist setting.</p>
<section id="credible-intervals">
<h3>Credible Intervals<a class="headerlink" href="#credible-intervals" title="Link to this heading">#</a></h3>
<p>This motivates the definition of a <strong>credible interval</strong>. A <span class="math notranslate nohighlight">\(p\%\)</span> credible interval for a parameter <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\((a, b)\)</span>, says that there is a <span class="math notranslate nohighlight">\(p\%\)</span> chance that, given the observed data, the parameter <span class="math notranslate nohighlight">\(\theta\)</span> falls between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. For example, suppose we produce <span class="math notranslate nohighlight">\(90\%\)</span> confidence interval for <span class="math notranslate nohighlight">\(\theta\)</span> of <span class="math notranslate nohighlight">\((0.3, 1.7)\)</span>. This tells us that given our observed data, the probability of <span class="math notranslate nohighlight">\(\theta\)</span> being between 0.3 and 1.7 is <span class="math notranslate nohighlight">\(0.9\)</span>, or equivalently <span class="math notranslate nohighlight">\(\int_{0.3}^{1.7} p(\theta|x_1, \ldots, x_n) d\theta = 0.9\)</span> (assuming <span class="math notranslate nohighlight">\(\theta\)</span> is continuous).</p>
<p>Notice how much simpler this definition is than the confidence interval! This is one of the advantages of taking a Bayesian approach: treating our unknown as random lets us make intuitive statements about the probability that it takes on certain values.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/nGeZ7G34jPI"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<p>However, one problem with this definition is that it is <strong>not unique</strong>! Indeed, there are many possible credible intervals for a given level of confidence/credibility. Consider the following distribution, and the three possible <span class="math notranslate nohighlight">\(80\%\)</span> credible intervals. For all three, the total shaded blue area is <span class="math notranslate nohighlight">\(80\%\)</span>, so all three are valid <span class="math notranslate nohighlight">\(80\%\)</span> credible intervals. Which one would you prefer?</p>
<p><img alt="" src="../../../_images/credible_interval_comparison.png" /></p>
<p>The one in the center seems the most appealing: it covers an area of highest density, and is also the narrowest. This motivates the definition of a <strong>highest density interval (HDI)</strong>, sometimes also called a highest posterior density (HPD) interval. The HDI is the narrowest interval for a given level of credibility/confidence.</p>
<p><em>Exercise: is the HDI always unique? If not, then what constraints could we place on the posterior to ensure it’s unique?</em></p>
<p>We can see that the intervals we saw in Section 3.3 were HDIs that PyMC automatically constructed for us.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/8ozFH1ZN7Qw"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="constructing-hdis-from-samples">
<h3>Constructing HDIs from samples<a class="headerlink" href="#constructing-hdis-from-samples" title="Link to this heading">#</a></h3>
<p>Given a posterior in closed form, we could analytically find <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> to minimize the width of the interval. But in practice, we rarely obtain posterior distributions in closed form: we usually end up approximating them with samples. So, how might we find an HDI with samples?</p>
<p>Let’s do a simple example with some samples from a Beta-distributed random variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>
<span class="n">samples</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.38210197, 0.80401855, 0.40002045, 0.53932934, 0.5861046 ])
</pre></div>
</div>
</div>
</div>
<p>Call the samples <span class="math notranslate nohighlight">\(\theta_1, \ldots, \theta_{100}\)</span>. We can construct a <span class="math notranslate nohighlight">\(90\%\)</span> credible interval by sorting the samples, choosing any <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> so that <span class="math notranslate nohighlight">\((j-i)/200 = 0.9\)</span>, and reporting <span class="math notranslate nohighlight">\([\theta_i, \theta_j]\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorted_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">190</span><span class="p">)</span>
<span class="p">(</span><span class="n">i2</span><span class="p">,</span> <span class="n">j2</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">195</span><span class="p">)</span>
<span class="n">credible_interval_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">sorted_samples</span><span class="p">[</span><span class="n">i1</span><span class="p">],</span> <span class="n">sorted_samples</span><span class="p">[</span><span class="n">j1</span><span class="p">])</span>
<span class="n">credible_interval_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">sorted_samples</span><span class="p">[</span><span class="n">i2</span><span class="p">],</span> <span class="n">sorted_samples</span><span class="p">[</span><span class="n">j2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">credible_interval_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">credible_interval_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(np.float64(0.15333018166217993), np.float64(0.7523258844091404))
(np.float64(0.1704470636483196), np.float64(0.7925316880638207))
</pre></div>
</div>
</div>
</div>
<p>How can we find the HDI? We can simply search over all intervals of width <span class="math notranslate nohighlight">\(200 \times 0.9 = 180\)</span>, and find the narrowest one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">credibility</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">narrowest_start_so_far</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">narrowest_width_so_far</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
<span class="n">interval_samples</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_samples</span> <span class="o">*</span> <span class="n">credibility</span><span class="p">)</span>
<span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span> <span class="c1"># Make sure you understand why we can stop at 20!</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">interval_samples</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">sorted_samples</span><span class="p">[</span><span class="n">end</span><span class="p">]</span> <span class="o">-</span> <span class="n">sorted_samples</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">width</span> <span class="o">&lt;</span> <span class="n">narrowest_width_so_far</span><span class="p">:</span>
        <span class="n">narrowest_start_so_far</span> <span class="o">=</span> <span class="n">start</span>
        <span class="n">narrowest_width_so_far</span> <span class="o">=</span> <span class="n">width</span>
<span class="nb">print</span><span class="p">((</span>
    <span class="n">sorted_samples</span><span class="p">[</span><span class="n">narrowest_start_so_far</span><span class="p">],</span>  
    <span class="n">sorted_samples</span><span class="p">[</span><span class="n">narrowest_start_so_far</span> <span class="o">+</span> <span class="n">interval_samples</span><span class="p">]</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(np.float64(0.15333018166217993), np.float64(0.7523258844091404))
</pre></div>
</div>
</div>
</div>
</section>
<section id="bayesian-uq-for-predictions">
<h3>Bayesian UQ for Predictions<a class="headerlink" href="#bayesian-uq-for-predictions" title="Link to this heading">#</a></h3>
<p>Just as in the frequentist world, in Bayesian GLMs, we can also construct credible intervals for our predictions! These will be constructed using the posterior predictive density, and inherit the uncertainty in the inference for the unknown coefficients as well as the uncertainty in the inference for parameters like <span class="math notranslate nohighlight">\(\sigma\)</span> in linear regression.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/9wtfEgpIn8k"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/chapters/03"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_model_checking.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Model Checking and Evaluation</p>
      </div>
    </a>
    <a class="right-next"
       href="06_nonparametric.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Nonparametric methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-uncertainty-quantification">Frequentist Uncertainty Quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uq-with-the-central-limit-theorem">UQ with the Central Limit Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uq-with-the-bootstrap">UQ with the Bootstrap</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bootstrap-limitations">Bootstrap limitations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">Confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty-in-glms-comparing-coefficient-and-prediction-intervals">Uncertainty in GLMs: comparing coefficient and prediction intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bootstrap-to-quantify-uncertainty-in-glms">Using Bootstrap to Quantify Uncertainty in GLMs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-uncertainty-quantification">Bayesian Uncertainty Quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#credible-intervals">Credible Intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-hdis-from-samples">Constructing HDIs from samples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-uq-for-predictions">Bayesian UQ for Predictions</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Data 102 Staff
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>