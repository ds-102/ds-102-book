
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Generalized Linear Models &#8212; Data, Inference, and Decisions</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapters/03/03_glms';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Model Checking and Evaluation" href="04_model_checking.html" />
    <link rel="prev" title="Regression review" href="02_regression_review.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data, Inference, and Decisions</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01/intro.html">Chapter 1: Binary Decision-Making</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01/01_decisions_and_errors.html">Binary Decision-Making and Error Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/02_hypothesis_testing.html">Hypothesis Testing and p-Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/03_multiple_tests.html">Multiple Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/04_binary_classification.html">Binary Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/05_decision_theory.html">Decision Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02/intro.html">Chapter 2: Bayesian modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02/01_parameter_estimation.html">Parameter Estimation and Bayesian Inference Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/02_hierarchical_models.html">Hierarchical Bayesian Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/03_graphical_models.html">Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/04_inference.html">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/05_inference_with_sampling.html">Bayesian Inference with Sampling</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Chapter 3: Prediction</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_prediction.html">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_regression_review.html">Linear Regression Review</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_model_checking.html">Model Checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_uncertainty_quantification.html">Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_nonparametric.html">Nonparametric Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_neural_networks.html">Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04/intro.html">Chapter 4: Causal Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04/01_association_correlation_causation.html">Understanding Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/02_quantifying_association.html">Quantifying Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/03_causality_potential_outcomes.html">Causality and Potential Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/04_randomized_experiments.html">Causality in Randomized Experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/05_observational_studies_unconfoundedness.html">Causality in Observational Studies: Unconfoundedness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/06_instrumental_variables.html">Causality in Observational Studies: Natural Experiments</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../05/intro.html">Chapter 5: Tail Bounds and Concentration Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../05/01_concentration.html">Tail Bounds and Concentration Inequalities</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/03/03_glms.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/chapters/03/03_glms.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Generalized Linear Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-from-bayesian-and-frequentist-perspectives">Linear Regression from Bayesian and frequentist perspectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbine-data-linear-regression-frequentist-approach">Turbine Data Linear Regression: Frequentist Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbine-data-linear-regression-bayesian-approach">Turbine Data Linear Regression: Bayesian Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#where-are-the-priors">Where are the Priors?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-summary">Linear regression summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-linear-regression-glms">Beyond Linear Regression: GLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">Poisson regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-binomial-regression">Negative binomial regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Generalized Linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glm-workflow">GLM Workflow</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">pymc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">bambi</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">bmb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">arviz</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">az</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="generalized-linear-models">
<h1>Generalized Linear Models<a class="headerlink" href="#generalized-linear-models" title="Link to this heading">#</a></h1>
<p>In this section, we’ll learn about an extension to linear regression called <strong>generalized linear models</strong>, or GLMs. In particular, GLMs are similar to linear regression, with two important extensions:</p>
<ol class="arabic simple">
<li><p>Instead of using the result of <span class="math notranslate nohighlight">\(X\beta\)</span> as the average prediction, we’ll apply a nonlinear function called the inverse link function or <span class="math notranslate nohighlight">\(g^{-1}\)</span> first, so that our average prediction becomes <span class="math notranslate nohighlight">\(g^{-1}(X\beta)\)</span>. While we can use any arbitrary function here, we’ll see several examples that are particularly useful.</p></li>
<li><p>Instead of assuming a normal distribution around the average prediction as our model for the likelihood of the data, we’ll allow for arbitrary likelihood distributions (but still centered around the average prediction <span class="math notranslate nohighlight">\(g^{-1}(X\beta)\)</span>).</p></li>
</ol>
<p>We’ll work through an example that demonstrates why these kinds of models are useful, and how choosing different inverse link functions and likelihood models can change the prediction results we get.</p>
<section id="linear-regression-from-bayesian-and-frequentist-perspectives">
<h2>Linear Regression from Bayesian and frequentist perspectives<a class="headerlink" href="#linear-regression-from-bayesian-and-frequentist-perspectives" title="Link to this heading">#</a></h2>
<p>For the rest of this section, we’ll work with a dataset that contains the number of wind turbines built in each state since the year 2000, focusing on Oklahoma. It contains the following columns:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">t_built</span></code>: the number of turbines built each year</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">t_cap</span></code>: the power capacity added that year</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">year</span></code>: the year, stored as the number of years since 2000</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">totals</span></code>: the total number of turbines in the state built since 2000</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_totals</span></code>: the log of the total.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">turbines</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;turbines.csv&#39;</span><span class="p">)</span>
<span class="c1"># The &quot;year&quot; column contains how many years since the year 2000</span>
<span class="n">turbines</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">turbines</span><span class="p">[</span><span class="s1">&#39;p_year&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2000</span>
<span class="n">turbines</span> <span class="o">=</span> <span class="n">turbines</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;p_year&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">turbines</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="n">ok_filter</span> <span class="o">=</span> <span class="p">(</span><span class="n">turbines</span><span class="o">.</span><span class="n">t_state</span> <span class="o">==</span> <span class="s1">&#39;OK&#39;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">turbines</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Turbines in Oklahoma from 2000 on</span>
<span class="n">ok_filter</span> <span class="o">=</span> <span class="p">(</span><span class="n">turbines</span><span class="o">.</span><span class="n">t_state</span> <span class="o">==</span> <span class="s1">&#39;OK&#39;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">turbines</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">ok_turbines</span> <span class="o">=</span> <span class="n">turbines</span><span class="p">[</span><span class="n">ok_filter</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">)</span>
<span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;totals&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;t_built&quot;</span><span class="p">])</span>
<span class="c1"># Log-transform the counts, too</span>
<span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;log_totals&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ok_turbines</span><span class="p">[</span><span class="s2">&quot;totals&quot;</span><span class="p">])</span>
<span class="n">ok_turbines</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>t_state</th>
      <th>t_built</th>
      <th>t_cap</th>
      <th>year</th>
      <th>totals</th>
      <th>log_totals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>361</th>
      <td>OK</td>
      <td>1</td>
      <td>100.0</td>
      <td>1.0</td>
      <td>1</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>362</th>
      <td>OK</td>
      <td>113</td>
      <td>176250.0</td>
      <td>3.0</td>
      <td>114</td>
      <td>4.736198</td>
    </tr>
    <tr>
      <th>363</th>
      <td>OK</td>
      <td>182</td>
      <td>298200.0</td>
      <td>5.0</td>
      <td>296</td>
      <td>5.690359</td>
    </tr>
    <tr>
      <th>364</th>
      <td>OK</td>
      <td>40</td>
      <td>60000.0</td>
      <td>6.0</td>
      <td>336</td>
      <td>5.817111</td>
    </tr>
    <tr>
      <th>365</th>
      <td>OK</td>
      <td>85</td>
      <td>154500.0</td>
      <td>7.0</td>
      <td>421</td>
      <td>6.042633</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;totals&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/a9cc926a4d6c2c300dc63725c6e49df47863bb8f489bb9f81622cdd06e556b0b.png" src="../../../_images/a9cc926a4d6c2c300dc63725c6e49df47863bb8f489bb9f81622cdd06e556b0b.png" />
</div>
</div>
<p>From looking at this data, we can immediately see that linear regression might not be a good fit: the relationship between the two variables seems exponential rather than linear. We can address this in one of two ways:</p>
<ol class="arabic simple">
<li><p>Log-transform our output data, so that we’re predicting <span class="math notranslate nohighlight">\(\log(\)</span>turbine count<span class="math notranslate nohighlight">\()\)</span>, and then use a linear model.</p></li>
<li><p>Incorporate the exponential relationship into our model.</p></li>
</ol>
<p>We’ll start with the first approach and see that it does reasonably well, and then see how GLMs can help us do even better by taking the second approach.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;log_totals&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/539d7e073bc89cb05b8209c8863089db6044cea081490fa517bafa6842c615df.png" src="../../../_images/539d7e073bc89cb05b8209c8863089db6044cea081490fa517bafa6842c615df.png" />
</div>
</div>
<p>With the exception of the outlier at the first year (<span class="math notranslate nohighlight">\(t=0\)</span>, at the year 2000), a linear model seems like a good fit here.</p>
<p>Since linear regression is a statistical model where we are estimating an unknown (model coefficients) from random data, we can approach it from either a frequentist or Bayesian paradigm. Let’s see how a linear model for the log-counts will work under each of these settings.</p>
<section id="turbine-data-linear-regression-frequentist-approach">
<h3>Turbine Data Linear Regression: Frequentist Approach<a class="headerlink" href="#turbine-data-linear-regression-frequentist-approach" title="Link to this heading">#</a></h3>
<p>In the frequentist paradigm, we treat our unknown coefficients as fixed, and estimate them using maximum likelihood (or similar techniques). While using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> as we saw in the previous section was a perfectly valid way to implement linear regression in a frequentist paradigm, it doesn’t support many of the generalized linear models we’ll want to use throughout this chapter. So, we’ll instead use the <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> package in this chapter (imported using <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">statsmodels</span> <span class="pre">as</span> <span class="pre">sm</span></code>).</p>
<p>We’ll primarily use the <code class="docutils literal notranslate"><span class="pre">sm.GLM</span></code> class, which takes in an array or series for <code class="docutils literal notranslate"><span class="pre">y</span></code>, an array or dataframe for <code class="docutils literal notranslate"><span class="pre">X</span></code>, and a model family: we’ll explore more of these families later, but for now we’ll stick to OLS, which we can implement with <code class="docutils literal notranslate"><span class="pre">sm.families.Gaussian()</span></code>. In order include an intercept in our model, we need to augment our data by applying the <code class="docutils literal notranslate"><span class="pre">sm.add_constant()</span></code> function.</p>
<p>Here’s what the model above would look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gaussian_model_intercept</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">totals</span><span class="p">),</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">ok_turbines</span><span class="o">.</span><span class="n">year</span><span class="p">),</span>
    <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">gaussian_results</span> <span class="o">=</span> <span class="n">gaussian_model_intercept</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gaussian_results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                 totals   No. Observations:                   17
Model:                            GLM   Df Residuals:                       15
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                          1.1810
Method:                          IRLS   Log-Likelihood:                -24.472
Date:                Tue, 28 Oct 2025   Deviance:                       17.716
Time:                        03:32:55   Pearson chi2:                     17.7
No. Iterations:                     3   Pseudo R-squ. (CS):             0.9131
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          3.2602      0.590      5.526      0.000       2.104       4.417
year           0.3023      0.047      6.435      0.000       0.210       0.394
==============================================================================
</pre></div>
</div>
</div>
</div>
<p>The model output shows us some useful information about the model at the top, and then information about the estimated coefficient(s) at the bottom:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">coef</span></code>, the coefficient itself,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">err</span></code>, the <a class="reference external" href="https://en.wikipedia.org/wiki/Standard_error">standard error</a> (i.e., standard deviation of the estimator)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">z</span></code>, the <span class="math notranslate nohighlight">\(z\)</span>-score for the coefficient for a hypothesis test where the null hypothesis states that it is 0,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">P&gt;|z|</span></code>, the <span class="math notranslate nohighlight">\(p\)</span>-value for the hypothesis test above, and</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">[0.025</span></code> and <code class="docutils literal notranslate"><span class="pre">0.975]</span></code>, the lower and upper bounds for a <span class="math notranslate nohighlight">\(95\%\)</span> confidence interval for the estimated coefficient.</p></li>
</ul>
<p>This model tells us that we can predict the log of the number of turbines <span class="math notranslate nohighlight">\(N\)</span> in any year <span class="math notranslate nohighlight">\(t\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
\log(N) = 3.26 + 0.30 \times t
\]</div>
<p>In most cases, we’re more interested in the actual turbine count, rather than the log: so, we can exponentiate both sides to obtain:</p>
<div class="math notranslate nohighlight">
\[
N = e^{3.26} e^{0.3t}
\]</div>
<p>This also tells us that according to this model, each year, the number of turbines is multiplied by <span class="math notranslate nohighlight">\(e^{0.3}\)</span>, or roughly <span class="math notranslate nohighlight">\(1.35\)</span>.</p>
<p>We can also visualize this prediction:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ok_turbines</span><span class="p">[</span><span class="s1">&#39;pred_freq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">3.2602</span> <span class="o">+</span> <span class="mf">0.3023</span> <span class="o">*</span> <span class="n">ok_turbines</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ok_turbines</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">ok_turbines</span><span class="p">[</span><span class="s1">&#39;totals&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual count&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ok_turbines</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">ok_turbines</span><span class="p">[</span><span class="s1">&#39;pred_freq&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted count&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/55dd1cd7db0f81d61e465606836a4f80b1ac7da3bcf24e75c7ff06e37c633bc0.png" src="../../../_images/55dd1cd7db0f81d61e465606836a4f80b1ac7da3bcf24e75c7ff06e37c633bc0.png" />
</div>
</div>
<p>We can see that the predictions from this model are reasonable from 2000-2016, but then start to diverge from reality after that.</p>
</section>
<section id="turbine-data-linear-regression-bayesian-approach">
<h3>Turbine Data Linear Regression: Bayesian Approach<a class="headerlink" href="#turbine-data-linear-regression-bayesian-approach" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ok_turbines</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>t_state</th>
      <th>t_built</th>
      <th>t_cap</th>
      <th>year</th>
      <th>totals</th>
      <th>log_totals</th>
      <th>pred_freq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>361</th>
      <td>OK</td>
      <td>1</td>
      <td>100.0</td>
      <td>1.0</td>
      <td>1</td>
      <td>0.000000</td>
      <td>35.251215</td>
    </tr>
    <tr>
      <th>362</th>
      <td>OK</td>
      <td>113</td>
      <td>176250.0</td>
      <td>3.0</td>
      <td>114</td>
      <td>4.736198</td>
      <td>64.528049</td>
    </tr>
    <tr>
      <th>363</th>
      <td>OK</td>
      <td>182</td>
      <td>298200.0</td>
      <td>5.0</td>
      <td>296</td>
      <td>5.690359</td>
      <td>118.119875</td>
    </tr>
    <tr>
      <th>364</th>
      <td>OK</td>
      <td>40</td>
      <td>60000.0</td>
      <td>6.0</td>
      <td>336</td>
      <td>5.817111</td>
      <td>159.812300</td>
    </tr>
    <tr>
      <th>365</th>
      <td>OK</td>
      <td>85</td>
      <td>154500.0</td>
      <td>7.0</td>
      <td>421</td>
      <td>6.042633</td>
      <td>216.220777</td>
    </tr>
    <tr>
      <th>366</th>
      <td>OK</td>
      <td>91</td>
      <td>141900.0</td>
      <td>8.0</td>
      <td>512</td>
      <td>6.238325</td>
      <td>292.539588</td>
    </tr>
    <tr>
      <th>367</th>
      <td>OK</td>
      <td>153</td>
      <td>299100.0</td>
      <td>9.0</td>
      <td>665</td>
      <td>6.499787</td>
      <td>395.796425</td>
    </tr>
    <tr>
      <th>368</th>
      <td>OK</td>
      <td>195</td>
      <td>352260.0</td>
      <td>10.0</td>
      <td>860</td>
      <td>6.756932</td>
      <td>535.499523</td>
    </tr>
    <tr>
      <th>369</th>
      <td>OK</td>
      <td>257</td>
      <td>524900.0</td>
      <td>11.0</td>
      <td>1117</td>
      <td>7.018402</td>
      <td>724.513214</td>
    </tr>
    <tr>
      <th>370</th>
      <td>OK</td>
      <td>596</td>
      <td>1127050.0</td>
      <td>12.0</td>
      <td>1713</td>
      <td>7.446001</td>
      <td>980.242510</td>
    </tr>
    <tr>
      <th>371</th>
      <td>OK</td>
      <td>369</td>
      <td>648100.0</td>
      <td>14.0</td>
      <td>2082</td>
      <td>7.641084</td>
      <td>1794.353376</td>
    </tr>
    <tr>
      <th>372</th>
      <td>OK</td>
      <td>710</td>
      <td>1399960.0</td>
      <td>15.0</td>
      <td>2792</td>
      <td>7.934513</td>
      <td>2427.701005</td>
    </tr>
    <tr>
      <th>373</th>
      <td>OK</td>
      <td>602</td>
      <td>1457525.0</td>
      <td>16.0</td>
      <td>3394</td>
      <td>8.129764</td>
      <td>3284.599481</td>
    </tr>
    <tr>
      <th>374</th>
      <td>OK</td>
      <td>323</td>
      <td>850725.0</td>
      <td>17.0</td>
      <td>3717</td>
      <td>8.220672</td>
      <td>4443.954890</td>
    </tr>
    <tr>
      <th>375</th>
      <td>OK</td>
      <td>272</td>
      <td>543245.0</td>
      <td>18.0</td>
      <td>3989</td>
      <td>8.291296</td>
      <td>6012.524565</td>
    </tr>
    <tr>
      <th>376</th>
      <td>OK</td>
      <td>33</td>
      <td>100050.0</td>
      <td>19.0</td>
      <td>4022</td>
      <td>8.299535</td>
      <td>8134.747659</td>
    </tr>
    <tr>
      <th>377</th>
      <td>OK</td>
      <td>440</td>
      <td>1120550.0</td>
      <td>20.0</td>
      <td>4462</td>
      <td>8.403352</td>
      <td>11006.045591</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In the Bayesian paradigm, we treat our unknown coefficients as random, and compute the posterior distribution over them, conditioned on the data we observe. While we could implement the entire model in PyMC, the Bambi package provides a convenient layer on top of PyMC that lets us specify our models with less code. We’ll use the <code class="docutils literal notranslate"><span class="pre">bmb.Model</span></code> class, which takes a formula, a dataframe, and a model family (for now, we’ll stick with the <code class="docutils literal notranslate"><span class="pre">gaussian</span></code> family for OLS). The formula takes the form <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">~</span> <span class="pre">predictor1</span> <span class="pre">+</span> <span class="pre">predictor2</span> <span class="pre">+</span> <span class="pre">...</span></code>, where each variables is a column names from the provided dataframe. Here’s how we implement the model from above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The y-values are in the column `log_totals`, and the x-values are in the column `year`. So:</span>
<span class="n">gaussian_model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;log_totals ~ year&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">ok_turbines</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">)</span>
<span class="n">gaussian_trace</span> <span class="o">=</span> <span class="n">gaussian_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pytensor/link/c/cmodule.py:2986: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.
This usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.
Alternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == &#39;NUMBA&#39;` or passing `mode=&#39;NUMBA&#39;` when compiling a PyTensor function.
For more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (2 chains in 2 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [sigma, Intercept, year]
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 1 seconds.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We recommend running at least 4 chains for robust computation of convergence diagnostics
</pre></div>
</div>
</div>
</div>
<p>We can see the results using the <code class="docutils literal notranslate"><span class="pre">arviz</span></code> function <code class="docutils literal notranslate"><span class="pre">plot_posterior</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">gaussian_trace</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/999f120670f8906c027c2846f16e36a30e9bc99692dd9fb466895a9a6dcd7d2f.png" src="../../../_images/999f120670f8906c027c2846f16e36a30e9bc99692dd9fb466895a9a6dcd7d2f.png" />
</div>
</div>
<p>The first two rows each represent one coefficient. The third row, <code class="docutils literal notranslate"><span class="pre">log_totals_sigma</span></code>, is the estimated standard deviation of the data points around the average prediction. The left column shows the posterior distribution for that coefficient (by using <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_density_estimation">KDE</a> on the histogram of samples), and the right shows the trajectory of samples obtained during HMC.</p>
<p>Looking at the posterior distribution for the coefficient of <code class="docutils literal notranslate"><span class="pre">year</span></code> (second row), we see that the results are similar to the frequentist version: the MAP and LMSE estimates are  around 0.3, By examining the histogram, we should expect that under the posterior, this coefficient will usually be between <span class="math notranslate nohighlight">\(0.2\)</span> to <span class="math notranslate nohighlight">\(0.4\)</span>, with values outside this range fairly unlikely. This uncertainty is reasonable: since we’re only estimating these coefficients from seventeen data points, it makes sense to have some uncertainty in the coefficients we estimate. Just as before, we can interpret these estimates in the context of the data: each year, the number of turbines increases by a multiplicative factor of about <span class="math notranslate nohighlight">\(1.3\)</span>.</p>
<p>When we use sampling for inference, we can think of each sample as one regression line: each sample has a value of <code class="docutils literal notranslate"><span class="pre">intercept</span></code>, a value of the coefficient for <code class="docutils literal notranslate"><span class="pre">year</span></code>, and an estimated <span class="math notranslate nohighlight">\(\sigma\)</span> for the residuals. We can therefore plot each sample as a line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">identity</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_posterior_samples</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">turbines_df</span><span class="p">,</span> <span class="n">num_lines</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">show_logy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">slope</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="n">num_lines</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_logy</span><span class="p">:</span>
        <span class="n">y_func</span> <span class="o">=</span> <span class="n">identity</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_func</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">y_func</span><span class="p">(</span><span class="n">intercept</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">slope</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">turbines_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">turbines_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">turbines_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">y_func</span><span class="p">(</span><span class="n">turbines_df</span><span class="p">[</span><span class="s1">&#39;log_totals&#39;</span><span class="p">]))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Years since 2000&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_logy</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;log(turbine count)&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;turbine count&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_posterior_samples</span><span class="p">(</span><span class="n">gaussian_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">show_logy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/2107ddc3b00f3210ae0bfb9c5ff1e2cff6f12c6c10816641e53dd1abb5f6cd93.png" src="../../../_images/2107ddc3b00f3210ae0bfb9c5ff1e2cff6f12c6c10816641e53dd1abb5f6cd93.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>np.identity<span class="o">?</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_posterior_samples</span><span class="p">(</span><span class="n">gaussian_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">show_logy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/f93f56de204c34ad6e4a95151d7138e5e2f1b6f75b84c14b5b4646217192c5b3.png" src="../../../_images/f93f56de204c34ad6e4a95151d7138e5e2f1b6f75b84c14b5b4646217192c5b3.png" />
</div>
</div>
<p>These plots help us see that many of the lines have too high of a slope in the log-scale, likely due to the outlier at <span class="math notranslate nohighlight">\(t=0\)</span>. This corresponds to a too-fast exponential rise after 2016 when looking at the counts.</p>
<p>It’s important to note that this only shows us the uncertainty in the <strong>average</strong> prediction <code class="docutils literal notranslate"><span class="pre">intercept</span> <span class="pre">+</span> <span class="pre">coefficient</span> <span class="pre">*</span> <span class="pre">year</span></code>: it doesn’t show us the uncertainty in the variation around the average prediction. Specifically, remember that we obtain predictions in linear regression by assuming <span class="math notranslate nohighlight">\(y \sim \mathcal{N}(X\beta, \sigma^2 I)\)</span>. These lines show us the uncertainty in the average prediction <span class="math notranslate nohighlight">\(X\beta\)</span>, but don’t show us how large the estimated variance <span class="math notranslate nohighlight">\(\sigma\)</span> is: we’ll come back to that later.</p>
<p><strong>Exercise</strong>: <em>How might you construct similar plots to visualize the uncertainty for the frequentist results from earlier?</em></p>
<section id="where-are-the-priors">
<h4>Where are the Priors?<a class="headerlink" href="#where-are-the-priors" title="Link to this heading">#</a></h4>
<p>You may have noticed that we just implemented a Bayesian model, but never specified any priors. In this case, Bambi chooses “sensible” defaults for us, inspired by the R library <code class="docutils literal notranslate"><span class="pre">rstanarm</span></code>. These defaults are chosen to be weak, uninformative priors that don’t encode much information about what the coefficients should be.</p>
<p>You can read more about these default choices at the <a class="reference external" href="https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html#default-weakly-informative-prior-distributions"><code class="docutils literal notranslate"><span class="pre">rstanarm</span></code> documentation</a>.</p>
</section>
</section>
<section id="linear-regression-summary">
<h3>Linear regression summary<a class="headerlink" href="#linear-regression-summary" title="Link to this heading">#</a></h3>
<p>We saw that linear regression can be implemented in both frequentist and Bayesian paradigms: at the end of the day, both approaches give us estimated coefficients, along with some measure of uncertainty. We can use those coefficients to interpret the model (as we saw above), and to make predictions for new data points.</p>
<p>We saw that when making predictions with linear regression, we start by multiplying each feature by its corresponding coefficient, adding them all up to obtain an average prediction, and assuming some randomness around that average.</p>
<p>In order to determine the coefficients from data, we can take either a frequentist or Bayesian approach. In the frequentist paradigm, we estimated them using frequentist methods like MLE, and in the Bayesian paradigm we approximated the posterior distribution over the coefficients (conditioned on the observed data) using samples.</p>
<p>For the remainder of this chapter, we’ll move back and forth between frequentist and Bayesian paradigms to illustrate ideas in each one.</p>
</section>
</section>
<section id="beyond-linear-regression-glms">
<h2>Beyond Linear Regression: GLMs<a class="headerlink" href="#beyond-linear-regression-glms" title="Link to this heading">#</a></h2>
<p>The regression model above works somewhat well for years before 2016, but it doesn’t account for the fact that the variable we’re predicting is a whole number (meaning that it takes values <span class="math notranslate nohighlight">\(0, 1, 2, 3, \ldots\)</span>). When we say that <span class="math notranslate nohighlight">\(y|\beta \sim N(X\beta, \sigma^2 I)\)</span>, and use log-transformed data for <span class="math notranslate nohighlight">\(y\)</span>, we’re implicitly saying that <span class="math notranslate nohighlight">\(y\)</span> can never be 0. We’ll start by asking the question: can we use a different likelihood that is designed specifically for this kind of data? In answering this, we’ll explore two examples of generalized linear models: Poisson regression and negative binomial regression.</p>
<section id="poisson-regression">
<h3>Poisson regression<a class="headerlink" href="#poisson-regression" title="Link to this heading">#</a></h3>
<p><em>You may find it helpful to review <a class="reference external" href="http://prob140.org/textbook/content/Chapter_07/01_Poisson_Distribution.html">Section 7.1 of the Data 140 textbook</a>, which covers the Poisson distribution.</em></p>
<p>Recall that the <a class="reference external" href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> is a distribution over counts and count-like values. It has one <em>positive</em> parameter <span class="math notranslate nohighlight">\(\lambda\)</span> that represents its mean (and variance, too).</p>
<p>In Poisson regression, we’re going to assume a Poisson likelihood for each <span class="math notranslate nohighlight">\(y_i\)</span>, and we’ll use the linear combination <span class="math notranslate nohighlight">\(x_i^T\beta\)</span> to help determine the parameter. The Poisson distribution is only defined for positive values of the parameter, but <span class="math notranslate nohighlight">\(x_i^T\beta\)</span> could be negative. There are several ways to make the possibly-negative value into a positive one so we can use it for the parameter (e.g., take absolute value, square, etc.), but inspired by our log-transformation of the outputs in the previous section, we’ll exponentiate.</p>
<p>So, we’ll use <span class="math notranslate nohighlight">\(\exp(x_i^T \beta)\)</span> as the mean. We can write out our likelihood for the observed data points <span class="math notranslate nohighlight">\(y_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
y_i | \beta \sim \text{Poisson}(\exp(x_i^T \beta))
\]</div>
<p>As before, we can use this likelihood model to estimate the coefficients <span class="math notranslate nohighlight">\(\beta\)</span> in either a frequentist or Bayesian setting. In this section, we’ll choose to take a Bayesian approach, mostly so that we can take advantage of the function <code class="docutils literal notranslate"><span class="pre">plot_posterior_samples</span></code> that we wrote in the last section to help us visualize the uncertainty in our model.</p>
<p>Using Bambi, switching from linear regression to Poisson regression is quite straightforward:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">poisson_model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;totals ~ year&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">ok_turbines</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;poisson&#39;</span><span class="p">)</span>
<span class="n">poisson_trace</span> <span class="o">=</span> <span class="n">poisson_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (2 chains in 2 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [Intercept, year]
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 1 seconds.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We recommend running at least 4 chains for robust computation of convergence diagnostics
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">poisson_trace</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/74b69c829653d8e9105100cc9e7f3b3c8ecc17cba374f4cb34bc530b6dc4a908.png" src="../../../_images/74b69c829653d8e9105100cc9e7f3b3c8ecc17cba374f4cb34bc530b6dc4a908.png" />
</div>
</div>
<p>Comparing this to the results from the Gaussian model, we can see that the posterior distributions for the two coefficients are <strong>much</strong> narrower. On top of that, the values of the coefficient for <code class="docutils literal notranslate"><span class="pre">year</span></code> seem much smaller: whereas before the posterior was centered around <span class="math notranslate nohighlight">\(0.3\)</span>, now it looks like the entire distribution is narrowly distributed around 0.183.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.183</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(1.2008144080808307)
</pre></div>
</div>
</div>
</div>
<p>This corresponds to only a <span class="math notranslate nohighlight">\(20\%\)</span> average growth rate, compared to our <span class="math notranslate nohighlight">\(35\%\)</span> average growth rate from the earlier model. The lower growth rate seems reasonable: recall that the resulting curve(s) from linear regression grew too fast, especially after 2016. But, the degree of certainty in the results is concerning: given that we only have 17 data points, our results should have a lot more uncertainty in them. We can visualize how “overconfident” the model is by plotting each sample as a line/curve using the <code class="docutils literal notranslate"><span class="pre">plot_posterior_samples</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_posterior_samples</span><span class="p">(</span><span class="n">poisson_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">show_logy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/97d9ced774a5066b70338a27a72fe2c4ad747c3796fa797fea3e65bf6c05195e.png" src="../../../_images/97d9ced774a5066b70338a27a72fe2c4ad747c3796fa797fea3e65bf6c05195e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_posterior_samples</span><span class="p">(</span><span class="n">poisson_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">show_logy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/81a2488fed5b8f9a95a0a09afec393763884915d402a60d81d6ba85d8ae19608.png" src="../../../_images/81a2488fed5b8f9a95a0a09afec393763884915d402a60d81d6ba85d8ae19608.png" />
</div>
</div>
<p>Just as before, it’s important to remember that these lines only show us the estimated uncertainty in the average prediction <span class="math notranslate nohighlight">\(X\beta\)</span>: we aren’t visualizing any of the uncertainty shown by the Poisson model itself.</p>
<p>Why are the results so different?</p>
<p>Let’s think about the implicit assumptions we’re making when choosing a Poisson likelihood. In particular, one important property of the Poisson distribution is that its mean is equal to its variance. So, while the linear model could allow for an average prediction of <span class="math notranslate nohighlight">\(\log(N) = 4\)</span> with a variance of <span class="math notranslate nohighlight">\(1.5\)</span>, the Poisson distribution requires that the mean and variance must be the same.</p>
<p>This problem is compounded by the fact that the Poisson distribution is modeling the true counts rather than the log-counts: in other words, consider 2018, when the number of turbines was about 4000. A Poisson distribution with mean 4000 has a variance of 4000, or in other words, a standard deviation of only around 63.</p>
<p>Clearly, the Poisson is a poor choice for fitting this data! When a model assumes a lower variance than is actually present in the data, like this Poisson model does, we say that the data are <strong>overdispersed</strong>: this means that they’re too spread out relative to the model’s assumptions). In order to solve this problem, we should choose a different distribution that gives us the ability to control the variance as well as the mean.</p>
<p>This wasn’t a problem with the normal likelihood earlier: because the normal distribution has two separate parameters for mean and variance, we can choose them separately to reflect the fact that the variance may be higher than the mean.</p>
</section>
<section id="negative-binomial-regression">
<h3>Negative binomial regression<a class="headerlink" href="#negative-binomial-regression" title="Link to this heading">#</a></h3>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">negative binomial distribution</a> is also a distribution over counts, but it allows for more complexity than the Poisson distribution. We can think of it one of two ways:</p>
<ul class="simple">
<li><p>It’s the sum of <span class="math notranslate nohighlight">\(r\)</span> [Geometric random variables], each with parameter <span class="math notranslate nohighlight">\(p\)</span> (success probability).</p></li>
<li><p>It’s like a Poisson distribution if the mean parameter (<span class="math notranslate nohighlight">\(\lambda\)</span> above) were also random.</p></li>
</ul>
<p>There are several different ways to parametrize the negative binomial. How do we choose which one to use? There are two answers:</p>
<ol class="arabic simple">
<li><p>We want a parametrization that lets us choose the mean, since we want the mean value for <span class="math notranslate nohighlight">\(y_i\)</span> to be <span class="math notranslate nohighlight">\(\exp(x_i^T \beta)\)</span> (in other words, we want to do almost the same thing as we just did with Poisson regression, but we want to use a negative binomial distribution instead of a Poisson distribution for the observed values).</p></li>
<li><p>Since we’re using PyMC through Bambi, we’re limited to whichever parametrization(s) it supports.</p></li>
</ol>
<p>Even though the form of the distribution is significantly more complex and manipulating it involves more work, using it in our regression model requires only a tiny change to what we were doing before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">negbin_model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;totals ~ year&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">ok_turbines</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;negativebinomial&#39;</span><span class="p">)</span>
<span class="n">negbin_trace</span> <span class="o">=</span> <span class="n">negbin_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (2 chains in 2 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [alpha, Intercept, year]
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We recommend running at least 4 chains for robust computation of convergence diagnostics
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">4000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(63.245553203367585)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ok_turbines</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>t_state</th>
      <th>t_built</th>
      <th>t_cap</th>
      <th>year</th>
      <th>totals</th>
      <th>log_totals</th>
      <th>pred_freq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>361</th>
      <td>OK</td>
      <td>1</td>
      <td>100.0</td>
      <td>1.0</td>
      <td>1</td>
      <td>0.000000</td>
      <td>35.251215</td>
    </tr>
    <tr>
      <th>362</th>
      <td>OK</td>
      <td>113</td>
      <td>176250.0</td>
      <td>3.0</td>
      <td>114</td>
      <td>4.736198</td>
      <td>64.528049</td>
    </tr>
    <tr>
      <th>363</th>
      <td>OK</td>
      <td>182</td>
      <td>298200.0</td>
      <td>5.0</td>
      <td>296</td>
      <td>5.690359</td>
      <td>118.119875</td>
    </tr>
    <tr>
      <th>364</th>
      <td>OK</td>
      <td>40</td>
      <td>60000.0</td>
      <td>6.0</td>
      <td>336</td>
      <td>5.817111</td>
      <td>159.812300</td>
    </tr>
    <tr>
      <th>365</th>
      <td>OK</td>
      <td>85</td>
      <td>154500.0</td>
      <td>7.0</td>
      <td>421</td>
      <td>6.042633</td>
      <td>216.220777</td>
    </tr>
    <tr>
      <th>366</th>
      <td>OK</td>
      <td>91</td>
      <td>141900.0</td>
      <td>8.0</td>
      <td>512</td>
      <td>6.238325</td>
      <td>292.539588</td>
    </tr>
    <tr>
      <th>367</th>
      <td>OK</td>
      <td>153</td>
      <td>299100.0</td>
      <td>9.0</td>
      <td>665</td>
      <td>6.499787</td>
      <td>395.796425</td>
    </tr>
    <tr>
      <th>368</th>
      <td>OK</td>
      <td>195</td>
      <td>352260.0</td>
      <td>10.0</td>
      <td>860</td>
      <td>6.756932</td>
      <td>535.499523</td>
    </tr>
    <tr>
      <th>369</th>
      <td>OK</td>
      <td>257</td>
      <td>524900.0</td>
      <td>11.0</td>
      <td>1117</td>
      <td>7.018402</td>
      <td>724.513214</td>
    </tr>
    <tr>
      <th>370</th>
      <td>OK</td>
      <td>596</td>
      <td>1127050.0</td>
      <td>12.0</td>
      <td>1713</td>
      <td>7.446001</td>
      <td>980.242510</td>
    </tr>
    <tr>
      <th>371</th>
      <td>OK</td>
      <td>369</td>
      <td>648100.0</td>
      <td>14.0</td>
      <td>2082</td>
      <td>7.641084</td>
      <td>1794.353376</td>
    </tr>
    <tr>
      <th>372</th>
      <td>OK</td>
      <td>710</td>
      <td>1399960.0</td>
      <td>15.0</td>
      <td>2792</td>
      <td>7.934513</td>
      <td>2427.701005</td>
    </tr>
    <tr>
      <th>373</th>
      <td>OK</td>
      <td>602</td>
      <td>1457525.0</td>
      <td>16.0</td>
      <td>3394</td>
      <td>8.129764</td>
      <td>3284.599481</td>
    </tr>
    <tr>
      <th>374</th>
      <td>OK</td>
      <td>323</td>
      <td>850725.0</td>
      <td>17.0</td>
      <td>3717</td>
      <td>8.220672</td>
      <td>4443.954890</td>
    </tr>
    <tr>
      <th>375</th>
      <td>OK</td>
      <td>272</td>
      <td>543245.0</td>
      <td>18.0</td>
      <td>3989</td>
      <td>8.291296</td>
      <td>6012.524565</td>
    </tr>
    <tr>
      <th>376</th>
      <td>OK</td>
      <td>33</td>
      <td>100050.0</td>
      <td>19.0</td>
      <td>4022</td>
      <td>8.299535</td>
      <td>8134.747659</td>
    </tr>
    <tr>
      <th>377</th>
      <td>OK</td>
      <td>440</td>
      <td>1120550.0</td>
      <td>20.0</td>
      <td>4462</td>
      <td>8.403352</td>
      <td>11006.045591</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">negbin_trace</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/cfc0a25982a1f8dc685e848f796f27380c0596b25309635ddd5716242f8a0239.png" src="../../../_images/cfc0a25982a1f8dc685e848f796f27380c0596b25309635ddd5716242f8a0239.png" />
</div>
</div>
<p>Here, the posterior distribution for the <code class="docutils literal notranslate"><span class="pre">year</span></code> coefficient has a wider spread, and the mean is around <span class="math notranslate nohighlight">\(0.24\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.24</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(1.2712491503214047)
</pre></div>
</div>
</div>
</div>
<p>This corresponds to a growth rate around 27%. We can visualize the uncertainty in our inference for the coefficients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_posterior_samples</span><span class="p">(</span><span class="n">negbin_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">show_logy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/388e583ec91d536610e58a3ad548d2cd1d4c44c569da9d1bff438d59cac2ad4c.png" src="../../../_images/388e583ec91d536610e58a3ad548d2cd1d4c44c569da9d1bff438d59cac2ad4c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_posterior_samples</span><span class="p">(</span><span class="n">negbin_trace</span><span class="p">,</span> <span class="n">ok_turbines</span><span class="p">,</span> <span class="n">show_logy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/ff1c79cc4804318668881e8bd0be027d5ca927cba37067bf57f3bfd60536dcd2.png" src="../../../_images/ff1c79cc4804318668881e8bd0be027d5ca927cba37067bf57f3bfd60536dcd2.png" />
</div>
</div>
<p>This seems to be a much better overall fit: the uncertainty in the various lines/curves matches up with the observed data, indicating that most of the prediction lines are somewhat reasonable given the observed data. It’s important once again to remember that the uncertainty visualized here is only in the average prediction, and that we aren’t visualizing the uncertainty in the estimated variance from the negative binomial model.</p>
<p>Comparing the Poisson model and the negative binomial model, both are very similar:</p>
<ul class="simple">
<li><p>oth models took a linear combination of the features and coefficients, with somewhat similar coefficient values</p></li>
<li><p>Both models transformed the result of the linear combination using an exponential function</p></li>
</ul>
<p>The key difference was our likelihood model for the observed data points around the average prediction: the Poisson model assumed a Poisson distribution, whose variance is determined by the mean. The negative binomial model, on the other hand, assumed a negative binomial distribution, whose variance could be inferred from the data as a separate variable in our model. This enabled the negative binomial model to better capture the uncertainty in the average prediction.</p>
<p><strong>Exercise</strong>: <em>Implement Poisson and negative binomial regression in the frequentist paradigm using <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>. Are the results similar?</em></p>
<p><em>Hint: you may find it helpful to use <code class="docutils literal notranslate"><span class="pre">sm.families.Poisson()</span></code> and <code class="docutils literal notranslate"><span class="pre">sm.families.NegativeBinomial()</span></code>.</em></p>
</section>
</section>
<section id="id1">
<h2>Generalized Linear Models<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>By now, you’ve seen four different versions of regression in the Bayesian setting:</p>
<ul class="simple">
<li><p>Linear regression, for predicting real-valued outputs</p></li>
<li><p>Logistic regression, for predicting binary outputs (classification)</p></li>
<li><p>Poisson regression, for predicting counts</p></li>
<li><p>Negative binomial regression, for predicting counts</p></li>
</ul>
<p>Let’s review what they had in common and what was different between them:</p>
<ol class="arabic simple">
<li><p>For all four, computing our prediction for <span class="math notranslate nohighlight">\(y_i\)</span> begins with computing <span class="math notranslate nohighlight">\(x_i^T \beta\)</span>. This part is a <em>linear</em> function of <span class="math notranslate nohighlight">\(x_i\)</span>, even if we do something nonlinear with it later.</p></li>
<li><p>Each one had a different function that we used to compute the average value of <span class="math notranslate nohighlight">\(y_i\)</span> from <span class="math notranslate nohighlight">\(x_i^T \beta\)</span>. Since this function links the linearly transformed input <span class="math notranslate nohighlight">\(x\)</span> to the output <span class="math notranslate nohighlight">\(y\)</span>, you might expect us to call it the <strong>link function</strong>: this would make a lot of sense. However, the convention is to do the opposite, and call it the <strong>inverse link function</strong>. As you might expect from this name, the <strong>link function</strong> is the inverse of the inverse link function.</p></li>
<li><p>For each one, we used a different distribution for the likelihood of the data points. In all cases, the the mean of this distribution was always output of the inverse link function above.</p></li>
</ol>
<p>The following table summarizes the different choices of likelihood and link function for the four versions that we’ve seen:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Regression</p></th>
<th class="head text-left"><p>Inverse link function</p></th>
<th class="head text-left"><p>Link function</p></th>
<th class="head text-left"><p>Likelihood</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Linear</p></td>
<td class="text-left"><p>identity</p></td>
<td class="text-left"><p>identity</p></td>
<td class="text-left"><p>Gaussian</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Logistic</p></td>
<td class="text-left"><p>sigmoid</p></td>
<td class="text-left"><p><a class="reference external" href="https://en.wikipedia.org/wiki/Logit">logit</a></p></td>
<td class="text-left"><p>Bernoulli</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Poisson</p></td>
<td class="text-left"><p>exponential</p></td>
<td class="text-left"><p>log</p></td>
<td class="text-left"><p>Poisson</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Negative binomial</p></td>
<td class="text-left"><p>exponential</p></td>
<td class="text-left"><p>log</p></td>
<td class="text-left"><p>Negative binomial</p></td>
</tr>
</tbody>
</table>
</div>
<p>These ideas form the basis for what are known as Generalized Linear Models, or GLMs. Once we choose a link function and a likelihood distribution, our model is fully specified, and we can approximate the posterior distribution over the coefficients in <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<section id="glm-workflow">
<h3>GLM Workflow<a class="headerlink" href="#glm-workflow" title="Link to this heading">#</a></h3>
<p>When making predictions with a GLM, here’s a general sequence to follow:</p>
<ol class="arabic simple">
<li><p><strong>Formulate your prediction problem</strong> by determining what you’re trying to predict (<span class="math notranslate nohighlight">\(y\)</span>) and what you’re using to predict it (<span class="math notranslate nohighlight">\(x\)</span>). This depends on the real-world problem that you’re trying to solve, and what data might be available to help you solve it.</p></li>
<li><p><strong>Gather training data</strong> in the form of <span class="math notranslate nohighlight">\((x, y)\)</span> pairs. This may require searching through publicly available data, private or proprietary datasets you have access to, or even going out and gathering new data or paying for data to be gathered. It’s important to make sure that any data <span class="math notranslate nohighlight">\(x\)</span> that you gather will be useful in predicting <span class="math notranslate nohighlight">\(y\)</span>. At this step, it’s also helpful to determine any features you may want to compute, especially features that might be derived from existing columns in <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><strong>Choose a link function and likelihood model</strong> that make sense for your data. For example, if your output is binary, then logistic regression might make sense. If your outputs are count-valued, then Poisson or negative binomial regression might be a good fit. You can see more possibilities at the <a class="reference external" href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">Wikipedia page for generalized linear models</a>.</p></li>
<li><p><strong>Fit the model using training data</strong>: this usually involves using Bambi, statsmodels, or <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> or another package to computationally determine the coefficients of the model that work on the training dataset.</p></li>
<li><p><strong>Check that your model is actually a good fit for the data</strong>: this step, often called model checking, involves making sure that the model makes and will make good predictions on your dataset. We’ll spend the next section exploring model checking.</p></li>
<li><p><strong>Interpret the coefficients</strong>: one benefit of GLMs is that the coefficients have an intuitive interpretation. In linear regression, a coefficient tells us how much the output is predicted to increase if the corresponding predictor increases by a certain amount. We saw earlier in this section that in Poisson and negative binomial regression (or in linear regression with log-transformed <span class="math notranslate nohighlight">\(y\)</span>-values), the coefficients indicate a multiplicative increase by a factor of <span class="math notranslate nohighlight">\(\exp(\beta)\)</span>. In logistic regression, the coefficients tell us how much the log-odds increases.</p></li>
<li><p><strong>Generate new predictions</strong> from new <span class="math notranslate nohighlight">\(x\)</span>-values for new data where <span class="math notranslate nohighlight">\(y\)</span> is unknown. Usually, we use prediction models to help us make predictions for future data points. These predictions are made by multiplying the coefficients that we learned in step 4 by the predictors for the new points and applying the inverse link function. This is usually handled computationally: most packages that you can use to fit models also have the ability to predict for new data points.</p></li>
<li><p><strong>Report uncertainty</strong> for coefficients and new predictions: since we fit our models using random data that come with some inherent uncertainty, anything we derive from that data also has uncertainty. This includes the estimated coefficients that we fit in step 4 and interpreted in step 6, as well as the new predictions that we made in step 7. We’ll spend more time on quantifying uncertainty later this chapter.</p></li>
</ol>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/chapters/03"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02_regression_review.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Regression review</p>
      </div>
    </a>
    <a class="right-next"
       href="04_model_checking.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Checking and Evaluation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-from-bayesian-and-frequentist-perspectives">Linear Regression from Bayesian and frequentist perspectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbine-data-linear-regression-frequentist-approach">Turbine Data Linear Regression: Frequentist Approach</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbine-data-linear-regression-bayesian-approach">Turbine Data Linear Regression: Bayesian Approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#where-are-the-priors">Where are the Priors?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-summary">Linear regression summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-linear-regression-glms">Beyond Linear Regression: GLMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">Poisson regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#negative-binomial-regression">Negative binomial regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Generalized Linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#glm-workflow">GLM Workflow</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Data 102 Staff
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>