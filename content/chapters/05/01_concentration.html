
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tail Bounds and Concentration Inequalities &#8212; Data, Inference, and Decisions</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapters/05/01_concentration';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="prev" title="Chapter 5: Tail Bounds and Concentration Inequalities" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data, Inference, and Decisions</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01/intro.html">Chapter 1: Binary Decision-Making</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01/01_decisions_and_errors.html">Binary Decision-Making and Error Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/02_hypothesis_testing.html">Hypothesis Testing and p-Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/03_multiple_tests.html">Multiple Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/04_binary_classification.html">Binary Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/05_decision_theory.html">Decision Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../02/intro.html">Chapter 2: Bayesian modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../02/01_parameter_estimation.html">Parameter Estimation and Bayesian Inference Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/02_hierarchical_models.html">Hierarchical Bayesian Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/03_graphical_models.html">Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/04_inference.html">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../02/05_inference_with_sampling.html">Bayesian Inference with Sampling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03/intro.html">Chapter 3: Prediction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03/01_prediction.html">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/02_regression_review.html">Linear Regression Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/03_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/04_model_checking.html">Model Checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/05_uncertainty_quantification.html">Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/06_nonparametric.html">Nonparametric Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/07_neural_networks.html">Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04/intro.html">Chapter 4: Causal Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04/01_association_correlation_causation.html">Understanding Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/02_quantifying_association.html">Quantifying Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/03_causality_potential_outcomes.html">Causality and Potential Outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/04_randomized_experiments.html">Causality in Randomized Experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/05_observational_studies_unconfoundedness.html">Causality in Observational Studies: Unconfoundedness</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/06_instrumental_variables.html">Causality in Observational Studies: Natural Experiments</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Chapter 5: Tail Bounds and Concentration Inequalities</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Tail Bounds and Concentration Inequalities</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/05/01_concentration.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/chapters/05/01_concentration.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tail Bounds and Concentration Inequalities</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-introduction">Motivation and Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-concentration-inequality">What is a Concentration Inequality?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-are-we-interested-in-concentration-inequalities">Why are we interested in concentration inequalities?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-one-sided-upper-tail-bound">Example: One-sided Upper Tail Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean">Example: Sample Mean</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-s-inequality">Markov’s Inequality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-properties-of-markov-s-inequality">Understanding properties of Markov’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-markov-s-inequality">Example: Sample Mean with Markov’s Inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-example-flipping-biased-coins">Video example: flipping biased coins</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chebyshev-s-inequality-and-higher-moments">Chebyshev’s inequality and higher moments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-properties-of-chebyshev-s-inequality">Understanding properties of Chebyshev’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-chebyshev-s-inequality">Example: Sample Mean with Chebyshev’s Inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-example-flipping-biased-coins-with-chebyshev-s-inequality">Video example: flipping biased coins with Chebyshev’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#higher-moment-bounds">Higher moment bounds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#moment-generating-function-and-chernoff-s-method">Moment generating function and Chernoff’s method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chernoff-bound">Chernoff Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-chernoff-bound">Example: Sample Mean with Chernoff Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-example-flipping-biased-coins-with-the-chernoff-bound">Video example: flipping biased coins with the Chernoff bound</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-example-comparing-bounds-on-a-known-gaussian-random-variable">(Optional) Example: comparing bounds on a known Gaussian random variable</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-the-probability-for-a-given-threshold">Bounding the probability for a given threshold</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-the-threshold-for-a-given-probability">Bounding the threshold for a given probability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hoeffding-s-inequality">Hoeffding’s Inequality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hoeffding-s-lemma">Hoeffding’s Lemma</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hoeffding’s Inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-hoeffding-s-inequality">Example: Sample Mean with Hoeffding’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-hoeffding-s-inequality">Proof of Hoeffding’s Inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concentration-inequalities-and-confidence-intervals">Concentration Inequalities and Confidence intervals</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tail-bounds-and-concentration-inequalities">
<h1>Tail Bounds and Concentration Inequalities<a class="headerlink" href="#tail-bounds-and-concentration-inequalities" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">YouTubeVideo</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><em>Throughout this section, we’ll need to be a bit more precise, so we’ll use uppercase letters to represent random variables, and lowercase letters to represent the values they take on.</em></p>
<p><em>You may find it helpful to review Sections <a class="reference external" href="https://data140.org/textbook/content/Chapter_12/03_Bounds.html">12.3</a> and <a class="reference external" href="https://data140.org/textbook/content/Chapter_19/04_Chernoff_Bound.html">19.4</a> of the Data 140 textbook.</em></p>
<section id="motivation-and-introduction">
<h2>Motivation and Introduction<a class="headerlink" href="#motivation-and-introduction" title="Link to this heading">#</a></h2>
<section id="what-is-a-concentration-inequality">
<h3>What is a Concentration Inequality?<a class="headerlink" href="#what-is-a-concentration-inequality" title="Link to this heading">#</a></h3>
<p>Concentration inequalities provide a way to quantify how likely a random variable is to be in the <strong>tail</strong> of its distribution (that is, the parts of the distribution on the far ends, away from the mean). We often will write these inequalities involving the distance between a random variable <span class="math notranslate nohighlight">\(X\)</span> and its mean <span class="math notranslate nohighlight">\(E[X]\)</span>. For example, consider the following inequality:</p>
<div class="math notranslate nohighlight">
\[P(|X-E[X]|\geq t) \leq \epsilon\]</div>
<p>This looks at the distance between random variable <span class="math notranslate nohighlight">\(X\)</span> and its mean <span class="math notranslate nohighlight">\(E[x]\)</span>, and says that the probability of that distance being large (specifically, larger than some threshold <span class="math notranslate nohighlight">\(t\)</span>) is small (specifically, smaller than some value <span class="math notranslate nohighlight">\(\delta\)</span>).</p>
<p>Such inequalities are useful in understanding the behavior of arbitrary random variables, as well as algorithms that use random variables. For example, consider a sample of i.i.d. random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span>, and their sample mean:</p>
<div class="math notranslate nohighlight">
\[
Y = \frac{1}{n} \sum_i X_i 
\]</div>
<p>If we’re using <span class="math notranslate nohighlight">\(Y\)</span> to estimate the mean of <span class="math notranslate nohighlight">\(X_i\)</span>, then we can already guarantee that the average value of <span class="math notranslate nohighlight">\(Y\)</span> will be the correct answer: <span class="math notranslate nohighlight">\(E[Y] = E\left[(1/n) \sum_i X_i\right] = E[X_i]\)</span>. But this statement doesn’t tell us anything about our chance of being far off from the correct answer. Is it possible that <span class="math notranslate nohighlight">\(Y\)</span> could be very far away from <span class="math notranslate nohighlight">\(E[X_i]\)</span>? How likely is such a thing to happen?</p>
<p>Concentration inequalities provide the answers to such questions: if we can guarantee that</p>
<div class="math notranslate nohighlight">
\[
P(|Y - E[X_i]| \geq t) \leq \epsilon,
\]</div>
<p>then we can have more confidence in our result. Suppose that the above statement holds for <span class="math notranslate nohighlight">\(t=1\)</span> and <span class="math notranslate nohighlight">\(\epsilon=0.01\)</span>. Then we can clearly state “<span class="math notranslate nohighlight">\(Y\)</span> is very likely to be within <span class="math notranslate nohighlight">\(1\)</span> of the true mean <span class="math notranslate nohighlight">\(E[X_i]\)</span>: the probability that it’s further away is less than <span class="math notranslate nohighlight">\(1\%\)</span>.”</p>
<p>If the <span class="math notranslate nohighlight">\(X_i\)</span> are drawn from a well-known distribution such as the Gaussian, binomial, Poisson, etc., then we can compute such probabilities exactly, and we have no need for a concentration inequality. These inequalities are useful when we have less information about a random variable, and we want to make strong statements that hold <strong>regardless of the specific distribution</strong>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/KiD4SHKpVho"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<p>The inequality above is a two-sided inequality, because it looks at both the upper and lower tails. Sometimes, we’ll also consider <strong>one-sided</strong> concentration inequalities, such as <span class="math notranslate nohighlight">\(\mathbb{P}(X-E[X] \geq t) \leq \epsilon\)</span> or <span class="math notranslate nohighlight">\(\mathbb{P}(X-E[X] \leq -t) \leq \epsilon\)</span>.</p>
</section>
<section id="why-are-we-interested-in-concentration-inequalities">
<h3>Why are we interested in concentration inequalities?<a class="headerlink" href="#why-are-we-interested-in-concentration-inequalities" title="Link to this heading">#</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/AmPJr9F90wQ"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="example-one-sided-upper-tail-bound">
<h3>Example: One-sided Upper Tail Bound<a class="headerlink" href="#example-one-sided-upper-tail-bound" title="Link to this heading">#</a></h3>
<p>We’ll start by looking at the upper tail bound for a random variable <span class="math notranslate nohighlight">\(X\)</span> with unknown density <span class="math notranslate nohighlight">\(f\)</span> and corresponding CDF <span class="math notranslate nohighlight">\(F\)</span>. Specifically, we want to find a probabilistic upper bound <span class="math notranslate nohighlight">\(\alpha\)</span> such that most of the time, <span class="math notranslate nohighlight">\(X\)</span> takes on values less than <span class="math notranslate nohighlight">\(\alpha\)</span>. Formally, we can write this one of two ways:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(X \geq \alpha) &amp;\leq \epsilon \\
P(X &lt; \alpha) &amp;\geq 1-\epsilon
\end{align*}
\end{split}\]</div>
<p>For small values of <span class="math notranslate nohighlight">\(\epsilon\)</span>, we can interpret this statement as saying that we are confident that <span class="math notranslate nohighlight">\(X\)</span> will be below <span class="math notranslate nohighlight">\(\alpha\)</span>. For this reason, we’ll use the following terminology:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is our <strong>upper confidence bound</strong>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is our <strong>failure probability</strong>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(1-\epsilon\)</span> is our <strong>confidence level</strong>.</p></li>
</ul>
<p>In general, we can express the bound as <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span> to show the dependence explicitly: we specify some desired level of confidence (or failure probability), and find a bound that holds for that level. In practice, we want these bounds to be as small as possible: we can always choose a trivial bound at <span class="math notranslate nohighlight">\(\infty\)</span>, but this isn’t useful to us. So, we want <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span> to grow relatively slowly as a function of <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p>Suppose we choose <span class="math notranslate nohighlight">\(\epsilon = 0.05\)</span>, and we want a value of <span class="math notranslate nohighlight">\(\alpha(0.05)\)</span> that satisfies the inequality above. If <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(F\)</span> were known, we could easily compute such a value by solving for <span class="math notranslate nohighlight">\(\alpha\)</span> (for this example, we’ll assume <span class="math notranslate nohighlight">\(X\)</span> is continuous so that <span class="math notranslate nohighlight">\(P(X &lt; \alpha) = P(X \leq \alpha)\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(X \leq \alpha) &amp;\geq 1-\epsilon \\
F(\alpha) &amp;\geq 1 - \epsilon \\
\alpha &amp;\geq F^{-1}(1-\epsilon)
\end{align*}
\end{split}\]</div>
<p>In other words, if we knew the CDF <span class="math notranslate nohighlight">\(F\)</span> (and therefore the inverse CDF <span class="math notranslate nohighlight">\(F^{-1}\)</span>), we could calculate <span class="math notranslate nohighlight">\(\alpha(0.05) = F^{-1}(0.95)\)</span>.</p>
<p>What if we don’t know the CDF? If we don’t know anything at all about the random variable <span class="math notranslate nohighlight">\(X\)</span>, then we’re stuck: we can’t really make any statement about <span class="math notranslate nohighlight">\(\alpha\)</span> that would apply to any arbitrary random variable. But, we’ll see that if we make just a few assumptions, or just know a few things about <span class="math notranslate nohighlight">\(X\)</span>, then we can make some interesting statements.</p>
</section>
<section id="example-sample-mean">
<h3>Example: Sample Mean<a class="headerlink" href="#example-sample-mean" title="Link to this heading">#</a></h3>
<p>One of the most common applications of concentration inequalities is to sample means. We’ll return to the example of a sequence of i.i.d. random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> with mean <span class="math notranslate nohighlight">\(\mu\)</span>, and their sample mean <span class="math notranslate nohighlight">\(Y = (1/n)\sum X_i\)</span>.</p>
</section>
</section>
<section id="markov-s-inequality">
<h2>Markov’s Inequality<a class="headerlink" href="#markov-s-inequality" title="Link to this heading">#</a></h2>
<p>We’ll start by assuming that <span class="math notranslate nohighlight">\(X\)</span> is <strong>nonnegative</strong>, and that we know its mean <span class="math notranslate nohighlight">\(E[X]\)</span>. Note that these are fairly weak assumptions: if we know that <span class="math notranslate nohighlight">\(E[X] = 10\)</span> and that <span class="math notranslate nohighlight">\(X\)</span> is nonnegative, there are an enormous number of possible densities for <span class="math notranslate nohighlight">\(X\)</span>! Markov’s inequality holds for all of them, and it tells us:</p>
<p><strong>Markov’s inequality</strong>: For a nonnegative random variable <span class="math notranslate nohighlight">\(X\)</span> and bound <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[P(X \geq \alpha) \leq \frac{E[X]}{\alpha}.\]</div>
<p><strong>Exercise</strong>: <em>Suppose a slot machine has a payout of <span class="math notranslate nohighlight">\(\$10\)</span>. Which of the following are possible values for the probability that the machine pays out exactly <span class="math notranslate nohighlight">\(\$100\)</span>?</em>
<span class="math notranslate nohighlight">\(2\%, 4\%, 5\%, 7\%\)</span></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/DlY81PW4VEg"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<section id="understanding-properties-of-markov-s-inequality">
<h3>Understanding properties of Markov’s inequality<a class="headerlink" href="#understanding-properties-of-markov-s-inequality" title="Link to this heading">#</a></h3>
<p>Let’s use Markov’s inequality to derive a general upper confidence bound that will hold for any nonnegative random variable. If we want <span class="math notranslate nohighlight">\(P(X \geq \alpha) \leq \epsilon\)</span> for a given value of <span class="math notranslate nohighlight">\(\epsilon\)</span> (i.e., a desired confidence level / failure probability), then we have <span class="math notranslate nohighlight">\(\epsilon = E[X]/\alpha\)</span>, or equivalently <span class="math notranslate nohighlight">\(\alpha = E[X]/\epsilon\)</span>. We can make some important observations about this bound:</p>
<ul class="simple">
<li><p>As <span class="math notranslate nohighlight">\(\epsilon\)</span> decreases, <span class="math notranslate nohighlight">\(\alpha\)</span> increases. Convince yourself that this makes sense: if we want our bound <span class="math notranslate nohighlight">\(\alpha\)</span> to have a very small probability of failing (i.e., to have a very small probability that the random variable takes on a value above the bound), then we must make the bound very large.</p></li>
<li><p><em>How fast does our bound increase as <span class="math notranslate nohighlight">\(\epsilon\)</span> gets small?</em> As we noted above, a good bound will increase relatively slowly. Unfortunately, Markov’s inequality provides a relatively poor bound: the bound increases as <span class="math notranslate nohighlight">\(O(\epsilon^{-1})\)</span>, which for most random variables is not a very tight bound.</p></li>
</ul>
</section>
<section id="example-sample-mean-with-markov-s-inequality">
<h3>Example: Sample Mean with Markov’s Inequality<a class="headerlink" href="#example-sample-mean-with-markov-s-inequality" title="Link to this heading">#</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> and their sample mean <span class="math notranslate nohighlight">\(Y\)</span>, how well does Markov’s inequality answer the question of whether <span class="math notranslate nohighlight">\(Y\)</span> can be far from the true mean <span class="math notranslate nohighlight">\(\mu\)</span>?</p>
<div class="math notranslate nohighlight">
\[
P(Y - \mu \geq \alpha) = P(Y \geq \mu + \alpha) \leq \frac{\mu}{\alpha}
\]</div>
<p>This is a remarkably poor bound! In particular, note that it doesn’t even improve as the number of samples <span class="math notranslate nohighlight">\(n\)</span> grows larger. If we were to ask for an upper confidence bound <span class="math notranslate nohighlight">\(\alpha(\epsilon, n)\)</span> at a given level of confidence <span class="math notranslate nohighlight">\(\epsilon\)</span> for a given number of samples <span class="math notranslate nohighlight">\(n\)</span>, we would see that this bound also has no dependence on <span class="math notranslate nohighlight">\(n\)</span>: in other words, it remains the same regardless of how many samples we collect. This is contrary to our intuition about sample means: in general, we expect that the more independent samples we collect, the better our result should be. This is something we’ll look for in future inequalities.</p>
<p><strong>Exercises</strong>:</p>
<ol class="arabic simple">
<li><p>Consider a Poisson random variable <span class="math notranslate nohighlight">\(T\)</span> with parameter <span class="math notranslate nohighlight">\(\lambda = 5\)</span>. Using the PDF/CDF, find the smallest value of <span class="math notranslate nohighlight">\(\alpha\)</span> such that <span class="math notranslate nohighlight">\(P(T \geq \alpha) \leq 0.05\)</span>. Then, use Markov’s inequality to find an upper bound on <span class="math notranslate nohighlight">\(\alpha\)</span>. What does this tell you about the usefulness of Markov’s inequality in this example?</p></li>
<li><p>Now, let <span class="math notranslate nohighlight">\(T_i \sim \mathrm{Poisson}(\lambda)\)</span>, for <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, n\)</span>, and let <span class="math notranslate nohighlight">\(S\)</span> be the sample mean of the <span class="math notranslate nohighlight">\(T_i\)</span>. If <span class="math notranslate nohighlight">\(\lambda = 5\)</span>, find the smallest value <span class="math notranslate nohighlight">\(\alpha\)</span> such that <span class="math notranslate nohighlight">\(P(S \geq \alpha) \leq 0.05\)</span> (<em>hint: use <a class="reference external" href="https://data140.org/textbook/content/Chapter_07/01_Poisson_Distribution.html#sums-of-independent-poisson-variables">the fact that the sum of independent Poisson RVs is also Poisson</a></em>). Then use Markov’s inequality to find an upper bound on <span class="math notranslate nohighlight">\(\alpha\)</span>. How does the difference compare to Exercise 1?</p></li>
</ol>
<p><em>Hint: you may find the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html"><code class="docutils literal notranslate"><span class="pre">scipy.stats.poisson.ppf</span></code></a> function useful for both exercises.</em></p>
<p>As we’ll see, part of the reason Markov’s inequality provides such a weak bound is that it uses very limited information.</p>
</section>
<section id="video-example-flipping-biased-coins">
<h3>Video example: flipping biased coins<a class="headerlink" href="#video-example-flipping-biased-coins" title="Link to this heading">#</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/zq_JwBjlIbU"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
<section id="chebyshev-s-inequality-and-higher-moments">
<h2>Chebyshev’s inequality and higher moments<a class="headerlink" href="#chebyshev-s-inequality-and-higher-moments" title="Link to this heading">#</a></h2>
<p>What if we use more information? Specifically, suppose we use both the mean and the variance. To do so, let’s consider a random variable <span class="math notranslate nohighlight">\(X\)</span>, for which we want to bound tail probabilities, and then the random variable <span class="math notranslate nohighlight">\(Z = (X-E[X])^2\)</span>. We can see that <span class="math notranslate nohighlight">\(Z\)</span> is always nonnegative, and by the definition of variance, <span class="math notranslate nohighlight">\(E[Z] = \mathrm{var}(X)\)</span>. So, if we apply Markov’s inequality to <span class="math notranslate nohighlight">\(Z\)</span>, we arrive at Chebyshev’s inequality:</p>
<p><strong>Chebyshev’s inequality</strong>: Consider a random variable <span class="math notranslate nohighlight">\(X\)</span> with known (finite) mean and variance. For any <span class="math notranslate nohighlight">\(t &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(\vert X - E[X] \vert \ge t) \le \frac{\mathrm{var}(X)}{t^2}
\]</div>
<p><em>Proof:</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(\vert X - E[X] \vert \geq t) 
    &amp;= P((X - E[X])^2 \geq t^2) \\
    &amp;= P(Z \geq t^2) \\
    &amp;\leq \frac{E[Z]}{t^2}  \\
    &amp;= \frac{\mathrm{var}(X)}{t^2} \blacksquare
\end{align*}
\end{split}\]</div>
<section id="understanding-properties-of-chebyshev-s-inequality">
<h3>Understanding properties of Chebyshev’s inequality<a class="headerlink" href="#understanding-properties-of-chebyshev-s-inequality" title="Link to this heading">#</a></h3>
<p>This tells us that the X is unlikely to be far away from its mean: the probability that the distance <span class="math notranslate nohighlight">\(|X-E[X]|\)</span> is large (specifically, larger than <span class="math notranslate nohighlight">\(t\)</span>), decreases proportional to var<span class="math notranslate nohighlight">\((X)\)</span> and inversely proportional to <span class="math notranslate nohighlight">\(t^2\)</span>.</p>
<p>Let’s use Chebyshev’s inequality to derive a general upper confidence bound that will hold for any random variable with known mean and variance. If we want <span class="math notranslate nohighlight">\(P(X \geq \alpha) \leq \epsilon\)</span> for a given value of <span class="math notranslate nohighlight">\(\epsilon\)</span>, we need to do some algebra first before we can apply Chebyshev’s inequality.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(X \geq \alpha)
    &amp;= P(X - E[X] \geq \alpha - E[X]) \\
    &amp;\leq P(|X - E[X]| \geq \alpha - E[X]) \\
    &amp;\leq \frac{\mathrm{var}(X)}{(\alpha - E[X])^2},
\end{align*}
\end{split}\]</div>
<p>So, we have <span class="math notranslate nohighlight">\(\epsilon = \frac{\mathrm{var}(X)}{(\alpha - E[X])^2}\)</span>. While we could use the quadratic formula to obtain an expression for <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span>, the most important observations here are:</p>
<ul class="simple">
<li><p>The bound <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span> is directly proportional to the variance of <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p><em>How fast does our bound increase as <span class="math notranslate nohighlight">\(\epsilon\)</span> gets small?</em> The bound <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span> is proportional to <span class="math notranslate nohighlight">\(\epsilon^{-1/2}\)</span> (i.e., it increases as <span class="math notranslate nohighlight">\(O(\epsilon^{-1/2})\)</span>, which is a significant improvement over what we saw with Markov’s inequality. The question remains: can we do better?</p></li>
</ul>
</section>
<section id="example-sample-mean-with-chebyshev-s-inequality">
<h3>Example: Sample Mean with Chebyshev’s Inequality<a class="headerlink" href="#example-sample-mean-with-chebyshev-s-inequality" title="Link to this heading">#</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> and their sample mean <span class="math notranslate nohighlight">\(Y\)</span>, how well does Chebyshev’s inequality answer the question of whether <span class="math notranslate nohighlight">\(Y\)</span> can be far from the true mean <span class="math notranslate nohighlight">\(\mu\)</span>?</p>
<div class="math notranslate nohighlight">
\[
P(|Y - \mu| &gt; \alpha) \leq \frac{\mathrm{var}(Y)}{\alpha^2} = \frac{\mathrm{var}(X_i)}{n\alpha^2},
\]</div>
<p>where we use the fact that the <span class="math notranslate nohighlight">\(X_i\)</span> are independent to derive <span class="math notranslate nohighlight">\(\mathrm{var}(Y) = \frac{1}{n^2} \sum \mathrm{var}(X_i) = \mathrm{var}(X_i)/n\)</span>.</p>
<p>This is significantly better than the bound from Markov’s inequality: it tells us that as <span class="math notranslate nohighlight">\(n\)</span> increases, the probability decreases proportional to <span class="math notranslate nohighlight">\(1/n\)</span>. What if we were to ask for an upper confidence bound <span class="math notranslate nohighlight">\(\alpha(\epsilon, n)\)</span> at a given level of confidence <span class="math notranslate nohighlight">\(\epsilon\)</span>, for a given number of samples <span class="math notranslate nohighlight">\(n\)</span>?</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\epsilon &amp;= \frac{\mathrm{var}(X_i)}{n\alpha^2} \\
\alpha &amp;= \frac{\sigma_{X_i}}{\sqrt{n\epsilon}}
\end{align*}
\end{split}\]</div>
<p>We see a dependence on <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}}\)</span>, which tells us how fast our bound shrinks as the number of samples increases.</p>
<p><strong>Exercise</strong>: Consider again the i.i.d. Poisson random variables <span class="math notranslate nohighlight">\(T_i\)</span> and their sample mean <span class="math notranslate nohighlight">\(S\)</span>. Use Chebyshev’s inequality to find the smallest value <span class="math notranslate nohighlight">\(\alpha\)</span> such that <span class="math notranslate nohighlight">\(P(S \geq \alpha) \leq 0.05\)</span>. How does your result compare to the true probability and to the Markov bound?</p>
</section>
<section id="video-example-flipping-biased-coins-with-chebyshev-s-inequality">
<h3>Video example: flipping biased coins with Chebyshev’s inequality<a class="headerlink" href="#video-example-flipping-biased-coins-with-chebyshev-s-inequality" title="Link to this heading">#</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/WGdIZWKf9eQ"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="higher-moment-bounds">
<h3>Higher moment bounds<a class="headerlink" href="#higher-moment-bounds" title="Link to this heading">#</a></h3>
<p>Chebyshev’s inequality uses the first and second moments (<span class="math notranslate nohighlight">\(E[X]\)</span> and <span class="math notranslate nohighlight">\(E[X^2]\)</span>). For any nonnegative random variable <span class="math notranslate nohighlight">\(X\)</span> and for any natural number <span class="math notranslate nohighlight">\(k\)</span>, we can apply Markov’s inequality to <span class="math notranslate nohighlight">\(X^k\)</span>, obtaining higher <strong>moment bounds</strong>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X \geq \alpha)  \leq \frac{E[X^k]}{\alpha^k}
\]</div>
<p>Some important observations:</p>
<ul class="simple">
<li><p>If we use this to find an upper confidence bound <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span> as above, we can derive that the bound increases as <span class="math notranslate nohighlight">\(O(\epsilon^{-1/k})\)</span>, which for large values of <span class="math notranslate nohighlight">\(k\)</span> can be very small.</p></li>
<li><p>Using such bounds requires additional information about the random variable <span class="math notranslate nohighlight">\(X\)</span>: in fact, if we knew exactly all the moments <span class="math notranslate nohighlight">\(E[X^k]\)</span> for <span class="math notranslate nohighlight">\(k = 1, 2, \ldots\)</span>, then we would most likely also know the CDF.</p></li>
</ul>
</section>
</section>
<section id="moment-generating-function-and-chernoff-s-method">
<h2>Moment generating function and Chernoff’s method<a class="headerlink" href="#moment-generating-function-and-chernoff-s-method" title="Link to this heading">#</a></h2>
<p><em>You may find it helpful to review <a class="reference external" href="https://data140.org/textbook/content/Chapter_19/02_Moment_Generating_Functions.html">Section 19.2 of the Data 140 textbook</a>.</em></p>
<p>Recall from calculus that we can write the exponential function <span class="math notranslate nohighlight">\(e^x\)</span> using aa Taylor series as:</p>
<div class="math notranslate nohighlight">
\[
e^x = 1 + x + \frac{1}{2!} x^2 + \frac{1}{3!} x^3 + \frac{1}{4!} x^4 + \cdots,
\]</div>
<p>Motivated by this, and by our observation earlier that we can use the higher moments of a random variable to establish better tail bounds, we’ll examine the <strong>moment-generating function</strong> (or MGF) of a random variable <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[
M_X(\lambda) = E[e^{\lambda X}] = 1 + \lambda E[X] + \frac{\lambda^2}{2!} E[X^2] + \frac{\lambda^3}{3!} E[X^3] + \cdots
\]</div>
<p>A very important fact about MGFs is that for a sum of independent random variables <span class="math notranslate nohighlight">\(Y = X_1 + \cdots + X_n\)</span>, the MGF of <span class="math notranslate nohighlight">\(Y\)</span> is equal to the product of the MGFs for each <span class="math notranslate nohighlight">\(X_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
M_Y(\lambda) 
    &amp;= E\left[e^{\lambda Y}\right] \\
    &amp;= E\left[e^{\sum_{i=1}^n  \lambda X_i}\right] \\
    &amp;= E\left[\prod_{i=1}^n e^{\lambda X_i}\right] \\
    &amp;= \prod_{i=1}^n E\left[e^{\lambda X_i}\right] &amp; \text{(by independence)} \\
    &amp;= \prod_{i=1}^n M_{X_i}(\lambda)
\end{align*}
\end{split}\]</div>
<section id="chernoff-bound">
<h3>Chernoff Bound<a class="headerlink" href="#chernoff-bound" title="Link to this heading">#</a></h3>
<p>Since <span class="math notranslate nohighlight">\(e^{\lambda X}\)</span> is a nonnegative random variable, we can again use Markov’s inequality to find a bound:</p>
<div class="math notranslate nohighlight">
\[
P(X \geq \alpha) = P(E[e^{\lambda X}] \geq e^{\lambda \alpha}) ~ \leq ~
\frac{E[e^{\lambda X}]}{e^{\lambda \alpha}} ~ = ~ \frac{M_X(\lambda)}{e^{\lambda \alpha}}
\]</div>
<p>This is a family of bounds, one for each positive <span class="math notranslate nohighlight">\(\lambda\)</span>. To get the best possible one (i.e., the smallest), we should minimize over all nonnegative <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P(X \geq \alpha) ~ \leq ~ \min_{\lambda \ge 0} \frac{M_X(\lambda)}{e^{\lambda \alpha}}
\]</div>
<p>This is the <strong>Chernoff bound.</strong></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/YpzSdCD81qg"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<p>Some important observations:</p>
<ul class="simple">
<li><p>This requires significantly more information about the distribution than the earlier two bounds we saw. Specifically, if we know all the moments of a distribution, in most cases we effectively know the distribution. But, as we’ll see shortly, it can still be useful if we can bound the MGF for an entire class of distributions: in that case, we can replace <span class="math notranslate nohighlight">\(M_X(\lambda)\)</span> with an upper bound on it.</p></li>
<li><p><em>How fast does our bound increase as <span class="math notranslate nohighlight">\(\epsilon\)</span> gets small?</em> Solving for an upper confidence bound <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span>, we see that <span class="math notranslate nohighlight">\(\epsilon = \frac{M_X(\lambda)}{e^{\lambda \alpha}}\)</span>. If we were to for <span class="math notranslate nohighlight">\(\alpha\)</span>, we  would see that it grows as <span class="math notranslate nohighlight">\(\log(\epsilon^{-1})\)</span>: this is much, much slower than our previous two results! So, if we can find a class of random variables where the MGF is known (or bounded, as in the previous observation), then we can place much tighter bounds on those random variables.</p></li>
</ul>
</section>
<section id="example-sample-mean-with-chernoff-bound">
<h3>Example: Sample Mean with Chernoff Bound<a class="headerlink" href="#example-sample-mean-with-chernoff-bound" title="Link to this heading">#</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> and their sample mean <span class="math notranslate nohighlight">\(Y\)</span>, how well does Chernoff’s bound answer the question of whether <span class="math notranslate nohighlight">\(Y\)</span> can be far from the true mean <span class="math notranslate nohighlight">\(\mu\)</span>? Because we know about the MGF of a sum of random variables, we’ll bound the variable <span class="math notranslate nohighlight">\(nY = \sum X_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
P(Y - \mu \geq \alpha) = P(nY - n\mu \geq n\alpha) = P(nY \geq n(\mu + \alpha)) \leq \min_{\lambda &gt; 0} \frac{\left[M_{X_i}(\lambda)\right]^n}{e^{\lambda n(\mu+\alpha)}}
\]</div>
<p>This is even better than the Chebyshev bound: as <span class="math notranslate nohighlight">\(n\)</span> increases, the probability decreases proportional to <span class="math notranslate nohighlight">\(e^{-n}\)</span>.</p>
<p>What if we were to ask for an upper confidence bound <span class="math notranslate nohighlight">\(\alpha(\epsilon, n)\)</span> at a given level of confidence <span class="math notranslate nohighlight">\(\epsilon\)</span> and a given number of samples <span class="math notranslate nohighlight">\(n\)</span>? Doing the computation properly would require some more extensive computation due to the implicit optimization over <span class="math notranslate nohighlight">\(\lambda\)</span> (specifically, the optimal value of <span class="math notranslate nohighlight">\(\lambda\)</span> will depend on <span class="math notranslate nohighlight">\(n\)</span>), but we would see that the dependence on the confidence level <span class="math notranslate nohighlight">\(\epsilon\)</span> is <span class="math notranslate nohighlight">\(O(\log(\epsilon^{-1}))\)</span> , and the dependence on the number of samples is (just as with Chebyshev’s inequality) <span class="math notranslate nohighlight">\(n^{-1/2}\)</span>.</p>
<p><strong>Exercise</strong>: Consider once more the i.i.d. Poisson random variables <span class="math notranslate nohighlight">\(T_i\)</span> and their sample mean <span class="math notranslate nohighlight">\(S\)</span>. Use the Chernoff bound to find the smallest value <span class="math notranslate nohighlight">\(\alpha\)</span> such that <span class="math notranslate nohighlight">\(P(S \geq \alpha) \leq 0.05\)</span>. How does your result compare to the true probability, to the Markov bound, and to the Chebyshev bound? <em>Hint: you can easily look up the MGF for the Poisson distribution.</em></p>
</section>
<section id="video-example-flipping-biased-coins-with-the-chernoff-bound">
<h3>Video example: flipping biased coins with the Chernoff bound<a class="headerlink" href="#video-example-flipping-biased-coins-with-the-chernoff-bound" title="Link to this heading">#</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/YpzSdCD81qg"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
<section id="optional-example-comparing-bounds-on-a-known-gaussian-random-variable">
<h2>(Optional) Example: comparing bounds on a known Gaussian random variable<a class="headerlink" href="#optional-example-comparing-bounds-on-a-known-gaussian-random-variable" title="Link to this heading">#</a></h2>
<section id="bounding-the-probability-for-a-given-threshold">
<h3>Bounding the probability for a given threshold<a class="headerlink" href="#bounding-the-probability-for-a-given-threshold" title="Link to this heading">#</a></h3>
<p>We now apply each of the bounds we’ve learned about to a Gaussian random variable with known mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>, <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>. For this example, we can also compute exact bounds and compare them.</p>
<p>We cannot use Markov’s inequality, since this random variable can take on negative values.</p>
<p>Because <span class="math notranslate nohighlight">\(X - \mu\)</span> is symmetric about zero, the Chebyshev bound for a single tail is:</p>
<div class="math notranslate nohighlight">
\[
P(X - \mu \geq t) = \frac{1}{2}P(|X - \mu| \geq t) \leq \frac{\sigma^2}{2t^2}
\]</div>
<p>Next, let’s apply Chernoff’s method. Let <span class="math notranslate nohighlight">\(Z = X - \mu \sim \mathcal{N}(0, \sigma^2)\)</span>. Then <span class="math notranslate nohighlight">\(Z\)</span> has MGF <span class="math notranslate nohighlight">\(M_Z(\lambda) = e^{\sigma^2 \lambda^2/2}\)</span>. So for <span class="math notranslate nohighlight">\(\lambda \ge 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X - \mu &gt; t) ~ \le ~ \min_{\lambda \geq 0} \exp \left( \frac{\sigma^2 \lambda^2}{2} - \lambda t \right)
\]</div>
<p>How do we optimize this in <span class="math notranslate nohighlight">\(\lambda\)</span>? Recognize that since <span class="math notranslate nohighlight">\(\exp(x)\)</span> is an increasing function, we have
$<span class="math notranslate nohighlight">\(
\min_{\lambda \geq 0} \exp \left( \frac{\sigma^2 \lambda^2}{2} - \lambda t \right) = \exp \left(\min_{\lambda \geq 0} \left\{\frac{\sigma^2 \lambda^2}{2} - \lambda t \right\} \right).
\)</span>$</p>
<p>The quantity in the exponent is a quadratic function <span class="math notranslate nohighlight">\(f(\lambda)\)</span>, and can be minimized using the quadratic formula to get <span class="math notranslate nohighlight">\(\lambda^* = \frac{t}{\sigma^2}\)</span>, and <span class="math notranslate nohighlight">\(f(\lambda^*) = \frac{-t^2}{2\sigma^2}\)</span>.</p>
<p>Putting everything together, we get
$<span class="math notranslate nohighlight">\(
P(X-\mu &gt; t) ~\leq ~\exp\left(-\frac{t^2}{2\sigma^2}\right)
\)</span>$</p>
<p>Let’s visualize these two bounds along with the exact probability (which we can calculate using the CDF):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">t_min</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">t_max</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">t_min</span><span class="p">,</span> <span class="n">t_max</span> <span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">chernoff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">((</span><span class="n">t</span><span class="o">/</span><span class="n">sigma</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">chebyshev</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">sigma</span><span class="o">/</span><span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">exact_probability</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="n">sigma</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">exact_probability</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exact&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">chernoff</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Chernoff&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">chebyshev</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Chebyshev&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_min</span><span class="p">,</span> <span class="n">t_max</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$t$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$t$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">()</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability (log scale)&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Bounds on $P(X - \mu \geq t)$ when $X \sim \mathcal</span><span class="si">{N}</span><span class="s1">(\mu, 2^2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/5d0955d531087077a5dbbb5c777852b138f71a3ed7ce444e9da7222cd2977ecb.png" src="../../../_images/5d0955d531087077a5dbbb5c777852b138f71a3ed7ce444e9da7222cd2977ecb.png" />
</div>
</div>
</section>
<section id="bounding-the-threshold-for-a-given-probability">
<h3>Bounding the threshold for a given probability<a class="headerlink" href="#bounding-the-threshold-for-a-given-probability" title="Link to this heading">#</a></h3>
<p>Suppose that instead of being given a threshold and asked for the probability of exceeding it, we instead want the opposite: we want to know, given a desired probability, a bound or threshold such that the probability of exceeding that bound is small. This is the <strong>upper confidence bound</strong> we described earlier. To simplify our calculations, and without loss of generality, we’ll assume that <span class="math notranslate nohighlight">\(\mu = 0\)</span>. If we go about computing this for the normal distribution, we find that:</p>
<ul class="simple">
<li><p>For Chebyshev’s inequality, we have that <span class="math notranslate nohighlight">\(\epsilon = \frac{\sigma^2}{2\alpha^2}\)</span>, or that <span class="math notranslate nohighlight">\(\alpha(\epsilon) = \frac{\sigma}{\sqrt{2\epsilon}}\)</span>.</p></li>
<li><p>For Chernoff’s bound, we have that <span class="math notranslate nohighlight">\(\epsilon = \exp\left(-\frac{\alpha^2}{2\sigma^2}\right)\)</span>, or equivalently <span class="math notranslate nohighlight">\(\alpha(\epsilon) = \sigma\sqrt{2\log(1/\epsilon)}\)</span>.</p></li>
<li><p>As we saw before, the exact probability is <span class="math notranslate nohighlight">\(F^{-1}(1-\epsilon)\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">eps_min</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">eps_max</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">eps_min</span><span class="p">,</span> <span class="n">eps_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">eps</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">eps</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>

<span class="n">chernoff</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
<span class="n">chebyshev</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
<span class="n">exact_probability</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">eps</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">exact_probability</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exact&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">chernoff</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Chernoff&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="n">chebyshev</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Chebyshev&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Upper conf. bound $\alpha(\epsilon)$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">semilogx</span><span class="p">()</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\epsilon$ (log scale)&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\epsilon$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$P(X \geq \alpha) \leq \epsilon$ when $X \sim \mathcal</span><span class="si">{N}</span><span class="s1">(0, 2^2)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/35802616a1a0881051a56b687843c4a4fd95b2e7b6b00011eca487c9db6b1f78.png" src="../../../_images/35802616a1a0881051a56b687843c4a4fd95b2e7b6b00011eca487c9db6b1f78.png" />
</div>
</div>
<p>Both bounds are clearly loose (i.e., too large compared to the true value), but the Chernoff bound is much tighter for larger values of <span class="math notranslate nohighlight">\(t\)</span> (or equivalently, for small values of <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p>It’s a bit silly to compute Chernoff and Chebyshev tail bounds this way for normal random variables, because we already know the true probabilities. Still, this helps us understand how good those bounds are, and we’ll see in the next section that we can apply these same bounds even in cases where we don’t know the MGF exactly.</p>
</section>
</section>
<section id="hoeffding-s-inequality">
<h2>Hoeffding’s Inequality<a class="headerlink" href="#hoeffding-s-inequality" title="Link to this heading">#</a></h2>
<p>So far, we’ve seen that using the MGF of a random variable can give us excellent tail bounds. But, if we know the MGF exactly, then we also know the distribution of the random variable, and in that case we might as well use the CDF to find tail bounds. As we saw in the previous example, even the Chernoff bound was not as good as using the actual CDF.</p>
<p>In that case, what’s the point of the Chernoff bound? In many cases, we may not know the MGF, but we can <strong>bound</strong> it: in other words, we can find some function of <span class="math notranslate nohighlight">\(\lambda\)</span>, <span class="math notranslate nohighlight">\(h(\lambda)\)</span>, such that <span class="math notranslate nohighlight">\(M_X(\lambda) \leq \lambda ~ ~ \forall \lambda\)</span>.</p>
<p>One such example is that of <strong>bounded</strong> random variables: that is, random variables that only take on values between some lower bound <span class="math notranslate nohighlight">\(a\)</span> and some upper bound <span class="math notranslate nohighlight">\(b\)</span>.</p>
<section id="hoeffding-s-lemma">
<h3>Hoeffding’s Lemma<a class="headerlink" href="#hoeffding-s-lemma" title="Link to this heading">#</a></h3>
<p>Hoeffding’s lemma gives us an upper bound on the MGF for any bounded random variable. Specifically, given a random variable <span class="math notranslate nohighlight">\(X\)</span> with mean <span class="math notranslate nohighlight">\(E[X]\)</span> that is bounded between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, we have that</p>
<div class="math notranslate nohighlight">
\[
M_X(\lambda) = E[e^{\lambda X}] \leq \exp\left\{\lambda E[X] + \frac{\lambda^2(b-a)^2}{8}\right\}
\]</div>
<p>For the proof and more on the lemma, see <a class="reference external" href="https://en.wikipedia.org/wiki/Hoeffding%27s_lemma">the Hoeffding’s lemma Wikipedia article</a>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/40RQ_O0VK7M"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="id1">
<h3>Hoeffding’s Inequality<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>We could use Hoeffding’s lemma on any bounded random variable, in combination with Chernoff’s bound, to bound tail probabilities. For the rest of this section, we’ll focus on the specific case of sample means, which is what Hoeffding’s inequality is all about.</p>
<p>Let <span class="math notranslate nohighlight">\(X_1 ,X_2, \ldots, X_n\)</span> be independent (but not necessarily identically distributed) random variables bounded between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. Then Hoeffding’s inequality states that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P\left(\frac{1}{n}\sum_{i=1}^n (X_i - \mathbb{E}[X_i]) \geq t\right) \leq \exp\left(-\frac{2nt^2}{(b-a)^2}\right) \\
P\left(\frac{1}{n}\sum_{i=1}^n (X_i - \mathbb{E}[X_i]) \leq -t\right) \leq \exp\left(-\frac{2nt^2}{(b-a)^2}\right).
\end{split}\]</div>
<p>We also have the two-sided version:</p>
<div class="math notranslate nohighlight">
\[
P\left(\left|\frac{1}{n}\sum_{i=1}^n (X_i - \mathbb{E}[X_i])\right| \geq t\right) \leq 2\exp\left(-\frac{2nt^2}{(b-a)^2}\right).
\]</div>
<p>In the case where the <span class="math notranslate nohighlight">\(X_i\)</span> are identically distributed with mean <span class="math notranslate nohighlight">\(\mu\)</span>, we can rewrite the first version as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{P}\left(\left[\frac{1}{n}\sum_{i=1}^n X_i\right] - \mu \geq t\right) \leq \exp\left(-\frac{2nt^2}{(b-a)^2}\right) \\
\end{split}\]</div>
<p>This is a remarkable result! Recall that the Central Limit Theorem tells us that a similar result holds asymptotically: that is, for large values of <span class="math notranslate nohighlight">\(n\)</span>, the sample mean converges to a normal distribution, and therefore, the probability of obtaining a value far from the mean decreases as <span class="math notranslate nohighlight">\(e^{-t^2}\)</span>. But this statement holds for <strong>any</strong> value of <span class="math notranslate nohighlight">\(n\)</span>: whether <span class="math notranslate nohighlight">\(n\)</span> is small or big, or the distribution of the <span class="math notranslate nohighlight">\(X_i\)</span> is “nice” or not, this is true for the sample mean of any bounded random variables.</p>
<p>We can also know it without knowing anything else about the distribution of the random variables <span class="math notranslate nohighlight">\(X_i\)</span>, other than some bounds on them.</p>
<p>Just as before, we can make some observations:</p>
<ul class="simple">
<li><p><em>How fast does our bound increase as <span class="math notranslate nohighlight">\(\epsilon\)</span> gets small?</em> Solving for an upper confidence bound <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span>, we see that <span class="math notranslate nohighlight">\(\epsilon = \frac{M_X(\lambda)}{e^{\lambda \alpha}}\)</span>. Solving for <span class="math notranslate nohighlight">\(\alpha\)</span> (see below for an example), we can see that it grows as <span class="math notranslate nohighlight">\(\log(\epsilon^{-1})\)</span>: this is much, much slower than our previous two results! So, if we can find a class of random variables where the MGF is known (or bounded, as in the previous observation), then we can place much tighter bounds on those random variables.</p></li>
</ul>
<p><em>Note a small error in the video below: the last equation should say <span class="math notranslate nohighlight">\(P(Y \leq -t)\)</span>, not <span class="math notranslate nohighlight">\(P(Y \leq t)\)</span></em></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/f1tbEnldSt0"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="example-sample-mean-with-hoeffding-s-inequality">
<h3>Example: Sample Mean with Hoeffding’s inequality<a class="headerlink" href="#example-sample-mean-with-hoeffding-s-inequality" title="Link to this heading">#</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> and their sample mean <span class="math notranslate nohighlight">\(Y\)</span>, how well does Hoefdding’s inequality answer the question of whether <span class="math notranslate nohighlight">\(Y\)</span> can be far from the true mean <span class="math notranslate nohighlight">\(\mu\)</span>? Answering this question simply requires us to state the inequality and then analyze the result:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{P}\left(Y - \mu \geq \alpha\right) \leq \exp\left(-\frac{2n\alpha^2}{(b-a)^2}\right) \\
\end{split}\]</div>
<p>As expected (since Hoeffding’s inequality is derived using the Chernoff bound), we see the same dependence on <span class="math notranslate nohighlight">\(n\)</span>: the probability decreases proportional to <span class="math notranslate nohighlight">\(e^{-n}\)</span>. As mentioned earlier, we also see a dependence on <span class="math notranslate nohighlight">\(\alpha\)</span> of the form <span class="math notranslate nohighlight">\(e^{-\alpha^2}\)</span>, as we would with a normal random variable or with the Central Limit Theorem.</p>
<p>What if we were to ask for an upper confidence bound <span class="math notranslate nohighlight">\(\alpha(\epsilon)\)</span> at a given level of confidence <span class="math notranslate nohighlight">\(\epsilon\)</span>?</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\epsilon &amp;= \exp\left(-\frac{2n\alpha^2}{(b-a)^2}\right) \\
(b-a)^2 \log(1/\epsilon) &amp;= 2n\alpha^2 \\
\alpha &amp;= |b-a|\sqrt{\frac{\log(1/\epsilon)}{2n}}
\end{align*}
\end{split}\]</div>
<p>We can see that while this bound’s dependence on <span class="math notranslate nohighlight">\(\epsilon\)</span> is much better than what we saw with the Chebyshev bound, its dependence on <span class="math notranslate nohighlight">\(n\)</span> is similar: <strong>this bound also decreases proportional to <span class="math notranslate nohighlight">\(n^{-1/2}\)</span></strong>.</p>
</section>
<section id="proof-of-hoeffding-s-inequality">
<h3>Proof of Hoeffding’s Inequality<a class="headerlink" href="#proof-of-hoeffding-s-inequality" title="Link to this heading">#</a></h3>
<p>Without loss of generality ,we can replace <span class="math notranslate nohighlight">\(X_i\)</span> with <span class="math notranslate nohighlight">\(X_i - \mathbb{E}[X_i]\)</span> if necessary, so we may assume that <span class="math notranslate nohighlight">\(\mathbb{E}[X_i] = 0\)</span> for each <span class="math notranslate nohighlight">\(i\)</span> to simplify our calculations. Let <span class="math notranslate nohighlight">\(Z = \sum_{i=1}^n \frac{X_i}{n}\)</span>, and observe that each term is bounded between <span class="math notranslate nohighlight">\(a/n\)</span> and <span class="math notranslate nohighlight">\(b/n\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
M_Z(\lambda) &amp;= \prod_{i=1}^n M_{X_i}(\lambda) \\
&amp; \leq \prod_{i=1}^n \exp \left( \frac{(b-a)^2}{8n^2}\lambda^2 \right) \\
&amp; = \exp \left( \frac{(b-a)^2}{8n}\lambda^2 \right)
\end{align*}
\end{split}\]</div>
<p>Applying Chernoff’s method:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
P(Z &gt; t) ~ &amp; \le ~ \min_{\lambda \geq 0} \exp \left( \frac{(b-a)^2 \lambda^2}{8n} - \lambda t \right) \\
&amp; = \exp\left(-\frac{2nt^2}{(b-a)^2}\right).
\end{align*}
\end{split}\]</div>
<p>The proof of the two-sided bound is left as an exercise (and/or can easily be looked up).</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/OgdPPGNrRdM"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
<section id="concentration-inequalities-and-confidence-intervals">
<h2>Concentration Inequalities and Confidence intervals<a class="headerlink" href="#concentration-inequalities-and-confidence-intervals" title="Link to this heading">#</a></h2>
<p>We’ve seen many times that if we have an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, we can construct a confidence interval for that estimator by using the distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. Up until now, we’ve relied on either the bootstrap, computing the distribution exactly, or using the Central Limit Theorem to approximate it.</p>
<p>But, all of the methods we just discussed are applicable to any random variable! So, we could use any of them to construct a confidence interval for an estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, especially if that estimator is a sample mean (but often even if it isn’t). While the other approaches are perfectly valid, there are often cases where we might prefer a confidence interval from concentration inequalities instead:</p>
<ul class="simple">
<li><p><em>Why concentration inequalities over computing the distribution exactly?</em> As we’ve already discussed, in many cases, we many not know the distribution of our random variables exactly, and computing the distribution exactly may not be feasible.</p></li>
<li><p><em>Why concentration inequalities over bootstrap?</em> Using the bootstrap requires us to observe data and use that data to generate bootstrap resamples. In some cases, if we want to show a theoretical confidence interval before observing any data, it’s useful to use concentration inequalities.</p></li>
<li><p><em>Why concentration inequalities over the Central Limit Theorem?</em> As we’ve already discussed, the Central Limit Theorem only applies for large values of <span class="math notranslate nohighlight">\(n\)</span>, while the results we’ve derived here are applicable for any value of <span class="math notranslate nohighlight">\(n\)</span>, and for any shape of distribution.</p></li>
</ul>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/uiwbn8DbCKk"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/chapters/05"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 5: Tail Bounds and Concentration Inequalities</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-and-introduction">Motivation and Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-concentration-inequality">What is a Concentration Inequality?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-are-we-interested-in-concentration-inequalities">Why are we interested in concentration inequalities?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-one-sided-upper-tail-bound">Example: One-sided Upper Tail Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean">Example: Sample Mean</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-s-inequality">Markov’s Inequality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-properties-of-markov-s-inequality">Understanding properties of Markov’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-markov-s-inequality">Example: Sample Mean with Markov’s Inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-example-flipping-biased-coins">Video example: flipping biased coins</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chebyshev-s-inequality-and-higher-moments">Chebyshev’s inequality and higher moments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-properties-of-chebyshev-s-inequality">Understanding properties of Chebyshev’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-chebyshev-s-inequality">Example: Sample Mean with Chebyshev’s Inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-example-flipping-biased-coins-with-chebyshev-s-inequality">Video example: flipping biased coins with Chebyshev’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#higher-moment-bounds">Higher moment bounds</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#moment-generating-function-and-chernoff-s-method">Moment generating function and Chernoff’s method</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chernoff-bound">Chernoff Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-chernoff-bound">Example: Sample Mean with Chernoff Bound</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-example-flipping-biased-coins-with-the-chernoff-bound">Video example: flipping biased coins with the Chernoff bound</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-example-comparing-bounds-on-a-known-gaussian-random-variable">(Optional) Example: comparing bounds on a known Gaussian random variable</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-the-probability-for-a-given-threshold">Bounding the probability for a given threshold</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-the-threshold-for-a-given-probability">Bounding the threshold for a given probability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hoeffding-s-inequality">Hoeffding’s Inequality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hoeffding-s-lemma">Hoeffding’s Lemma</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Hoeffding’s Inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sample-mean-with-hoeffding-s-inequality">Example: Sample Mean with Hoeffding’s inequality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-hoeffding-s-inequality">Proof of Hoeffding’s Inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concentration-inequalities-and-confidence-intervals">Concentration Inequalities and Confidence intervals</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Data 102 Staff
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>