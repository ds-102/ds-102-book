
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Parameter estimation &#8212; Data, Inference, and Decisions</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Data, Inference, and Decisions</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01/intro.html">
   Chapter 1: Binary Decision-Making
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/01_decisions_and_errors.html">
     Binary Decision-Making and Error Rates
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/02_hypothesis_testing.html">
     Hypothesis Testing and p-Values
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="intro.html">
   Chapter 2: Bayesian modeling
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="placeholder.html">
     Intro to Bayesian modeling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03/intro.html">
   Chapter 3: Generalized linear models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/placeholder.html">
     Generalized Linear Models
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/ds-102/ds-102-book/master?urlpath=tree/ds-102-book/content/chapters/02/parameter_estimation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ds-102/ds-102-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/02/parameter_estimation.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/content/chapters/02/parameter_estimation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intuition-and-computation">
   Intuition and computation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequentist-parameter-estimation-maximum-likelihood">
   Frequentist parameter estimation: maximum likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#writing-the-likelihood">
     1. Writing the likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#taking-the-log">
     2. Taking the log
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximizing">
     3. Maximizing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applying-the-maximum-likelihood-estimate">
     Applying the maximum likelihood estimate
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-different-way-bayesian-estimation">
   A different way: Bayesian estimation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-prior-beta-distributions">
     Choosing a prior: Beta distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#normalizing-the-beta-distribution-optional">
       Normalizing the Beta distribution (optional)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#intuition-for-the-beta-distribution">
       Intuition for the Beta distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computing-the-posterior">
       Computing the posterior
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#demo-prior-and-posterior-betas">
       Demo: prior and posterior Betas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#point-estimates">
       Point estimates
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#choosing-the-prior-parameters">
       Choosing the prior parameters
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#another-attempt-a-non-conjugate-prior">
     Another attempt: a non-conjugate prior
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-continuous-example-estimating-heights">
   A continuous example: estimating heights
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mle-for-the-gaussian-likelihood">
     MLE for the Gaussian likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-inference-for-heights">
     Bayesian inference for heights
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Parameter estimation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intuition-and-computation">
   Intuition and computation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#frequentist-parameter-estimation-maximum-likelihood">
   Frequentist parameter estimation: maximum likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#writing-the-likelihood">
     1. Writing the likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#taking-the-log">
     2. Taking the log
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maximizing">
     3. Maximizing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applying-the-maximum-likelihood-estimate">
     Applying the maximum likelihood estimate
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-different-way-bayesian-estimation">
   A different way: Bayesian estimation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-a-prior-beta-distributions">
     Choosing a prior: Beta distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#normalizing-the-beta-distribution-optional">
       Normalizing the Beta distribution (optional)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#intuition-for-the-beta-distribution">
       Intuition for the Beta distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computing-the-posterior">
       Computing the posterior
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#demo-prior-and-posterior-betas">
       Demo: prior and posterior Betas
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#point-estimates">
       Point estimates
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#choosing-the-prior-parameters">
       Choosing the prior parameters
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#another-attempt-a-non-conjugate-prior">
     Another attempt: a non-conjugate prior
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-continuous-example-estimating-heights">
   A continuous example: estimating heights
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mle-for-the-gaussian-likelihood">
     MLE for the Gaussian likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-inference-for-heights">
     Bayesian inference for heights
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">6</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;inline&#39;</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">6</span> <span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;scipy&#39;
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="parameter-estimation">
<h1>Parameter estimation<a class="headerlink" href="#parameter-estimation" title="Permalink to this headline">#</a></h1>
<p>Consider our usual setup: we have some data <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>, and we want to estimate some parameter <span class="math notranslate nohighlight">\(\theta\)</span> from the distribution. Here are two examples that we’ll keep returning to through these notes:</p>
<ul class="simple">
<li><p><strong>Estimating average height</strong>: <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span> are the heights of <span class="math notranslate nohighlight">\(n\)</span> individuals in a random sample, and <span class="math notranslate nohighlight">\(\theta\)</span> is the average height of individuals in the population.</p></li>
<li><p><strong>Estimating product quality</strong>: <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span> are the reviews (👍/0 or 👎/1) for a product being sold online, and <span class="math notranslate nohighlight">\(\theta\)</span> is a number between 0 and 1 that represents the probability of someone leaving a good review. Intuitively, <span class="math notranslate nohighlight">\(\theta\)</span> represents our estimate of the quality of the product.</p></li>
</ul>
<section id="intuition-and-computation">
<h2>Intuition and computation<a class="headerlink" href="#intuition-and-computation" title="Permalink to this headline">#</a></h2>
<p>Suppose you’re shopping for a new microwave. You find two choices, both of which cost about the same amount:</p>
<ul class="simple">
<li><p>Microwave A has three positive reviews and no negative reviews</p></li>
<li><p>Microwave B has 19 positive reviews and 1 negative review</p></li>
</ul>
<p>Intuitively, which one would you rather buy? There isn’t necessarily a right or wrong answer, but in a scenario like this, most shoppers would probably prefer option B: there’s something reassuring about seeing more data points.</p>
<p>Now, we’ll approach the question computationally from a frequentist and then a Bayesian perspective, and see how they capture different aspects of the intuition above.</p>
<p>The key is that we’ve chosen a parameter <span class="math notranslate nohighlight">\(\theta\)</span> that fits nicely within our probability model (it’s the probability of a positive review), and also has an intuitive interpretation that’s useful to us (if we know that one microwave has a higher <span class="math notranslate nohighlight">\(\theta\)</span> than the other, then it’s probably better than the other one).</p>
<p>Our goal will be to estimate and draw conclusions about the parameter <span class="math notranslate nohighlight">\(\theta\)</span> based on the observed data <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>.</p>
</section>
<section id="frequentist-parameter-estimation-maximum-likelihood">
<h2>Frequentist parameter estimation: maximum likelihood<a class="headerlink" href="#frequentist-parameter-estimation-maximum-likelihood" title="Permalink to this headline">#</a></h2>
<p>Before we can actually do any parameter estimation, we need to first specify the relevant probability distributions. Recall that in the frequentist setting, we’re assuming that the data (our binary reviews, <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>) are random, but that our parameter (our probability of generating a positive review <span class="math notranslate nohighlight">\(\theta\)</span>) is fixed and unknown.</p>
<p>To set up a probability model in the frequentist setting, all we need is a <strong>likelihood</strong> <span class="math notranslate nohighlight">\(p(x_i|\theta)\)</span>. This tells us how likely a data point is, given a certain value of <span class="math notranslate nohighlight">\(\theta\)</span>. Since our data are binary and our parameter is a number between 0 and 1, a natural choice is a <strong>Bernoulli distribution</strong> (for more on Bernoulli distributions, see the <a class="reference external" href="http://prob140.org/textbook/content/Chapter_03/02_Distributions.html#named-distributions">Data 140 textbook</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Wikipedia</a>):</p>
<div class="math notranslate nohighlight">
\[
x_i | \theta \sim \mathrm{Bernoulli}(\theta)
\]</div>
<p>This notation means “conditioned on <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(x_i\)</span> follows a Bernoulli distribution with parameter <span class="math notranslate nohighlight">\(\theta\)</span>”. In other words:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
p(x_i|\theta) = \begin{cases}
    \theta &amp; \text{ if }x_i = 1 \\
    1-\theta &amp; \text{ if }x_i = 0
\end{cases}
\end{split}\]</div>
<p>The simplest way to estimate <span class="math notranslate nohighlight">\(\theta\)</span> in the frequentist setting is to use <strong>maximum likelihood estimation (MLE)</strong>. This says to choose the value of <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes the likelihood: in other words, we’ll pick the value of <span class="math notranslate nohighlight">\(\theta\)</span> that makes our data look as likely as possible.</p>
<p>Here’s how we’ll go about it:</p>
<ol class="simple">
<li><p>We’ll write the likelihood for all the data points, <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span> (we already did it for a single point above)</p></li>
<li><p>We’ll use the log-likelihood instead of the likelihood. This will make things a little easier computationally. Why is this okay? Because <span class="math notranslate nohighlight">\(\log\)</span> is a monotonically increasing function, so the same value of <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes <span class="math notranslate nohighlight">\(\log(p(x|\theta))\)</span> also maximizes <span class="math notranslate nohighlight">\(p(x|\theta)\)</span>.</p></li>
<li><p>To find the best value of <span class="math notranslate nohighlight">\(\theta\)</span>, we’ll take the derivative of the log-likelihood with respect to <span class="math notranslate nohighlight">\(\theta\)</span>, set it equal to 0, and solve.</p></li>
</ol>
<section id="writing-the-likelihood">
<h3>1. Writing the likelihood<a class="headerlink" href="#writing-the-likelihood" title="Permalink to this headline">#</a></h3>
<p>Our likelihood for all the data points is:</p>
<div class="math notranslate nohighlight">
\[
p(x_1, \ldots, x_n | \theta)
\]</div>
<p>We’ll assume that our samples are conditionally i.i.d. (independent and identically distributed) given the parameter <span class="math notranslate nohighlight">\(\theta\)</span>. In our example, this means that <strong>if</strong> we know how good the product is, then knowing one review doesn’t tell us anything about the other reviews. This means we can simplify our likelihood:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
p(x_1, \ldots, x_n|\theta) &amp;= \prod_{i=1}^n p(x_i|\theta)
\end{align}
\]</div>
<p>Next, we need to figure out a more mathematically convenient way to write the likelihood for an individual point, <span class="math notranslate nohighlight">\(p(x_i|\theta)\)</span>, than the version we wrote in the previous section. Here’s an alternative way of writing it that means the same thing:</p>
<div class="math notranslate nohighlight">
\[
p(x_i|\theta) = \theta^{x_i}(1-\theta)^{1-x_i}
\]</div>
<p>Convince yourself that this is the same by plugging in <span class="math notranslate nohighlight">\(x_i=1\)</span> and <span class="math notranslate nohighlight">\(x_i=0\)</span>.</p>
<p>Now that we have this notationally convenient way of writing it, when we multiply all of them together, the exponents add:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(x_1, \ldots, x_n|\theta) 
    &amp;= \prod_{i=1}^n p(x_i|\theta) \\
    &amp;= \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i} \\
    &amp;= \theta^{\left[\sum_i x_i\right]}(1-\theta)^{\left[\sum_i (1-x_i)\right]} \\
\end{align}
\end{split}\]</div>
<p>The first exponent is simply the number of positive reviews, and the second exponent is simply the number of negative reviews. To simplify our expression, we’ll <strong>let <span class="math notranslate nohighlight">\(k\)</span> be the number of positive reviews (<span class="math notranslate nohighlight">\(k = \sum_i x_i\)</span>)</strong>, so that this becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(x_1, \ldots, x_n|\theta) 
    &amp;= \theta^{k}(1-\theta)^{n-k} \\
\end{align}
\end{split}\]</div>
</section>
<section id="taking-the-log">
<h3>2. Taking the log<a class="headerlink" href="#taking-the-log" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\begin{align}
\log p(x_1, \ldots, x_n|\theta) 
    &amp;= 
        k\log\theta + 
        (n-k)\log(1-\theta)
\end{align}
\]</div>
</section>
<section id="maximizing">
<h3>3. Maximizing<a class="headerlink" href="#maximizing" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{d}{d\theta} \log p(x_1, \ldots, x_n|\theta) &amp;= 0 \\
\frac{d}{d\theta} \left[k\log\theta + (n-k)\log(1-\theta)\right] &amp;= 0 \\
\frac{k}{\theta} - \frac{n-k}{1-\theta} &amp;= 0 \\
\theta &amp;= \frac{k}{n}
\end{align}
\end{split}\]</div>
<p>Finally, we’ve arrived at our maximum likelihood estimate for the Bernoulli likelihood: it’s simply the fraction of total reviews that are positive. This is a very intuitive result!</p>
<p>At this point, we can see why we chose to use the log likelihood: it was easier to differentiate the expression from step 2 than it would have been to differentiate the expression from step 1 (if you need to, convince yourself that this is true by computing the derivative).</p>
</section>
<section id="applying-the-maximum-likelihood-estimate">
<h3>Applying the maximum likelihood estimate<a class="headerlink" href="#applying-the-maximum-likelihood-estimate" title="Permalink to this headline">#</a></h3>
<p>Let’s return to our example comparing microwaves A and B. If we apply maximum likelihood for each one, we get that <span class="math notranslate nohighlight">\(\hat{\theta}_{A, \text{MLE}} = 3/3 = 1\)</span> and <span class="math notranslate nohighlight">\(\hat{\theta}_{B,\text{MLE}} = 19/20 = 0.95\)</span>: in other words, MLE tells us that Microwave A is the better choice.</p>
<p>In other words, the value of <span class="math notranslate nohighlight">\(\theta\)</span> that makes “3 positive reviews only” as likely as possible is <span class="math notranslate nohighlight">\(\theta=1\)</span>. Similarly, the value of <span class="math notranslate nohighlight">\(\theta\)</span> that makes “19 positive reviews and 1 negative review” as likely as possible is <span class="math notranslate nohighlight">\(\theta=0.95\)</span>.</p>
</section>
</section>
<section id="a-different-way-bayesian-estimation">
<h2>A different way: Bayesian estimation<a class="headerlink" href="#a-different-way-bayesian-estimation" title="Permalink to this headline">#</a></h2>
<p>In the Bayesian setting, we’ll assume that the unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span> is random. Instead of just thinking about the likelihood <span class="math notranslate nohighlight">\(p(x|\theta)\)</span>, we’ll consider the <strong>posterior distribution</strong> <span class="math notranslate nohighlight">\(p(\theta|x)\)</span>, which represents our understanding of the randomness in <span class="math notranslate nohighlight">\(\theta\)</span> after seeing the data <span class="math notranslate nohighlight">\(x\)</span>. We’ll compute it using Bayes’ rule:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\underbrace{p(\theta|x)}_{\text{posterior}} 
    &amp;= 
    \frac{\overbrace{p(x|\theta)}^{\text{likelihood}}\,\,
          \overbrace{p(\theta)}^{\text{prior}}}{p(x)}
\end{align}
\]</div>
<p>When we set up a probability model above, we chose a likelihood that described the connection between our parameter <span class="math notranslate nohighlight">\(\theta\)</span> and our data <span class="math notranslate nohighlight">\(x\)</span>. In the Bayesian formulation, we need to choose one more thing: a <strong>prior</strong> <span class="math notranslate nohighlight">\(p(\theta)\)</span>, which represents our belief about <span class="math notranslate nohighlight">\(\theta\)</span> before we get to see any data. Typically, we choose a prior based on domain knowledge and/or computational convenience.</p>
<p>Note that the denominator here is a constant: <span class="math notranslate nohighlight">\(p(\theta|x)\)</span> is a distribution over values of <span class="math notranslate nohighlight">\(\theta\)</span>, and the denominator <span class="math notranslate nohighlight">\(p(x)\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>. So, we’ll often write the posterior as follows (the symbol <span class="math notranslate nohighlight">\(\propto\)</span> means “is proportional to”):</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
p(\theta|x) &amp;\propto p(x|\theta)p(\theta)
\end{align}
\]</div>
<p>Even though the denominator is a constant, it still matters: more on this later. For now, we’ll just try to find ways to get away without computing it.</p>
<p>We have two important questions left to answer before we can compute a Bayesian estimate:</p>
<ol class="simple">
<li><p>How do we choose the prior <span class="math notranslate nohighlight">\(p(\theta)\)</span>?</p></li>
<li><p>Once we get the posterior distribution <span class="math notranslate nohighlight">\(p(\theta|x)\)</span>, how do we use that to choose a single value of <span class="math notranslate nohighlight">\(\theta\)</span> as our estimate?</p></li>
</ol>
<p>We’ll answer these two questions in the next section.</p>
<section id="choosing-a-prior-beta-distributions">
<h3>Choosing a prior: Beta distributions<a class="headerlink" href="#choosing-a-prior-beta-distributions" title="Permalink to this headline">#</a></h3>
<p>For our prior <span class="math notranslate nohighlight">\(p(\theta)\)</span>, which represents our belief about the unknown <span class="math notranslate nohighlight">\(\theta\)</span> before seeing any data, we’ll choose the <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>. This isn’t the only distribution we can choose, but it happens to be a particularly convenient choice (we’ll see why later). The Beta distribution has two parameters, <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. It’s defined as:</p>
<div class="math notranslate nohighlight">
\[
p(\theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}
\]</div>
<p>The normalization constant is quite complicated, but we don’t need to know what it is to use the Beta distribution. Since this is a well-studied distribution, we can find out any facts about it that we need just by looking them up. Try it yourself: without doing any computation, what is the expectation of a Beta-distributed random variable? What is the mode?</p>
<section id="normalizing-the-beta-distribution-optional">
<h4>Normalizing the Beta distribution (optional)<a class="headerlink" href="#normalizing-the-beta-distribution-optional" title="Permalink to this headline">#</a></h4>
<p>How would we go about computing the normalization constant for this distribution? We know that the formula above is missing a constant of proportionality, which we’ll call <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(\theta) = \frac{1}{Z} \theta^{\alpha-1} (1-\theta)^{\beta-1}
\]</div>
<p>We’ll need to solve for <span class="math notranslate nohighlight">\(Z\)</span>. As a probability density function (PDF), we know that the distribution <span class="math notranslate nohighlight">\(p(\theta)\)</span> should integrate to 1. So,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\int_0^1 \frac{1}{Z} \theta^{\alpha-1} (1-\theta)^{\beta-1} \, d\theta &amp;= 1 \\
\int_0^1 \theta^{\alpha-1} (1-\theta)^{\beta-1} \, d\theta &amp;= Z
\end{align*}
\end{split}\]</div>
<p>This integral is difficult to compute, but we can look up that the result is something called the <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_function">Beta function</a>. Details about this function are beyond the scope of this class, but it’s related to the Gamma function (an extension of the factorial function to non-integer values).</p>
<p>In other words, no matter what the values of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are, the denominator will always have the same form: a beta function. This is why we typically ignore it, and only write out the proportionality.</p>
<p>Also note that <span class="math notranslate nohighlight">\(Z\)</span> does not depend on <span class="math notranslate nohighlight">\(\theta\)</span>, but it does depend on the parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
</section>
<section id="intuition-for-the-beta-distribution">
<h4>Intuition for the Beta distribution<a class="headerlink" href="#intuition-for-the-beta-distribution" title="Permalink to this headline">#</a></h4>
<p>Before we compute the posterior, let’s build some intuition for how the Beta distribution works. The <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_distribution">Wikipedia page</a> has some helpful examples of what it looks like for different values of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. Let’s look at a few of those examples:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">beta_11</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">beta_72</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">beta_34</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">beta_11</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\alpha = 1, \beta = 1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">beta_34</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\alpha = 3, \beta = 4$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">beta_72</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\alpha = 7, \beta = 2$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7ff4ef522490&gt;
</pre></div>
</div>
<img alt="../../../_images/parameter_estimation_13_1.png" src="../../../_images/parameter_estimation_13_1.png" />
</div>
</div>
<p>We can see that different values of the parameters lead to different shapes for the Beta distribution. In particular, when <span class="math notranslate nohighlight">\(\alpha\)</span> is larger than <span class="math notranslate nohighlight">\(\beta\)</span>, we see that smaller values of <span class="math notranslate nohighlight">\(\theta\)</span> are more likely, and vice versa. Try experimenting with different values: what changes as <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> get larger?</p>
</section>
<section id="computing-the-posterior">
<h4>Computing the posterior<a class="headerlink" href="#computing-the-posterior" title="Permalink to this headline">#</a></h4>
<p>Now that we’ve set up the prior, we can compute the posterior distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(\theta|x) 
    &amp;\propto p(x|\theta)p(\theta) \\
    &amp;\propto \Bigg[\theta^{\left[\sum_i x_i\right]}(1-\theta)^{\left[\sum_i (1-x_i)\right]}\Bigg]\Big[\theta^{\alpha-1} (1-\theta)^{\beta-1}\Big] \\
    &amp;\propto \theta^{\alpha + \left[\sum_i x_i\right] - 1}(1-\theta)^{\beta + \left[\sum_i (1-x_i)\right] - 1}
\end{align}
\end{split}\]</div>
<p>We can see that this is also a Beta distribution, because it looks like <span class="math notranslate nohighlight">\(\theta^{\text{stuff}}(1-\theta)^{\text{stuff}}\)</span>. It has parameters <span class="math notranslate nohighlight">\(\alpha + \sum_i x_i\)</span> and <span class="math notranslate nohighlight">\(\beta + \sum_i (1-x_i)\)</span>, so we can write the posterior as</p>
<div class="math notranslate nohighlight">
\[
\theta|x \sim \mathrm{Beta}\Bigg(\alpha + \sum_i x_i\,,\,\, \beta + \sum_i (1-x_i)\Bigg)
\]</div>
<p>By making this observation, we avoided having to compute the denominator <span class="math notranslate nohighlight">\(p(x)\)</span>!</p>
<p>This is because the Beta distribution is the <strong>conjugate prior</strong> for the Bernoulli distribution. This means that whenever we have a Bernoulli likelihood, if we choose a prior from the Beta family, then the posterior distribution will also be in the Beta family.</p>
</section>
<section id="demo-prior-and-posterior-betas">
<h4>Demo: prior and posterior Betas<a class="headerlink" href="#demo-prior-and-posterior-betas" title="Permalink to this headline">#</a></h4>
<p>Here are some useful functions that we’ll use to plot prior and posterior Beta distributions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="c1">#def plot_beta_prior_and_posterior(r, s, m, y, show_map=False, show_lmse=False):</span>
<span class="k">def</span> <span class="nf">plot_beta_prior_and_posterior</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">pos_obs</span><span class="p">,</span> <span class="n">neg_obs</span><span class="p">,</span> <span class="n">show_map</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_lmse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>   
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="n">alpha_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">pos_obs</span>
    <span class="n">beta_new</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">neg_obs</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha_new</span><span class="p">,</span> <span class="n">beta_new</span><span class="p">)</span>
    
    <span class="c1"># You never have to memorize these: when making this notebook,</span>
    <span class="c1"># I found them on the wikipedia page for the Beta distribution:</span>
    <span class="c1"># https://en.wikipedia.org/wiki/Beta_distribution</span>

    <span class="k">if</span> <span class="n">show_lmse</span><span class="p">:</span>
        <span class="n">x_lmse</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha_new</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">alpha_new</span> <span class="o">+</span> <span class="n">beta_new</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_lmse</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="k">if</span> <span class="n">show_map</span><span class="p">:</span>
        <span class="n">x_map</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha_new</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha_new</span> <span class="o">+</span> <span class="n">beta_new</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_map</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">plot_prior_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">1.02</span><span class="p">),</span>
                         <span class="n">prior_label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Prior: Beta(</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span>
                         <span class="n">posterior_label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Posterior: Beta(</span><span class="si">{</span><span class="n">alpha_new</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">beta_new</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span>
                         <span class="n">x_map</span><span class="o">=</span><span class="n">x_map</span><span class="p">,</span> <span class="n">x_lmse</span><span class="o">=</span><span class="n">x_lmse</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">plot_prior_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">xlim</span><span class="p">,</span> 
                         <span class="n">prior_label</span><span class="p">,</span> <span class="n">posterior_label</span><span class="p">,</span>
                         <span class="n">x_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x_lmse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">prior_label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">posterior_label</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">map_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_map</span><span class="p">))</span>
        <span class="n">posterior_map</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">[</span><span class="n">map_index</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;MAP estimate: </span><span class="si">{</span><span class="n">x_map</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x_map</span><span class="p">,</span> <span class="n">x_map</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">posterior_map</span><span class="p">],</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_lmse</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lmse_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x_lmse</span><span class="p">))</span>
        <span class="n">posterior_lmse</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">[</span><span class="n">lmse_index</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;LMSE estimate: </span><span class="si">{</span><span class="n">x_lmse</span><span class="si">:</span><span class="s1">0.2f</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x_lmse</span><span class="p">,</span> <span class="n">x_lmse</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">posterior_lmse</span><span class="p">],</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="c1">#plt.legend(bbox_to_anchor=(1.32, 1.02))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ymax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">prior</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">prior</span><span class="p">)]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">posterior</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">posterior</span><span class="p">)]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">ymax</span><span class="o">+</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Prior $p(\theta)$ and posterior given observed data $x$: $p(\theta|x)$&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_beta_prior_and_posterior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/parameter_estimation_19_0.png" src="../../../_images/parameter_estimation_19_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_beta_prior_and_posterior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/parameter_estimation_20_0.png" src="../../../_images/parameter_estimation_20_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_beta_prior_and_posterior</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/parameter_estimation_21_0.png" src="../../../_images/parameter_estimation_21_0.png" />
</div>
</div>
</section>
<section id="point-estimates">
<h4>Point estimates<a class="headerlink" href="#point-estimates" title="Permalink to this headline">#</a></h4>
<p>Once we have a posterior distribution, we will often want to answer the question: how do we use the distribution to get a single estimate for <span class="math notranslate nohighlight">\(\theta\)</span>? This single number is typically called a <strong>point estimate</strong>.</p>
<p>We’ll start with two answers for question 1: the <strong>Maximum a Posteriori (MAP) estimate</strong> and the <strong>Minimum/Least Mean Squared Error (MMSE/LMSE) Estimate</strong>.</p>
<p>The <strong>MAP estimate</strong> is the estimate that maximizes the posterior distribution. Remember that the posterior distribution represents our belief about the unknown parameter <span class="math notranslate nohighlight">\(\theta\)</span>. Intuitively, the MAP estimate is the <span class="math notranslate nohighlight">\(\theta\)</span> value that is most likely according to that belief. For a Beta<span class="math notranslate nohighlight">\((\alpha, \beta)\)</span> distribution, we can look up the maximum value (mode) of the distribution to find that it’s <span class="math notranslate nohighlight">\(\frac{\alpha-1}{\alpha+\beta-2}\)</span>.</p>
<p><strong>Optional exercise</strong>: derive the mode of the Beta distribution. <em>Hint</em>: instead of maximizing the Beta PDF with respect to <span class="math notranslate nohighlight">\(\theta\)</span>, try taking the log first, just like we did before.</p>
<p>The <strong>LMSE estimate</strong> is simply the mean of the posterior distribution. It has this name because we can show that the mean of the posterior distribution minimizes the mean squared error:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\text{argmin}_a E_{p(\theta|x)}\left[(a-\theta)^2\right] = E_{p(\theta|x)}\left[\theta\right]
\end{align}
\]</div>
<p>The mean of a Beta<span class="math notranslate nohighlight">\((\alpha, \beta)\)</span>-distributed random variable is simply <span class="math notranslate nohighlight">\(\frac{\alpha}{\alpha+\beta}\)</span>.</p>
</section>
<section id="choosing-the-prior-parameters">
<h4>Choosing the prior parameters<a class="headerlink" href="#choosing-the-prior-parameters" title="Permalink to this headline">#</a></h4>
<p>Returning to our microwave example, we can see that our choice of prior implicitly encodes what we believe about products and reviews.</p>
<p>For example, if we believe that most products are bad until proven otherwise by the review data, we may choose a Beta<span class="math notranslate nohighlight">\((1, 5)\)</span> prior, which favors smaller values of <span class="math notranslate nohighlight">\(\theta\)</span>. In this case, our posterior for microwave 1 (three positive reviews) is Beta<span class="math notranslate nohighlight">\((4, 5)\)</span>: if we compute the MAP estimate, we get 0.43. Our posterior for microwave 2 (nineteen positive reviews and one negative review) would be Beta<span class="math notranslate nohighlight">\((20, 6)\)</span>, and the MAP estimate is 0.79. So, under this prior, the first microwave looks better.</p>
<p>But, if we think that most products are good until proven otherwise by the review data, then we could choose a more “optimistic” prior, such as Beta<span class="math notranslate nohighlight">\((5, 1)\)</span>, which favors larger values of <span class="math notranslate nohighlight">\(\theta\)</span>. In this case, the posterior for microwave 1 is Beta<span class="math notranslate nohighlight">\((8, 1)\)</span>, and the MAP estimate is 1. The posterior for microwave 2 is Beta<span class="math notranslate nohighlight">\((24, 2)\)</span>, and the MAP estimate is 0.96.</p>
<p>From this, we can take away that <strong>our choice of prior can often have a significant effect on our results</strong>. So, it’s important to choose priors that capture your assumptions and belief about the world, and to be explicit about communicating those assumptions with others when sharing your results.</p>
</section>
</section>
<section id="another-attempt-a-non-conjugate-prior">
<h3>Another attempt: a non-conjugate prior<a class="headerlink" href="#another-attempt-a-non-conjugate-prior" title="Permalink to this headline">#</a></h3>
<p>To see why the conjugate prior is so helpful, let’s see what would have happened if we chose a different prior.</p>
<p>Suppose we instead choose the prior <span class="math notranslate nohighlight">\(p(\theta) = \cos\left(\frac{\pi}{2} \theta\right)\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">thetas</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\cos(\pi \theta/2)$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/parameter_estimation_26_0.png" src="../../../_images/parameter_estimation_26_0.png" />
</div>
</div>
<p>We can try to compute the posterior just as we did before:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(\theta|x) 
    &amp;\propto p(x|\theta)p(\theta) \\
    &amp;\propto \Bigg[\theta^{\left[\sum_i x_i\right]}(1-\theta)^{\left[\sum_i (1-x_i)\right]}\Bigg]\cos\left(\frac{\pi}{2}\theta\right)
\end{align}
\end{split}\]</div>
<p>This distribution looks much more complicated: we can’t reduce it to a known distribution at all.</p>
<p>This makes it hard to reason about the distribution, and to compute quantities of interest about it (e.g., the mean of the posterior for the LMSE estimate).</p>
</section>
</section>
<section id="a-continuous-example-estimating-heights">
<h2>A continuous example: estimating heights<a class="headerlink" href="#a-continuous-example-estimating-heights" title="Permalink to this headline">#</a></h2>
<p>Suppose instead that now our parameter of interest <span class="math notranslate nohighlight">\(\theta\)</span> is the average height of a population, and our data <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span> are heights of individuals in a sample. A Bernoulli distribution no longer makes sense for these continuous data. There are many options we could choose, but for simplicity we’ll use a normal distribution:</p>
<div class="math notranslate nohighlight">
\[
x_i|\theta \sim \mathcal{N}(\theta, \sigma^2)
\]</div>
<section id="mle-for-the-gaussian-likelihood">
<h3>MLE for the Gaussian likelihood<a class="headerlink" href="#mle-for-the-gaussian-likelihood" title="Permalink to this headline">#</a></h3>
<p>We can go through a similar process as before to find the MLE for the Gaussian likelihood. Because the normalizing constant <span class="math notranslate nohighlight">\(\frac{1}{\sigma\sqrt{2\pi}}\)</span> doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span> (and our goal is to optimize for <span class="math notranslate nohighlight">\(\theta\)</span>, we’ll leave it out for now.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(x_i|\theta) &amp; \frac{1}{\sigma\sqrt{2\pi}} \exp\left\{\frac{1}{2\sigma^2}(x_i-\theta)^2\right\} \\
p(x_1, \ldots, x_n|\theta) &amp;= \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^n \exp\left\{\sum_{i=1}^n \frac{1}{2\sigma^2}(x_i-\theta)^2\right\}
\end{align}
\end{split}\]</div>
<p>The log-likelihood is:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
\log p(x_1, \ldots, x_n|\theta) &amp;= n\log\left(\frac{1}{\sigma\sqrt{2\pi}}\right) + \sum_{i=1}^n \frac{1}{2\sigma^2}(x_i-\theta)^2
\end{align}
\]</div>
<p>Differentiating with respect to <span class="math notranslate nohighlight">\(\theta\)</span> and solving is straightforward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\sum_{i=1}^n -\frac{1}{\sigma^2}(x_i-\theta) &amp;= 0 \\
\theta &amp;= \frac{1}{n} \sum_i x_i
\end{align}
\end{split}\]</div>
<p>Once again, MLE has given us a very intuitive answer: the value for our parameter (population height average) that makes the data most likely is simply the mean of the sample heights. Note that our answer didn’t depend on <span class="math notranslate nohighlight">\(\sigma\)</span>!</p>
<p><strong>Exercise</strong>: Suppose instead of the population average, we had been interested in the population standard deviation. What is the MLE for the population standard deviation?</p>
</section>
<section id="bayesian-inference-for-heights">
<h3>Bayesian inference for heights<a class="headerlink" href="#bayesian-inference-for-heights" title="Permalink to this headline">#</a></h3>
<p>Now suppose we instead take a Bayesian approach, and treat <span class="math notranslate nohighlight">\(\theta\)</span> as random. Just like before, we’re interested in the posterior distribution:</p>
<div class="math notranslate nohighlight">
\[
p(\theta|x_1, \ldots, x_n) = \frac{p(x_1, \ldots, x_n|\theta)p(\theta)}{p(x_1, \ldots, x_n)}
\]</div>
<p>Since our parameter of interest is also a continuous variable, we could try using a normal distribution for the prior as well:</p>
<div class="math notranslate nohighlight">
\[
\theta \sim \mathcal{N}(\mu_0, \sigma_0^2)
\]</div>
<p>Note that the parameters of this distribution, <span class="math notranslate nohighlight">\(\mu_0\)</span> and <span class="math notranslate nohighlight">\(\sigma_0^2\)</span>, are different from the parameters in the likelihood! This says that our parameter <span class="math notranslate nohighlight">\(\theta\)</span>, the mean of the population, follows a normal distribution with some mean <span class="math notranslate nohighlight">\(\mu_0\)</span> and some variance <span class="math notranslate nohighlight">\(\sigma_0^2\)</span>.</p>
<p>Although the algebra is a bit more involved, we can show that the normal distribution is the conjugate prior for the mean of a normal distribution. In other words, the posterior distribution for <span class="math notranslate nohighlight">\(\theta\)</span> is also normal.</p>
<div class="math notranslate nohighlight">
\[
\theta | x_1, \ldots x_n  \sim \mathcal{N}\left(
    \frac{1}{\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}}\left(\frac{\mu_0}{\sigma_0^2} + \frac{\sum_{i=1}^n x_i}{\sigma^2}\right),
    \left(\frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}\right)^{-1} 
\right)
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You don&#39;t need to understand how this function is implemented.</span>

<span class="k">def</span> <span class="nf">plot_gaussian_prior_and_posterior</span><span class="p">(</span><span class="n">μ_0</span><span class="p">,</span> <span class="n">σ_0</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="n">range_in_σs</span><span class="o">=</span><span class="mf">3.5</span><span class="p">,</span> <span class="n">show_map</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_lmse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots prior and posterior Normaly distribution</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        μ_0, σ_0: parameters (mean, SD) of the prior distribution</span>
<span class="sd">        xs: list or array of observations</span>
<span class="sd">        σ: SD of the likelihood</span>
<span class="sd">        range_in_σs: how many SDs away from the mean to show on the plot</span>
<span class="sd">        show_map: whether or not to show the MAP estimate as a vertical line</span>
<span class="sd">        show_lmse: whether or not to show the LMSE/MMSE estimate as a vertical line</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="n">posterior_σ</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">σ_0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">n</span><span class="o">/</span><span class="p">(</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">posterior_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">posterior_σ</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">μ_0</span><span class="o">/</span><span class="p">(</span><span class="n">σ_0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    
    <span class="c1"># Compute range for plot</span>
    <span class="n">posterior_min</span> <span class="o">=</span> <span class="n">posterior_mean</span> <span class="o">-</span> <span class="p">(</span><span class="n">range_in_σs</span> <span class="o">*</span> <span class="n">posterior_σ</span><span class="p">)</span>
    <span class="n">posterior_max</span> <span class="o">=</span> <span class="n">posterior_mean</span> <span class="o">+</span> <span class="p">(</span><span class="n">range_in_σs</span> <span class="o">*</span> <span class="n">posterior_σ</span><span class="p">)</span>
    <span class="n">prior_min</span> <span class="o">=</span> <span class="n">μ_0</span> <span class="o">-</span> <span class="p">(</span><span class="n">range_in_σs</span> <span class="o">*</span> <span class="n">σ</span><span class="p">)</span>
    <span class="n">prior_max</span> <span class="o">=</span> <span class="n">μ_0</span> <span class="o">+</span> <span class="p">(</span><span class="n">range_in_σs</span> <span class="o">*</span> <span class="n">σ</span><span class="p">)</span>
    
    <span class="n">xmin</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">posterior_min</span><span class="p">,</span> <span class="n">prior_min</span><span class="p">)</span>
    <span class="n">xmax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">posterior_max</span><span class="p">,</span> <span class="n">prior_max</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_lmse</span><span class="p">:</span>
        <span class="n">x_lmse</span> <span class="o">=</span> <span class="n">posterior_mean</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_lmse</span> <span class="o">=</span> <span class="kc">None</span>
        
    <span class="k">if</span> <span class="n">show_map</span><span class="p">:</span>
        <span class="n">x_map</span> <span class="o">=</span> <span class="n">posterior_mean</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_map</span> <span class="o">=</span> <span class="kc">None</span>

    
    <span class="n">prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">μ_0</span><span class="p">,</span> <span class="n">σ_0</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior_mean</span><span class="p">,</span> <span class="n">posterior_σ</span><span class="p">)</span>
    
    
    <span class="n">plot_prior_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">),</span> <span class="s1">&#39;Prior&#39;</span><span class="p">,</span> <span class="s1">&#39;Posterior&#39;</span><span class="p">,</span>
                         <span class="n">x_map</span><span class="o">=</span><span class="n">x_map</span><span class="p">,</span> <span class="n">x_lmse</span><span class="o">=</span><span class="n">x_lmse</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_gaussian_prior_and_posterior</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="mi">12</span><span class="o">+</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="o">*</span><span class="mi">12</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="o">*</span><span class="mi">12</span><span class="o">+</span><span class="mi">9</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/parameter_estimation_33_0.png" src="../../../_images/parameter_estimation_33_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_gaussian_prior_and_posterior</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="o">*</span><span class="mi">12</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="o">*</span><span class="mi">12</span><span class="o">+</span><span class="mi">9</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/parameter_estimation_34_0.png" src="../../../_images/parameter_estimation_34_0.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/chapters/02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Data 102 Staff<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>