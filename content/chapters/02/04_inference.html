
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bayesian Inference &#8212; Data, Inference, and Decisions</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapters/02/04_inference';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Bayesian Inference with Sampling" href="05_inference_with_sampling.html" />
    <link rel="prev" title="Graphical Models, Probability Distributions, and Independence" href="03_graphical_models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data, Inference, and Decisions</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data, Inference, and Decisions
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01/intro.html">Chapter 1: Binary Decision-Making</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01/01_decisions_and_errors.html">Binary Decision-Making and Error Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/02_hypothesis_testing.html">Hypothesis Testing and p-Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/03_multiple_tests.html">Multiple Hypothesis Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/04_binary_classification.html">Binary Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../01/05_decision_theory.html">Decision Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Chapter 2: Bayesian modeling</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_parameter_estimation.html">Parameter Estimation and Bayesian Inference Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_hierarchical_models.html">Hierarchical Bayesian Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_graphical_models.html">Graphical Models</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Bayesian Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_inference_with_sampling.html">Bayesian Inference with Sampling</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../03/intro.html">Chapter 3: Prediction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../03/01_prediction.html">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/02_regression_review.html">Linear Regression Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/03_glms.html">Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/04_model_checking.html">Model Checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/05_uncertainty_quantification.html">Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/06_nonparametric.html">Nonparametric Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../03/07_neural_networks.html">Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../04/intro.html">Chapter 4: Causal Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../04/01_association_correlation_causation.html">Understanding Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/02_quantifying_association.html">Quantifying Association</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/04_randomized_experiments.html">Causality in Randomized Experiments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../04/05_observational_studies_unconfoundedness.html">Causality in Observational Studies: Unconfoundedness</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ds-102/ds-102-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapters/02/04_inference.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/chapters/02/04_inference.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian Inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-we-need-posterior-distributions">Why we need posterior distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-probabilities">Computing probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-estimation">MAP Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lmse-estimation">LMSE Estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-computing-posterior-distributions-is-hard">Why computing posterior distributions is hard</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-dimensional-non-conjugate-prior">One-dimensional non-conjugate prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensional-example">Multi-dimensional example</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">YouTubeVideo</span>

<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="bayesian-inference">
<h1>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Link to this heading">#</a></h1>
<p>In this section, we’ll focus on computing and using posterior distributions in more sophisticated Bayesian models. We’ll start by discussing why posterior distributions are useful in Bayesian inference, and then explain why they’re hard. Then, in the next section, we’ll learn about approximating distributions using sampling.</p>
<section id="why-we-need-posterior-distributions">
<h2>Why we need posterior distributions<a class="headerlink" href="#why-we-need-posterior-distributions" title="Link to this heading">#</a></h2>
<p>In general, we need the posterior distribution so that we can make statements and decisions about our unknown quantity of interest, <span class="math notranslate nohighlight">\(\theta\)</span>. We saw that for simple models like the product review model or the model for heights, it was easy to compute the posterior exactly, because we chose a conjugate prior.</p>
<p>In the product review example:</p>
<ul class="simple">
<li><p>Our parameter of interest <span class="math notranslate nohighlight">\(\theta\)</span> represents the probability of a positive review.</p></li>
<li><p>If we chose a Beta prior, i.e., <span class="math notranslate nohighlight">\(\theta \sim \mathrm{Beta}(\alpha, \beta)\)</span>, then the posterior distribution also belonged to the Beta family: <span class="math notranslate nohighlight">\(\theta | x \sim \mathrm{Beta}\left(\alpha + \sum x_i, \beta + n - \sum x_i\right)\)</span>.</p></li>
<li><p>This made it easy to determine things like the MAP estimate or LMSE estimate, simply by using known properties of the Beta distribution.</p></li>
</ul>
<p>But what if our posterior distribution didn’t have such a convenient form? In that case, we would have to compute the posterior (and any estimates from it) ourselves:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(\theta|x)
&amp;= \frac{p(x|\theta)p(\theta)}{p(x)} \\
&amp;= \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta)\,d\theta} \\
\end{align}
\end{split}\]</div>
<p>In general, the integral in the denominator could be impossible to compute. We call the denominator the <strong>normalizing constant</strong>: it’s a constant because it doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>, and it’s normalizing because we need it for the distribution or density to sum or integrate to 1.</p>
<p>In the next section, we’ll see a few examples that illustrate why computing the normalizing constant is hard, but first, let’s look at three examples of why we might need to know it in the first place.</p>
<section id="computing-probabilities">
<h3>Computing probabilities<a class="headerlink" href="#computing-probabilities" title="Link to this heading">#</a></h3>
<p>Suppose we want to know the probability that <span class="math notranslate nohighlight">\(\theta\)</span> is greater than 0.7, given the observed data. In this case, we can set up an integral to compute this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
P(\theta &gt; 0.7 | x)
&amp;= \int_{0.7}^1 p(\theta|x) \, dx \\
&amp;= \int_{0.7}^1 \frac{p(x|\theta)p(\theta)}{p(x)} \, dx \\
&amp;= \frac{1}{p(x)} \int_{0.7}^1 p(x|\theta)p(\theta) \, dx
\end{align}
\end{split}\]</div>
<p>In the last step, we used the fact that p(x) doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>If we don’t know <span class="math notranslate nohighlight">\(p(x)\)</span>, then our probability will be off by an unknown factor. For example, suppose the true probability is 0.9, the integral is 0.0009, and the normally-unknown denominator <span class="math notranslate nohighlight">\(p(x)\)</span> is <span class="math notranslate nohighlight">\(0.001\)</span>. In this case, if we don’t know the normalizing constant, there’s no way we can determine the probability: we’ll always be wrong by an unknown factor, which means that our answer is useless.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/WOS7iFlsN5c"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="map-estimation">
<h3>MAP Estimation<a class="headerlink" href="#map-estimation" title="Link to this heading">#</a></h3>
<p>Suppose we want to compute the MAP estimate:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat{\theta}_{MAP} 
&amp;= \underset{\theta}{\operatorname{argmax}} p(\theta|x) \\
&amp;= \underset{\theta}{\operatorname{argmax}} \frac{p(x|\theta)p(\theta)}{p(x)} \\
&amp;= \underset{\theta}{\operatorname{argmax}} p(x|\theta)p(\theta) \\
\end{align}
\end{split}\]</div>
<p>In the last step, we used the fact that p(x) doesn’t depend on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\theta\)</span> is low-dimensional and continuous, we can often optimize this either analytically or sometimes numerically. If <span class="math notranslate nohighlight">\(\theta\)</span> is discrete and doesn’t take on too many different values, we can search over all possible values. However, if <span class="math notranslate nohighlight">\(\theta\)</span> is discrete and takes on an intractably large number of possible values, then we’d need to search over all of them, which would be impossible. Similarly, if <span class="math notranslate nohighlight">\(\theta\)</span> is high-dimensional, then the search can also be intractable.</p>
<p>To summarize: for low-dimensional continuous variables, or discrete random variables with a low number of possible values, we can compute the MAP estimate without needing to know the exact posterior. For higher-dimensional random variables and/or discrete random variables with many possible values, this won’t work.</p>
</section>
<section id="lmse-estimation">
<h3>LMSE Estimation<a class="headerlink" href="#lmse-estimation" title="Link to this heading">#</a></h3>
<p>Suppose we want to compute the LMSE estimate. Recall the definition of conditional expectation (see Data 140 textbook, <a class="reference external" href="http://prob140.org/textbook/content/Chapter_09/02_Expectation_by_Conditioning.html">Chapter 9</a> and <a class="reference external" href="http://prob140.org/textbook/content/Chapter_15/03_Expectation.html">Chapter 15</a>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat{\theta}_{LMSE} 
&amp;= E_{\theta|x}[\theta] \\
&amp;= \int \theta \cdot p(\theta|x) \, d\theta \\
&amp;= \int \theta \cdot \frac{p(x|\theta)p(\theta)}{p(x)} \, d\theta \\
&amp;= \frac{1}{p(x)} \int \theta \cdot p(x|\theta)p(\theta)\, d\theta 
\end{align}
\end{split}\]</div>
<p>In order to compute the LMSE estimate, we need to compute the denominator, <span class="math notranslate nohighlight">\(p(x)\)</span>. If we don’t know it, then our estimate will be off by a multiplicative factor that we don’t know, making it effectively useless.</p>
<p>The same is true for computing the expected value of any other function of <span class="math notranslate nohighlight">\(\theta\)</span>, or any other probability involving the posterior distribution. For example, answering the question “according to the posterior distribution, what is the variance of <span class="math notranslate nohighlight">\(\theta\)</span>?” will lead to the same problem.</p>
<p>To summarize: any computations involving the posteriors (probabilities, expectations, etc.) require us to have the full normalized distribution: the numerator in Bayes’ rule isn’t enough.</p>
</section>
</section>
<section id="why-computing-posterior-distributions-is-hard">
<h2>Why computing posterior distributions is hard<a class="headerlink" href="#why-computing-posterior-distributions-is-hard" title="Link to this heading">#</a></h2>
<p>In simple models like our product review model or our model for heights, it was easy to compute the exact posterior for the unknown variable that we were interested in. This happened because we chose a conjugate prior. In most other cases, computing the exact posterior is hard! Here are two examples:</p>
<section id="one-dimensional-non-conjugate-prior">
<h3>One-dimensional non-conjugate prior<a class="headerlink" href="#one-dimensional-non-conjugate-prior" title="Link to this heading">#</a></h3>
<p>Let’s return to the product review example, but this time, instead of a Beta prior, we choose <span class="math notranslate nohighlight">\(p(\theta) = \frac{2}{\pi}\cos\left(\frac{\pi}{2} \theta\right)\)</span> for <span class="math notranslate nohighlight">\(\theta \in [0, 1]\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(\theta|x) 
    &amp;\propto p(x|\theta)p(\theta) \\
    &amp;\propto \Big[\theta^{\left[\sum_i x_i\right]}(1-\theta)^{\left[\sum_i (1-x_i)\right]}\Big]\cos\left(\frac{\pi}{2}\theta\right)
\end{align}
\end{split}\]</div>
<p>This distribution looks much more complicated: we can’t reduce it to a known distribution at all. So, in order to properly compute <span class="math notranslate nohighlight">\(p(\theta|x)\)</span>, we’d need to figure out the normalizing constant. This requires solving the integral:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
p(x) &amp;= \int_0^1 \Big[\theta^{\left[\sum_i x_i\right]}(1-\theta)^{\left[\sum_i (1-x_i)\right]}\Big]\cos\left(\frac{\pi}{2}\theta\right)\,d\theta
\end{align}
\]</div>
<p>This integral is difficult to solve in closed form. In this specific example, since this is a one-dimensional problem, we could take advantage of numerical integration. In other words, for a particular sequence of values <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>, we can plug them in and compute a numerical approximation to the integral, and then find the normalizing constant that way. As we saw above, we don’t need the normalizing constant if we’re only interested in the MAP estimate, but we can’t compute the LMSE estimate without it.</p>
</section>
<section id="multi-dimensional-example">
<h3>Multi-dimensional example<a class="headerlink" href="#multi-dimensional-example" title="Link to this heading">#</a></h3>
<p>Consider the exoplanet model from last time: <span class="math notranslate nohighlight">\(x_i\)</span> is the (observed) radius of planet <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(z_i\)</span> is whether the planet belongs to group 0 (small, possibly habitable planets) or group 1 (large, possibly inhabitable planets), and <span class="math notranslate nohighlight">\(\mu_0\)</span> and <span class="math notranslate nohighlight">\(\mu_1\)</span> are the mean radii of those two groups, respectively.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    z_i &amp;\sim \mathrm{Bernoulli}(\pi) &amp; i = 1, \ldots, n \\
    \mu_k &amp;\sim \mathcal{N}(\mu_p, \sigma_p) &amp;  k =0, 1 \\
    x_i | z_i, \mu_0, \mu_1 &amp;\sim \mathcal{N}(\mu_{z_i}, \sigma) &amp; i = 1, \ldots, n\\
\end{align}
\end{split}\]</div>
<p>We can write the likelihood and prior. To simplify, we’ll write <span class="math notranslate nohighlight">\(\mathcal{N}(y; m, s) = \frac{1}{s \sqrt{2\pi}} \exp\left\{-\frac{1}{2s^2}(y - m)^2\right\}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    p(z_i) &amp;= \pi^{z_i}(1-\pi)^{1-z_i} \\
    p(\mu_k) &amp;= \mathcal{N}(\mu_k; \mu_p, \sigma_p) \\
    p(x_i | z_i, \mu_0, \mu_1) &amp;= \mathcal{N}(x_i; \mu_{z_i}, \sigma)
\end{align}
\end{split}\]</div>
<p>We can try computing the posterior over the hidden variables <span class="math notranslate nohighlight">\(z_i\)</span>, <span class="math notranslate nohighlight">\(\mu_0\)</span>, and <span class="math notranslate nohighlight">\(\mu_1\)</span>. We’ll use the notation <span class="math notranslate nohighlight">\(z_{1:n}\)</span> to represent <span class="math notranslate nohighlight">\(z_1, \dots, z_n\)</span> (and similarly for <span class="math notranslate nohighlight">\(x_{1:n}\)</span>).</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
    p(z_{1:n}, \mu_0, \mu_1 | x_{1:n}) &amp;\propto p(\mu_0)p(\mu_1)\prod_i \left[p(z_i) p(x_i | z_i, \mu_0, \mu_1)\right]
\end{align}
\]</div>
<p>This distribution is more complicated than anything we’ve seen up until now. It’s the joint distribution over <span class="math notranslate nohighlight">\(n+2\)</span> random variables (the group labels <span class="math notranslate nohighlight">\(z_1, \ldots, z_n\)</span> and the two group means <span class="math notranslate nohighlight">\(\mu_0\)</span> and <span class="math notranslate nohighlight">\(\mu_1\)</span>).</p>
<p>Computing the normalization constant <span class="math notranslate nohighlight">\(p(x_{1:n})\)</span> requires a complicated combination of sums and integrals:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
p(x_{1:n}) &amp;= \sum_{z_1=0}^1 \sum_{z_2=0}^1 \ldots \sum_{z_n=0}^1 \int \int p(\mu_0)p(\mu_1)\prod_i \left[p(z_i) p(x_i | z_i, \mu_0, \mu_1)\right] d\mu_0 d\mu_1
\end{align}
\]</div>
<p>For our dataset of over 500 planets, the sums alone would require a completely intractable amount of computation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">2</span><span class="o">**</span><span class="mi">517</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>429049853758163107186368799942587076079339706258956588087153966199096448962353503257659977541340909686081019461967553627320124249982290238285876768194691072
</pre></div>
</div>
</div>
</div>
<p>Worse still, we can’t even compute the MAP estimate for the labels <span class="math notranslate nohighlight">\(z_i\)</span>: in order to find the one that maximizes the numerator, we’d have to search over all <span class="math notranslate nohighlight">\(2^{517}\)</span> combinations, which is also completely intractable.</p>
<p>Even in this fairly simple model, with two groups, we’ve found that exact inference is completely hopeless: there’s no way we can compute the exact posterior for all our unknowns. In the rest of this section, we’ll talk about ways to get around this problem using approximations to the posterior distribution.</p>
<p>Specifically, our approximation methods will take advantage of what we’ve learned. We know that the hardest part of computing posterior distributions is computing the normalization constant <span class="math notranslate nohighlight">\(p(x)\)</span>. So, we’ll build methods that start with the unnormalized posterior <span class="math notranslate nohighlight">\(p(x|\theta)p(\theta)\)</span> and use that to give us an approximation of the actual posterior <span class="math notranslate nohighlight">\(p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}\)</span>. While there are multiple families of methods to provide such approximations, in this textbook we’ll focus on ones that use <strong>samples</strong> to approximate the distribution.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/BlQ6IVoJ0X8"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/chapters/02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_graphical_models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Graphical Models, Probability Distributions, and Independence</p>
      </div>
    </a>
    <a class="right-next"
       href="05_inference_with_sampling.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian Inference with Sampling</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-we-need-posterior-distributions">Why we need posterior distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-probabilities">Computing probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#map-estimation">MAP Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lmse-estimation">LMSE Estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-computing-posterior-distributions-is-hard">Why computing posterior distributions is hard</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-dimensional-non-conjugate-prior">One-dimensional non-conjugate prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-dimensional-example">Multi-dimensional example</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Data 102 Staff
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>