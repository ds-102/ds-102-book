{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "judicial-gospel",
   "metadata": {},
   "source": [
    "# Causal Inference in Observational Studies: Unconfoundedness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-garden",
   "metadata": {},
   "source": [
    "So far, we've looked at ways of measuring association in any dataset. We've also looked at how we can draw conclusions about causality in randomized controlled trials. But in many cases, randomized controlled trials are not viable. Some reasons for that might be:\n",
    "\n",
    "* **Ethical considerations**: suppose we believe that a certain treatment might harm people. In order to test this belief in a randomized controlled trial, we would have to subject people to a treatment that we believe would harm them, which is profoundly unethical. In such cases, we need alternative ways of drawing causal conclusions: we have an obligation as data scientists and researchers to avoid making [the mistakes of the past](https://en.wikipedia.org/wiki/Unethical_human_experimentation_in_the_United_States).\n",
    "* **Cost**: Randomized controlled trials can be expensive to run: we must find randomly chosen individuals that are representative the population; apply a treatment to half of them; and measure the outcomes, potentially over a long range of time. In some cases, this may be infeasible.\n",
    "* **Sample size**: due to cost considerations, randomized controlled trials may not have enough subjects to draw strong statistical conclusions.\n",
    "\n",
    "There is a wealth of data available on observational studies. In this section, we'll look at ways we can use this data to draw causal conclusions. As we saw when looking at association, the most common problem when trying to draw conclusions about causality is the presence of confounding variables. We'll focus our attention on methods that try to account for these confounding variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-military",
   "metadata": {},
   "source": [
    "## Why it's hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-harassment",
   "metadata": {},
   "source": [
    "When dealing with data from observational studies, we can't just take the simple difference in means anymore. To see why not, let's return to our restaurants example. We'll do the same computations we did before, but this time from the potential outcomes perspective.\n",
    "\n",
    "Consider the science table version of the data (where we've converted üëç to 1 and üëé to 0, so that averages correspond to percentages liked):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fantastic-tokyo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dish rating for A</th>\n",
       "      <th>Dish rating for B</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dish rating for A  Dish rating for B  Year\n",
       "0                0.0                NaN  2020\n",
       "1                0.0                NaN  2020\n",
       "2                1.0                NaN  2020\n",
       "3                NaN                0.0  2019\n",
       "4                1.0                NaN  2020"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "food = pd.read_csv('data/restaurants_counterfactuals.csv')\n",
    "food.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-scanner",
   "metadata": {},
   "source": [
    "The `NaN` values indicate missing counterfactual outcomes that we don't get to observe: for example, dish 0 was ordered from Restaurant A, so we don't know what rating the critics would have given a dish from Restaurant B on that day.\n",
    "\n",
    "Displaying the data this way requires us to think about it slightly differently, and highlights the implications of the potential outcomes framework! In particular:\n",
    "* $Z_i$ corresponds to whether the critic ordered dish $i$ from restaurant A or B.\n",
    "* $Y_i(A)$ and $Y_i(B)$ represent what-ifs: *if* the critics had ordered dish $i$ from a particular restaurant, would they have liked it?\n",
    "* The year is our confounder (or confounding variable): we'll use $X_i$ to denote the year that dish $i$ was ordered in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-anthropology",
   "metadata": {},
   "source": [
    "Our representation of the data this way makes it clear that we can't compute the ATE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "synthetic-recall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_ATE = (food[\"Dish rating for B\"] - food[\"Dish rating for A\"]).mean()\n",
    "food_ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-pharmacology",
   "metadata": {},
   "source": [
    "In trying to understand a causal relationship between choice of restaurant and liking the food, naively using the simple difference in observed means (SDO) can mislead us, as we already saw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bored-swedish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.20000000000000007)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_sdo = food[\"Dish rating for B\"].mean() - food[\"Dish rating for A\"].mean()\n",
    "food_sdo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-supplier",
   "metadata": {},
   "source": [
    "## Confounding variables and the superpopulation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-glasgow",
   "metadata": {},
   "source": [
    "In the case of a randomized controlled trial, we saw that we could use the difference in means estimator because of the independence between the treatment assignment $Z_i$ and the potential outcomes $\\big(Y_i(0), Y_i(1)\\big)$. But, in the case of an observational study, this independence assumption no longer holds. We've seen a few examples of this in the last few sections, where confounding variables affect both the treatment and the outcome.\n",
    "\n",
    "In this section, we'll focus on a few methods that answer the question: if we also observe the confounding variable, can we determine whether the treatment has an effect on the outcome?\n",
    "\n",
    "Our notation will be similar to what we've used before, and we'll add a new variable $X_i$ for each unit that represents the confounding information. We have tuples $(Z_i, Y_i(0), Y_i(1), X_i)$ for each unit that are i.i.d. (that is, the tuple for one unit is independent from the tuple for the next unit). This is called the **superpopulation model**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-brook",
   "metadata": {},
   "source": [
    "## Conditional average treatment effect and unconfoundedness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-insertion",
   "metadata": {},
   "source": [
    "We'll define a new quantity called the **conditional average treatment effect (CATE)**. This is like the ATE we saw before, but conditioned on a particular value of $X$:\n",
    "\n",
    "$$\n",
    "\\text{CATE} = \\tau(x) = E[Y(1) - Y(0) | X = x]\n",
    "$$\n",
    "\n",
    "Introducing the conditioning alone doesn't help us: if we were to expand this using the tower property to condition on $Z$, we'll still have counterfactual terms. So why did we do it? It turns out that we only need to make one assumption for this to help us. This assumption is called **unconfoundedness**:\n",
    "\n",
    "$$\n",
    "    \\big(Y(0), Y(1)\\big) \\perp \\!\\!\\! \\perp Z \\mid X\n",
    "$$\n",
    "\n",
    "This is a statement of conditional independence. The potential outcomes $Y(0)$ and $Y(1)$ might not be independent of the treatment decision $Z$, but this assumption states that they are conditionally independent *if* we know the value of a confounding variable $X$. \n",
    "\n",
    "In the restaurant example above, unconfoundedness would mean that within each year, the decision whether to order from A or B (treatment) is independent of the ratings that dishes from A and B would get (potential outcomes). It does *not* state that the restaurant is conditionally independent of the rating: this is a subtle but important distinction.\n",
    "\n",
    "If we assume unconfoundedness, then we can safely estimate the CATE using the difference in conditional means:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\tau(x) \n",
    "        &= E[Y(1) - Y(0) | X = x] &\\\\\n",
    "        &= E[Y(1) | X = x] - E[Y(0) | X = x] & {\\tiny\\text{(linearity of (conditional) expectation)}}\\\\\n",
    "        &= E[Y(1) | X = x, Z = 1] - E[Y(0) | X = x, Z = 0] & {\\tiny\\text{(unconfoundedness)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### From CATE to ATE\n",
    "\n",
    "In most cases, what we're really interested in is the ATE. How do we get from the CATE to the ATE? We can use the law of iterated expectation (tower property):\n",
    "\n",
    "$$\n",
    "E[Y(1) - Y(0)] = E_X[E_Y[Y(1)-Y(0) | X]]\n",
    "$$\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    ATE \n",
    "        &= E[\\tau(X)] \\\\\n",
    "        &= \\sum_x \\tau(x) P(X=x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(where we would use an integral instead of a sum if $X$ is continuous).\n",
    "\n",
    "What does this look like in practice? Let's go back to the restaurant example. Instead of computing the difference in means for the entire dataset, we'll do it separately, once for 2020 and once for 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "august-denmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05555555555555558, -0.125\n"
     ]
    }
   ],
   "source": [
    "food_2020 = food[food['Year'] == 2020]\n",
    "food_2019 = food[food['Year'] == 2019]\n",
    "\n",
    "food_sdo_2020 = food_2020[\"Dish rating for B\"].mean() - food_2020[\"Dish rating for A\"].mean()\n",
    "food_sdo_2019 = food_2019[\"Dish rating for B\"].mean() - food_2019[\"Dish rating for A\"].mean()\n",
    "print(f\"{food_sdo_2020}, {food_sdo_2019}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-testimony",
   "metadata": {},
   "source": [
    "These correspond to the CATEs $\\tau(2020)$ and $\\tau(2019)$ respectively. We can interpret $\\tau(2019)$ in English as: \"given that a dish was ordered in 2019, what is the causal effect of choosing restaurant B (rather than A) on the probability of the critics liking their food?\", and similarly for $\\tau(2020)$.\n",
    "\n",
    "Just like earlier, we see that these values have the opposite sign from the SDO computed on the entire data: Restaurant A was better-liked in 2020 and 2019. But, because the critics ordered from Restaurant A more in 2020 (which had lower ratings overall), Restaurant A looks worse in the aggregate data.\n",
    "\n",
    "How do we combine these to estimate the ATE? We need the marginal probabilities $P(2020)$ and $P(2019)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "increasing-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_2019=np.float64(0.33333333333333337), p_2020=np.float64(0.6666666666666666)\n",
      "Estimated ATE: -0.07870370370370372\n"
     ]
    }
   ],
   "source": [
    "p_2020 = (food['Year'] == 2020).mean()\n",
    "p_2019 = 1 - p_2020\n",
    "print(f\"{p_2019=}, {p_2020=}\")\n",
    "\n",
    "est_food_ATE = food_sdo_2020 * p_2020 + food_sdo_2019 * p_2019\n",
    "print(\"Estimated ATE:\", est_food_ATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-conditioning",
   "metadata": {},
   "source": [
    "This is our first causal inference from observational data! To state it formally:\n",
    "\n",
    "**If we assume that the year is the only confounder for the effect of restaurant choice on reviews, then choosing Restaurant A causes the critics to like dishes about 8% more**.\n",
    "\n",
    "When we assume that the year is the only confounder, we're assuming **unconfoundedness*: that the pair of potential outcomes (i.e., the what-ifs for their reviews) are independent of which restaurant they actually chose, given the year.\n",
    "\n",
    "Note that we can't just make a blanket statement about causality: this statement depends heavily on our use of the unconfoundedness assumption. This is true in general: in order to draw causal inferences outside of randomized controlled experiments, we need to make assumptions. When reporting your causal conclusions, it's always important to communicate the assumptions you made!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-suite",
   "metadata": {},
   "source": [
    "## Outcome Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-advocate",
   "metadata": {},
   "source": [
    "*Coming soon*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-fundamental",
   "metadata": {},
   "source": [
    "## Inverse propensity weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-photographer",
   "metadata": {},
   "source": [
    "In inverse propensity weighting, we're intuitively trying to undo or compensate for the effect of the confounder by reweighting the outcome variables based on the treatment and confounders. To illustrate the effect, we'll use a synthetic dataset from a treatment for high blood pressure for twenty people. The data contain three columns: \n",
    "* `treated`, which says whether or not each person received the treatment\n",
    "* `is_old`, whether the patient is older than 45 (1) or not (0)\n",
    "* `bp_change`, how much the person's (systolic) blood pressure dropped (in mm Hg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee89634-131b-48d6-8a3e-97731d97c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treated</th>\n",
       "      <th>is_old</th>\n",
       "      <th>bp_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    treated  is_old  bp_change\n",
       "0         0       0       -0.2\n",
       "1         0       0       -3.4\n",
       "2         0       0       -4.0\n",
       "3         0       0       -5.6\n",
       "4         0       0       -2.9\n",
       "5         0       0       -3.8\n",
       "6         0       0       -4.7\n",
       "7         0       1       -0.7\n",
       "8         0       1       -0.9\n",
       "9         1       0       -5.2\n",
       "10        1       0       -4.8\n",
       "11        1       0       -5.9\n",
       "12        1       0       -6.1\n",
       "13        1       1       -1.1\n",
       "14        1       1       -1.5\n",
       "15        1       1       -2.1\n",
       "16        1       1       -0.8\n",
       "17        1       1       -1.7\n",
       "18        1       1       -4.9\n",
       "19        1       1       -5.4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp = pd.read_csv('data/bp_treatment.csv')\n",
    "bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b10123f6-057a-4da4-92e7-1e8afa4b9971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bp_change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_old</th>\n",
       "      <th>treated</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>-3.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bp_change\n",
       "is_old treated           \n",
       "0      0        -3.514286\n",
       "       1        -5.500000\n",
       "1      0        -0.800000\n",
       "       1        -2.500000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp.groupby(['is_old', 'treated']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d84ae-21db-4dcd-a7d5-ba668476849d",
   "metadata": {},
   "source": [
    "From a quick look at the data, we can see a few facts:\n",
    "* Most of the people who received the treatment were older\n",
    "* Almost all the younger peoples' BP improved (i.e., went down)\n",
    "* Most of the older peoples' BP did not improve\n",
    "* We have very few data points. There are only twenty people overall, and even fewer once we slice the data up. For example, there are only two older people who took the treatment.\n",
    "\n",
    "We'll ignore the last point for now, for the sake of illustration. The first three points make it very clear that *age is a confounding variable* for the effect of the treatment. One way to quantify the degree of confounding is using the conditional probability of receving the treatment given the confounder: this is called the **propensity score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6fc01c-e1c4-456f-b0f7-f94404e1973f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bp_change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_old</th>\n",
       "      <th>treated</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bp_change\n",
       "is_old treated           \n",
       "0      0                7\n",
       "       1                4\n",
       "1      0                2\n",
       "       1                7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp.groupby(['is_old', 'treated']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a700c-73ce-4d5a-a8ed-9ecb6b693f76",
   "metadata": {},
   "source": [
    "For the older group, the probability of receiving the treatment is $7/9 \\approx 0.78$. For the younger group, the probability is $4/11 \\approx 0.36$. We can add these into the table:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3addb1a4-2946-4a44-b94e-fe02157f8cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treated</th>\n",
       "      <th>is_old</th>\n",
       "      <th>bp_change</th>\n",
       "      <th>propensity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    treated  is_old  bp_change  propensity_score\n",
       "0         0       0       -0.2          0.363636\n",
       "1         0       0       -3.4          0.363636\n",
       "2         0       0       -4.0          0.363636\n",
       "3         0       0       -5.6          0.363636\n",
       "4         0       0       -2.9          0.363636\n",
       "5         0       0       -3.8          0.363636\n",
       "6         0       0       -4.7          0.363636\n",
       "7         0       1       -0.7          0.777778\n",
       "8         0       1       -0.9          0.777778\n",
       "9         1       0       -5.2          0.363636\n",
       "10        1       0       -4.8          0.363636\n",
       "11        1       0       -5.9          0.363636\n",
       "12        1       0       -6.1          0.363636\n",
       "13        1       1       -1.1          0.777778\n",
       "14        1       1       -1.5          0.777778\n",
       "15        1       1       -2.1          0.777778\n",
       "16        1       1       -0.8          0.777778\n",
       "17        1       1       -1.7          0.777778\n",
       "18        1       1       -4.9          0.777778\n",
       "19        1       1       -5.4          0.777778"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp['propensity_score'] = bp['is_old'] * 7/9 + (1-bp['is_old']) * 4/11\n",
    "bp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed790f-f1fa-41ba-b740-fbc8460dc8cf",
   "metadata": {},
   "source": [
    "This tells us that person 0 had a $0.36$ probability of receiving the treatment, given that they were younger, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd831e-faa9-4e0c-92cf-6306910692d5",
   "metadata": {},
   "source": [
    "### Reweighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e5516-602b-4a4e-85f5-202e1309577f",
   "metadata": {},
   "source": [
    "If we try to naively compute a causal effect using the data above, we'll be led astray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52705e7-a4cc-4344-a4e6-3d3275d47ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.6797979797979803)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_mean_confounded = bp.loc[bp['treated'] == 1, 'bp_change'].mean()\n",
    "control_mean_confounded = bp.loc[bp['treated'] == 0, 'bp_change'].mean()\n",
    "treatment_mean_confounded - control_mean_confounded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c9880-a878-4f37-a185-31ba8d156444",
   "metadata": {},
   "source": [
    "The effect appears to be very small: only a drop of 0.68mm Hg. This is because of the confounding effect: younger people are overrepresented in the control group, and underrepresented in the treatment group (and vice versa for older people).\n",
    "\n",
    "What if we try to adjust for the overrepresentation/underrepresentation by re-weighting each data point? Let's start by looking at the treatment group. For this group, we should **increase** the weight on the younger people (because there are too few of them). There are many ways we could do this: one is to divide by the propensity score. In particular, the approach we're going to take will give us an **unbiased estimate of the ATE**, even in the presence of confounding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b555e95e-e86b-420a-b06a-e1e4349da6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treated</th>\n",
       "      <th>is_old</th>\n",
       "      <th>bp_change</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>outcome_reweighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-14.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-13.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-16.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-16.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.414286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-2.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-2.185714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-6.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-6.942857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    treated  is_old  bp_change  propensity_score  outcome_reweighted\n",
       "0         0       0       -0.2          0.363636                 NaN\n",
       "1         0       0       -3.4          0.363636                 NaN\n",
       "2         0       0       -4.0          0.363636                 NaN\n",
       "3         0       0       -5.6          0.363636                 NaN\n",
       "4         0       0       -2.9          0.363636                 NaN\n",
       "5         0       0       -3.8          0.363636                 NaN\n",
       "6         0       0       -4.7          0.363636                 NaN\n",
       "7         0       1       -0.7          0.777778                 NaN\n",
       "8         0       1       -0.9          0.777778                 NaN\n",
       "9         1       0       -5.2          0.363636          -14.300000\n",
       "10        1       0       -4.8          0.363636          -13.200000\n",
       "11        1       0       -5.9          0.363636          -16.225000\n",
       "12        1       0       -6.1          0.363636          -16.775000\n",
       "13        1       1       -1.1          0.777778           -1.414286\n",
       "14        1       1       -1.5          0.777778           -1.928571\n",
       "15        1       1       -2.1          0.777778           -2.700000\n",
       "16        1       1       -0.8          0.777778           -1.028571\n",
       "17        1       1       -1.7          0.777778           -2.185714\n",
       "18        1       1       -4.9          0.777778           -6.300000\n",
       "19        1       1       -5.4          0.777778           -6.942857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp['outcome_reweighted'] = bp['bp_change'] / bp['propensity_score']\n",
    "bp.loc[bp['treated'] == 0, 'outcome_reweighted'] = pd.NA  # only look at treatment group for now\n",
    "bp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5d51a-26d6-4465-b119-c4ffcb0bd124",
   "metadata": {},
   "source": [
    "This gives a significantly increased weight to the improvements from the younger group (and only a very slightly increased weight to the improvements from the older group).\n",
    "\n",
    "What about the control group? There, we want to increase the weight on the older people. Dividing by the propensity score would have the opposite effect from what we want: instead, we'll divide by 1 minus the propensity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad23c48-a91d-4b97-a2c6-05ea6807b7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treated</th>\n",
       "      <th>is_old</th>\n",
       "      <th>bp_change</th>\n",
       "      <th>propensity_score</th>\n",
       "      <th>outcome_reweighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-0.314286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-5.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-6.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-4.557143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-5.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-7.385714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-3.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-4.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-14.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-13.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-16.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>-16.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.414286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-2.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-2.185714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-6.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>-6.942857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    treated  is_old  bp_change  propensity_score  outcome_reweighted\n",
       "0         0       0       -0.2          0.363636           -0.314286\n",
       "1         0       0       -3.4          0.363636           -5.342857\n",
       "2         0       0       -4.0          0.363636           -6.285714\n",
       "3         0       0       -5.6          0.363636           -8.800000\n",
       "4         0       0       -2.9          0.363636           -4.557143\n",
       "5         0       0       -3.8          0.363636           -5.971429\n",
       "6         0       0       -4.7          0.363636           -7.385714\n",
       "7         0       1       -0.7          0.777778           -3.150000\n",
       "8         0       1       -0.9          0.777778           -4.050000\n",
       "9         1       0       -5.2          0.363636          -14.300000\n",
       "10        1       0       -4.8          0.363636          -13.200000\n",
       "11        1       0       -5.9          0.363636          -16.225000\n",
       "12        1       0       -6.1          0.363636          -16.775000\n",
       "13        1       1       -1.1          0.777778           -1.414286\n",
       "14        1       1       -1.5          0.777778           -1.928571\n",
       "15        1       1       -2.1          0.777778           -2.700000\n",
       "16        1       1       -0.8          0.777778           -1.028571\n",
       "17        1       1       -1.7          0.777778           -2.185714\n",
       "18        1       1       -4.9          0.777778           -6.300000\n",
       "19        1       1       -5.4          0.777778           -6.942857"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control = bp['treated'] == 0\n",
    "bp.loc[control, 'outcome_reweighted'] = (\n",
    "    bp.loc[control, 'bp_change'] / (1 - bp.loc[control, 'propensity_score'])\n",
    ")\n",
    "bp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c304e313-df28-4d2a-a135-9a8b74b55360",
   "metadata": {},
   "source": [
    "Now, we can compare the difference in reweighted means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bcf3055-b3fb-4a44-9dc6-425dc649a872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.857142857142857)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_total = bp.loc[bp['treated'] == 1, 'outcome_reweighted'].sum()\n",
    "control_total = bp.loc[bp['treated'] == 0, 'outcome_reweighted'].sum()\n",
    "\n",
    "(treatment_total - control_total)/20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41cad2-f6ad-4046-85f2-e4729799f95e",
   "metadata": {},
   "source": [
    "We can see that this does a much better job of capturing the real effect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874cc35-1c80-4cbd-bdf5-21dcf74226e3",
   "metadata": {},
   "source": [
    "### Inverse Propensity Score Weighting (IPW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c299a-51b5-4c8e-8028-10a1ab6abdf3",
   "metadata": {},
   "source": [
    "Using the logic above, and our ad hoc processing of the table, we're now ready to define the IPW (inverse propensity weighted) estimator for the ATE:\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}_{IPW} =\n",
    "    \\frac{1}{n}\n",
    "    \\underbrace{%\n",
    "         \\sum_{i: Z_i = 1} \\frac{Y_i}{e(X_i)}\n",
    "    }_{\\text{reweighted treated rows}}\n",
    "    -\n",
    "    \\frac{1}{n}\n",
    "    \\underbrace{%\n",
    "        \\sum_{i: Z_i = 0} \\frac{Y_i}{1-e(X_i)}\n",
    "    }_{\\text{reweighted control rows}}\n",
    "$$\n",
    "\n",
    "This is a particularly useful estimator because **if the unconfoundedness assumption** (also known as the conditional independence assumption or CIA) **is true, then this will be an unbiased estimate of the true ATE**: $E[\\hat{\\tau}_{IPW}] = \\tau$.\n",
    "\n",
    "\n",
    "Intuitively, why does it make sense to divide by the propensity score (for the treatment group) and one minus the propensity score (for the control group)? We're trying to find units that were less likely to end up in the group that they did, and give those higher weight. Recall that the propensity score and its complement are conditional probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "e(x) &= P(Z=1 | X=x) \\\\\n",
    "1 - e(x) &= P(Z=0 | X=x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the treatment group, we want to find units that, because of confounding, were unlikely to end up in the treatment group (in the example above, this corresponds to finding younger people). Those units will have a very small propensity score, so we can divide by it to increase the weight on them.\n",
    "\n",
    "For the control group, we want to find units that, because of confounding, were unlikely to end up in the control group (in the example above, this corresponds to finding older people). Those units will have a very **large** propensity score (close to 1), so we can divide by 1 minus the propensity score to increase the weight on them.\n",
    "\n",
    "The same process will give **less** weight to units that are overrepresented in their group.\n",
    "\n",
    "For a proof that the IPW estimator above gives an **unbiased estimate** of the true ATE, see one of these sources. Try the proof yourself as an exercise before looking at the answer!\n",
    "\n",
    "* [A First Course in Causal Inference](https://arxiv.org/pdf/2305.18793) by Peng Ding (Section 11.2.1, pg. 159) has a proof that uses the same notation as this book.\n",
    "* [Causal Inference: The Mixtape](https://mixtape.scunning.com/05-matching_and_subclassification#weighting-on-the-propensity-score) by Scott Cunningham has a proof that uses slightly different notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabacb8a-ee9b-45ec-a71a-03736a02fc33",
   "metadata": {},
   "source": [
    "### Computing propensity scores using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3fdf3f-97f6-4c50-9baf-6a7747b5c06e",
   "metadata": {},
   "source": [
    "In this example, our confounder was binary, which made it very easy to compute the propensity scores using simple arithmetic. But in many applications, our confounders could be continuous, or we could have multiple confounders. In such cases, computing propensity scores is not as trivial.\n",
    "\n",
    "In these cases, we typically use logistic regression to predict them. Note that although this is the same logistic regression you're familiar with, the purpose of using it is very different: here, we aren't trying to make predictions, or achieve high prediction accuracy. We're instead just using it to determine for each row, what is the probability (based on the confounders) of that row ending up in the treatment group?\n",
    "\n",
    "This means that we don't have to worry about splitting our data into testing and training sets: instead, we fit the model on our entire dataset, and use the predicted probabilities as propensity scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
